{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    "from itertools import product\n",
    "from itertools import combinations_with_replacement\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import time\n",
    "import math\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, SpectralClustering, AffinityPropagation\n",
    "from sklearn.metrics.pairwise import pairwise_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gj = pd.read_csv('./gj_df.csv')\n",
    "# gj = gj[['ifp_id', 'ctt', 'cond', 'training', 'team', 'user_id', 'value', 'fcast_date']]\n",
    "# gj['fcast_year'] = pd.to_datetime(gj['fcast_date']).dt.year\n",
    "# gj['fcast_week'] = pd.to_datetime(gj['fcast_date']).dt.week\n",
    "# gj['ifp_week'] = gj['fcast_year'].map(str) + gj['fcast_week'].map(str) + gj['ifp_id']\n",
    "# gj = gj.drop('fcast_date', axis=1)\n",
    "# gj = gj.drop_duplicates()\n",
    "# gj.to_csv('./gj_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['G', 'P', 'R', 'X'], dtype=object)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# adult = pd.read_csv('../labels.txt', delimiter='\\t', header=0, names=['user_id','website','rating'])\n",
    "# trec = pd.read_csv('../trec-rf10-crowd/trec-rf10-data.txt', delimiter='\\t')\n",
    "# gj = pd.read_csv('./filled_active_df.csv')\n",
    "\n",
    "# best_users = trec.groupby('workerID').count().sort_values('docID', ascending=False)[:150].index\n",
    "# trec = trec[trec['workerID'].isin(best_users)]\n",
    "\n",
    "# r = pd.Series([2,3,2,3], index=[1,2,0,-2])\n",
    "# trec['label_bin'] = trec['label'].map(r)\n",
    "# adult.rating.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ifp_var = gj.groupby('ifp_week')['value'].var().sort_values()\n",
    "ifps_less_var = (ifp_var<.1).index\n",
    "ifps_less_var\n",
    "testframe = testframe[testframe['ifp_week'].isin(ifps_less_var)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "389"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ratingcts = adult.groupby('user_id')['rating'].nunique()\n",
    "# ratingcts = ratingcts[ratingcts==4]\n",
    "# len(ratingcts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>website</th>\n",
       "      <th>rating</th>\n",
       "      <th>bin</th>\n",
       "      <th>task_id</th>\n",
       "      <th>uid</th>\n",
       "      <th>bin_primes</th>\n",
       "      <th>bin_levels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A2LGX47NN7C5D3</td>\n",
       "      <td>http://000.cc</td>\n",
       "      <td>G</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A2OVKTB7VNY8EW</td>\n",
       "      <td>http://000.cc</td>\n",
       "      <td>G</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>A31Z3E5SLATLML</td>\n",
       "      <td>http://000.cc</td>\n",
       "      <td>P</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AAVCPZ8WTCFVK</td>\n",
       "      <td>http://000.cc</td>\n",
       "      <td>G</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AHY98D5P05XIV</td>\n",
       "      <td>http://000.cc</td>\n",
       "      <td>G</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          user_id        website rating  bin  task_id  uid  bin_primes  \\\n",
       "3  A2LGX47NN7C5D3  http://000.cc      G    0        0    0           2   \n",
       "4  A2OVKTB7VNY8EW  http://000.cc      G    0        0    1           2   \n",
       "5  A31Z3E5SLATLML  http://000.cc      P    1        0    2           3   \n",
       "6   AAVCPZ8WTCFVK  http://000.cc      G    0        0    3           2   \n",
       "7   AHY98D5P05XIV  http://000.cc      G    0        0    4           2   \n",
       "\n",
       "   bin_levels  \n",
       "3           2  \n",
       "4           2  \n",
       "5           3  \n",
       "6           2  \n",
       "7           2  "
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(len(adult['user_id'].unique()))\n",
    "# ratingcts = adult.groupby('user_id')['rating'].nunique()\n",
    "# ratingcts = ratingcts[ratingcts==4]\n",
    "# uids = ratingcts.index\n",
    "# print(len(uids))\n",
    "# adult = adult[adult['user_id'].isin(uids)]\n",
    "# testframe = create_user_task_ids(adult, 'user_id', 'website', 'rating', prime=True)\n",
    "# print(np.max(testframe['uid'].values))\n",
    "testframe = create_user_task_ids(gj, 'user_id', 'ifp_week', 'value', False, True)\n",
    "testframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(65144, 8)\n",
      "389\n",
      "10381\n"
     ]
    }
   ],
   "source": [
    "print(testframe.shape)\n",
    "print(len(testframe['uid'].unique()))\n",
    "print(len(testframe['task_id'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testframe['value'] = (testframe['value'] - .5)*2\n",
    "testframe['value'] = testframe['value']\n",
    "# testframe['value'] = (testframe['value']*.5) + .5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ifp_id</th>\n",
       "      <th>ctt</th>\n",
       "      <th>cond</th>\n",
       "      <th>training</th>\n",
       "      <th>team</th>\n",
       "      <th>user_id</th>\n",
       "      <th>value</th>\n",
       "      <th>fcast_year</th>\n",
       "      <th>fcast_week</th>\n",
       "      <th>ifp_week</th>\n",
       "      <th>bin</th>\n",
       "      <th>task_id</th>\n",
       "      <th>uid</th>\n",
       "      <th>bin_levels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1244-0</td>\n",
       "      <td>1a</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>2015</td>\n",
       "      <td>3</td>\n",
       "      <td>201531244-0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1244-0</td>\n",
       "      <td>1a</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>2015</td>\n",
       "      <td>4</td>\n",
       "      <td>201541244-0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1244-0</td>\n",
       "      <td>1a</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>2015</td>\n",
       "      <td>5</td>\n",
       "      <td>201551244-0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1244-0</td>\n",
       "      <td>1a</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>2015</td>\n",
       "      <td>6</td>\n",
       "      <td>201561244-0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1244-0</td>\n",
       "      <td>1a</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>2015</td>\n",
       "      <td>7</td>\n",
       "      <td>201571244-0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ifp_id ctt  cond training  team  user_id  value  fcast_year  fcast_week  \\\n",
       "0  1244-0  1a     1        a   NaN       51   -0.6        2015           3   \n",
       "1  1244-0  1a     1        a   NaN       51   -0.6        2015           4   \n",
       "2  1244-0  1a     1        a   NaN       51   -0.6        2015           5   \n",
       "3  1244-0  1a     1        a   NaN       51   -0.6        2015           6   \n",
       "4  1244-0  1a     1        a   NaN       51   -0.6        2015           7   \n",
       "\n",
       "      ifp_week  bin  task_id  uid bin_levels  \n",
       "0  201531244-0  0.2        0    0          2  \n",
       "1  201541244-0  0.2        1    0          2  \n",
       "2  201551244-0  0.2        2    0          2  \n",
       "3  201561244-0  0.2        3    0          2  \n",
       "4  201571244-0  0.2        4    0          2  "
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c = pd.cut(\n",
    "    testframe['value'],\n",
    "    [-np.inf, .2, .4, .6, .8, np.inf],\n",
    "    labels=[2,3,5,7,11]\n",
    ")\n",
    "testframe['bin_levels'] = c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# testframe = testframe[testframe['uid']<1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batcher(seq, size):\n",
    "    return (seq[pos:pos + size] for pos in range(0, len(seq), size))\n",
    "\n",
    "def split(df):\n",
    "    train_df, validate_df, test_df = np.split(df.sample(frac=1), [int(.6*len(df)), int(.8*len(df))])\n",
    "    return train_df, validate_df, test_df\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, users, tasks, k=2):\n",
    "        super(Model, self).__init__()\n",
    "        self.user_lut = nn.Embedding(users, k)\n",
    "        self.task_lut = nn.Embedding(tasks, k)\n",
    "\n",
    "        self.user_bias = nn.Embedding(users, 1)\n",
    "        self.task_bias = nn.Embedding(tasks, 1)\n",
    "        self.global_bias = nn.Parameter(torch.FloatTensor(1))\n",
    "        \n",
    "    def forward(self, users, jokes):\n",
    "        user_vectors = self.user_lut(users)\n",
    "        task_vectors = self.task_lut(jokes)\n",
    "        user_bias = self.user_bias(users)\n",
    "        task_bias = self.task_bias(jokes)\n",
    "        preds = torch.bmm(user_vectors.unsqueeze(1), task_vectors.unsqueeze(2)).squeeze() \\\n",
    "                         + user_bias.squeeze() + task_bias.squeeze() + self.global_bias.expand_as(user_bias.squeeze())\n",
    "#         m = nn.Sigmoid()\n",
    "        return preds\n",
    "        \n",
    "class Model2(nn.Module):\n",
    "    def __init__(self, users, tasks, k=2):\n",
    "        super(Model2, self).__init__()\n",
    "        self.user_lut = nn.Embedding(users, k)\n",
    "        self.task_lut = nn.Embedding(tasks, k)\n",
    "\n",
    "        self.user_bias = nn.Embedding(users, 1)\n",
    "        self.user_add_bias = nn.Embedding(users, 1)\n",
    "        self.task_bias = nn.Embedding(tasks, 1)\n",
    "        self.global_bias = nn.Parameter(torch.FloatTensor(1))\n",
    "        \n",
    "    def forward(self, users, jokes):\n",
    "        user_vectors = self.user_lut(users)\n",
    "        task_vectors = self.task_lut(jokes)\n",
    "        user_bias = self.user_bias(users)\n",
    "        task_bias = self.task_bias(jokes)\n",
    "        user_add_bias = self.user_add_bias(users)\n",
    "\n",
    "        preds = torch.bmm(user_vectors.unsqueeze(1), task_vectors.unsqueeze(2)).squeeze() \\\n",
    "                          + task_bias.squeeze() + self.global_bias.expand_as(user_bias.squeeze())\n",
    "        m = nn.Tanh()\n",
    "#         return m((user_bias.squeeze() * (preds - .5)) + .5)\n",
    "#         return m(user_bias.squeeze() * preds )\n",
    "        return m(user_bias.squeeze() * preds + user_add_bias.squeeze() )\n",
    "            \n",
    "\n",
    "def val(df, model):\n",
    "    crit = nn.MSELoss(size_average=False)\n",
    "    crit2 = nn.L1Loss(size_average=False)\n",
    "    total_loss = 0.\n",
    "    total_l1 = 0.\n",
    "    total_num = 0\n",
    "    for batch in batcher(df, 100):\n",
    "        true_rating = Variable(torch.Tensor(batch.bin.values.astype(float)))\n",
    "        total_num = total_num + true_rating.size(0)\n",
    "        users = Variable(torch.LongTensor(batch.uid.values))\n",
    "        tasks = Variable(torch.LongTensor(batch.task_id.values))\n",
    "#         print(users, tasks)\n",
    "        scores = model.forward(users, tasks)\n",
    "        total_loss += crit(scores, true_rating).data[0]\n",
    "        total_l1 += crit2(scores,true_rating).data[0]\n",
    "    return math.sqrt(total_loss/total_num), total_l1/total_num\n",
    "\n",
    "\n",
    "def train(train_iter, val_iter, test_iter, model):\n",
    "    opt = optim.SGD(model.parameters(), lr=.9)\n",
    "    crit = nn.MSELoss()\n",
    "    crit2 = nn.L1Loss()\n",
    "\n",
    "    print(\"val:\", val(validate_df, model))\n",
    "    for epochs in range(50):\n",
    "        avg_loss = 0\n",
    "        avg_l1 = 0\n",
    "        total = 0\n",
    "        for i,batch in enumerate(batcher(train_df, 100)):\n",
    "            opt.zero_grad()\n",
    "            rating = Variable(torch.Tensor(batch.bin.values.astype(float)))\n",
    "#             print(rating)\n",
    "            users = Variable(torch.LongTensor(batch.uid.values))\n",
    "            tasks = Variable(torch.LongTensor(batch.task_id.values))\n",
    "#             print(users, tasks)\n",
    "            scores = model.forward(users, tasks) \n",
    "#             + torch.sum(model.user_lut.weight.data.pow(2)) + \\\n",
    "#             torch.sum(model.task_lut.weight.data.pow(2)) + torch.sum(model.user_bias.weight.data.pow(2)) + \\\n",
    "#             torch.sum(model.task_bias.weight.data.pow(2))\n",
    "            loss = crit(scores, rating)\n",
    "            #if i % 1000==0:\n",
    "            #    print (loss.data[0])\n",
    "            loss.backward()\n",
    "            avg_loss += loss.data[0]\n",
    "            avg_l1 += crit2(scores,rating).data[0]\n",
    "            total += 1\n",
    "            opt.step()\n",
    "        print(\"train:\", math.sqrt(avg_loss / float(total)), avg_l1/ float(total))\n",
    "        print(\"val:\", val(validate_df, model))\n",
    "#         print(model.user_bias.weight.data)\n",
    "    return model.user_lut.weight.data, model.user_bias.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "388 10380\n",
      "388\n",
      "10380\n",
      "val: (2.6970005834564876, 2.1141692928248337)\n",
      "train: 1.7928485236919074 1.377020536481267\n",
      "val: (1.9083510788396418, 1.6013520283963503)\n",
      "train: 1.3452194456083661 1.0592210289767332\n",
      "val: (1.5168031530519093, 1.2393035342284107)\n",
      "train: 1.2109570888360242 0.9533623226768221\n",
      "val: (1.356291861763988, 1.093625904410174)\n",
      "train: 1.116388487975432 0.8772270153550541\n",
      "val: (1.2569957752245935, 1.0043823407318024)\n",
      "train: 1.039852139598294 0.8151958359171972\n",
      "val: (1.1843494197008357, 0.9390137011371005)\n",
      "train: 0.9752090914714647 0.7625205384191039\n",
      "val: (1.1270837567008452, 0.8875040367128286)\n",
      "train: 0.9196058984022646 0.7169822507807057\n",
      "val: (1.080084265919966, 0.8452345956678065)\n",
      "train: 0.8713144134419144 0.6772512192921261\n",
      "val: (1.0405264507858003, 0.8097755123723366)\n",
      "train: 0.8291238094598108 0.6423555012706601\n",
      "val: (1.0066627993069972, 0.7793431397771422)\n",
      "train: 0.7921134868376278 0.6115545083952072\n",
      "val: (0.9773280603823602, 0.7529184744088094)\n",
      "train: 0.7595489232840791 0.5843823776982934\n",
      "val: (0.9516990170440542, 0.7298351462989431)\n",
      "train: 0.7308262497570777 0.5602780305363638\n",
      "val: (0.9291650736871457, 0.7096132311984521)\n",
      "train: 0.7054392404743774 0.5388216789421219\n",
      "val: (0.9092546900776731, 0.6916459612414956)\n",
      "train: 0.6829578909744783 0.5196895305915257\n",
      "val: (0.891591417995671, 0.6755309943852319)\n",
      "train: 0.663013649652348 0.5026037896533146\n",
      "val: (0.8758674249665243, 0.6610197931008739)\n",
      "train: 0.6452886565688802 0.48729805896044387\n",
      "val: (0.8618264055456151, 0.6479688404453885)\n",
      "train: 0.6295076234851233 0.47355252283308513\n",
      "val: (0.8492523441923451, 0.6362052089399451)\n",
      "train: 0.6154313897508895 0.461226172764283\n",
      "val: (0.8379616239969465, 0.6255672658515202)\n",
      "train: 0.6028517653446331 0.4501503202159082\n",
      "val: (0.8277974007295303, 0.6159101926199982)\n",
      "train: 0.5915872753391186 0.4401992981696068\n",
      "val: (0.8186251067365747, 0.607092812663285)\n",
      "train: 0.5814795644534192 0.43124496891065633\n",
      "val: (0.8103289546781297, 0.5990704487642525)\n",
      "train: 0.5723903361253485 0.42314998222434\n",
      "val: (0.8028090779255979, 0.5917276346172212)\n",
      "train: 0.5641987782985765 0.4158348585181224\n",
      "val: (0.7959791255644736, 0.5850020055223727)\n",
      "train: 0.5567992469755109 0.4092047244996366\n",
      "val: (0.7897641641415962, 0.5788500037504056)\n",
      "train: 0.5500993566609539 0.4031708790823017\n",
      "val: (0.7840991443279177, 0.5732024723944501)\n",
      "train: 0.5440182588530231 0.39766438011927985\n",
      "val: (0.7789272443588645, 0.5679870979440822)\n",
      "train: 0.5384851381359479 0.39263293894050677\n",
      "val: (0.7741988118873612, 0.563189796647489)\n",
      "train: 0.5334379339339328 0.3880247244292208\n",
      "val: (0.7698701991198669, 0.5587527952833943)\n",
      "train: 0.5288222524803273 0.3838022253702364\n",
      "val: (0.765903132962661, 0.5546308077158578)\n",
      "train: 0.5245903080626663 0.3799264001876802\n",
      "val: (0.7622635691804163, 0.5507812542496826)\n",
      "train: 0.5207001295066191 0.37634822398500367\n",
      "val: (0.7589215454472694, 0.5471931576510645)\n",
      "train: 0.5171148100681449 0.37304686631083184\n",
      "val: (0.7558502671896989, 0.5438511534959726)\n",
      "train: 0.5138018213555431 0.36998875031385886\n",
      "val: (0.7530259010338524, 0.5407717721392243)\n",
      "train: 0.5107325015136165 0.3671455954956582\n",
      "val: (0.7504271006046299, 0.5378996089505174)\n",
      "train: 0.5078815369374171 0.36449547123421183\n",
      "val: (0.748034756030652, 0.5352293035623006)\n",
      "train: 0.5052265554544111 0.3620241992461407\n",
      "val: (0.7458316548906275, 0.5327458737184317)\n",
      "train: 0.5027477280119458 0.35971170069311587\n",
      "val: (0.7438023050953855, 0.530453576993817)\n",
      "train: 0.5004274974741295 0.35754846177442606\n",
      "val: (0.741932668235136, 0.5282905795954194)\n",
      "train: 0.49825024485952474 0.35551782813675875\n",
      "val: (0.7402101307866388, 0.5262688295059769)\n",
      "train: 0.4962020859786406 0.3536078142921638\n",
      "val: (0.7386231740762029, 0.5243761113162939)\n",
      "train: 0.49427065890016453 0.3518085481641847\n",
      "val: (0.7371613909181539, 0.5226051781032766)\n",
      "train: 0.49244491389209605 0.3501145112926088\n",
      "val: (0.7358152947951443, 0.5209443553816193)\n",
      "train: 0.49071498464458063 0.3485150446214944\n",
      "val: (0.7345762449391267, 0.5193961859792838)\n",
      "train: 0.48907203482442607 0.3469957013797882\n",
      "val: (0.7334363872818921, 0.5179495244964529)\n",
      "train: 0.4875081342380877 0.3455521110302347\n",
      "val: (0.7323885406179887, 0.5165951248379002)\n",
      "train: 0.48601615438467516 0.34417837252244926\n",
      "val: (0.7314261105234197, 0.5153183085997919)\n",
      "train: 0.48458969297165155 0.3428711140018595\n",
      "val: (0.7305430782258096, 0.5141189027593349)\n",
      "train: 0.48322296440440005 0.3416227457087363\n",
      "val: (0.729733962968348, 0.5130014987950717)\n",
      "train: 0.4819107549037429 0.3404317947909655\n",
      "val: (0.7289936771941574, 0.5119608040308244)\n",
      "train: 0.4806483434801939 0.3392918595420125\n",
      "val: (0.7283176209411806, 0.5109891456645894)\n"
     ]
    }
   ],
   "source": [
    "train_df, validate_df, test_df = split(testframe)\n",
    "validate_df = validate_df[validate_df['uid'].isin(train_df['uid'].values)][validate_df['task_id'].isin(train_df['task_id'].values)]\n",
    "\n",
    "users = int(np.max(train_df.uid.unique()))\n",
    "tasks = int(np.max(train_df.task_id.unique()))\n",
    "print(users, tasks)\n",
    "print(int(np.max(validate_df.uid.unique())))\n",
    "print(int(np.max(validate_df.task_id.unique())))\n",
    "model = Model2(users+1, tasks+1, k=4)\n",
    "user_vec, user_bias = train(train_df, validate_df, test_df, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_vec, user_bias = model.user_lut.weight.data, model.user_bias.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_features = np.zeros((user_vec.numpy().shape[0], user_vec.numpy().shape[1]+1))\n",
    "user_features[:, :user_vec.numpy().shape[1]] = user_vec.numpy()\n",
    "user_features[:, -1] = user_bias.numpy().reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(389, 5)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# np.savetxt('./user_id_mapping.csv', testframe[['uid','user_id']].drop_duplicates().values)\n",
    "# np.savetxt('./cluster_array_200.csv', kmeans.labels_)\n",
    "# np.savetxt('./latent_vectors.csv', user_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20 18 78 20 18 98 99 43 98 39 90 18  3 43 39 59 18 39 20 18 20 20 18 98 61\n",
      "  3 67 30 67 18 98 39 39 70 30 20 75 20 39 20 30 18 99 96 18 18 30 20 99 18\n",
      "  0 77 84 18 57 19 78 18 18 39 39 20 81 81 43 41 39 18 14 39 39 30 77 18 20\n",
      " 20 43 39  0  8  5 30 78 98 99  1 43 28 43 78 67 39 39  0 65 20 20 18 20  0\n",
      " 90 20 53  0 90 39  3 59 78 49 78 73 39 39 39 30 86 47 20 18 18 30 39 78 78\n",
      " 18 61 18 43 18 43 20 39 20 39 20 19 20 39 75  3 69 18 90 81 43 39 18 59 18\n",
      " 63 42 83 18 90 43 91 48 18 39  0 18 44 63 59 39 31 70 60 18 75 18 78 53 28\n",
      " 39 78 31 39 30 43 32 43 59 93  0  3 77 42 21 20 78  5 78 39 22 43 78 90 59\n",
      " 20  5 96 20 20 20 90 30  3 30 48 75 18 94 63 46 67 96 38 90 18 43 42 75 30\n",
      " 30  5  7 43 39 10 96 77 59 59 27 30 61  7  5 59 81 20 39  0 30 30 74 68 59\n",
      " 32 71 30 11 30 73 61 10 70  7 83 42 31 39 67  5 84 20 30  3 84 21 24 58 90\n",
      " 72 39  7 99 25 42 18 46 20 43 77 78 81 70 39 21 20 48 85  0 90 52 48 77 70\n",
      " 73 13 55 96  3 39 70 78 22 83  3 84 67 81 78 78 86 29 75 30 75 19 78 77 70\n",
      " 87 78 61 64 21  5 54 77 61 92 20 12 20 80 14 89 83  3  0 48 40 88 51 31  1\n",
      "  4 23 30 97 95 82 93 95 50 35 89 66 74 15 33 75 53 34 45 26 65 79 76 37  2\n",
      " 62  2  9  5 36 28 15 16 30 17 56  6 53  7]\n"
     ]
    }
   ],
   "source": [
    "kmeans = KMeans(n_clusters=100, random_state=0).fit(user_features)\n",
    "print(kmeans.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# completed, values, ind = compute_individual_dist(testframe, True, False)\n",
    "completed, values, ind = compute_individual_dist(testframe, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "182.58380913734436\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "features = np.empty((ind.shape[0], ind.shape[1]**2*ind.shape[0]))\n",
    "delta_matrices_all = np.empty((ind.shape[0], ind.shape[0], ind.shape[1], ind.shape[1]))\n",
    "score_matrices_all = np.empty((ind.shape[0], ind.shape[0], ind.shape[1], ind.shape[1]))\n",
    "for user_index in range(values.shape[0]):\n",
    "#     print(ind[user_index])\n",
    "    if user_index%100==0:\n",
    "        print(user_index)\n",
    "    #check for full joint distribution or add a prior later\n",
    "    if np.sum(ind[user_index]==0) > 0:\n",
    "        print(user_index)\n",
    "        continue\n",
    "    #compute delta matrices with all other users where applicable\n",
    "    else:\n",
    "        #create a mask so that other half of tasks can be used later to find score matrix\n",
    "#         mask = np.random.randint(0,2,values.shape).astype(bool)\n",
    "        mask = np.ones((values.shape)).astype(bool)\n",
    "        delta_matrices, t_m_i_1, cluster_img = compute_deltas(user_index, completed, values, ind, mask, False, 2)\n",
    "#         print(cluster_img)\n",
    "        features[user_index,:] = cluster_img.flatten()\n",
    "        delta_matrices_all[user_index,:,:,:] = delta_matrices\n",
    "#         score_matrices, t_m_i_2 = compute_deltas(user_index, completed, values, ind, ~mask, True, 20)\n",
    "#         score_matrices_all[user_index,:,:,:] = score_matrices\n",
    "#         print(np.sum(t_m_i_1), np.sum(t_m_i_2))\n",
    "#         if len(np.intersect1d(np.array(np.where(t_m_i_1==True)), np.array(np.where(t_m_i_2==True))))>0:\n",
    "#             print(np.intersect1d(np.array(np.where(t_m_i_1==True)), np.array(np.where(t_m_i_2==True))))\n",
    "#             print(regret(score_matrices, delta_matrices, \\\n",
    "#                      np.logical_and((t_m_i_1==True), (t_m_i_2==True))))\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with open('./dm_all.pkl', 'wb') as f:\n",
    "#     pickle.dump(delta_matrices_all, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with open('./features.pkl', 'wb') as f:\n",
    "#     pickle.dump(features, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with open('./dm_all.pkl', 'rb') as f:\n",
    "#     x = pickle.load(f)\n",
    "# print(np.array(x))\n",
    "# print(np.array(x).shape)\n",
    "type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def kmeans_missing(X, n_clusters, max_iter=10):\n",
    "    \"\"\"Perform K-Means clustering on data with missing values.\n",
    "\n",
    "    Args:\n",
    "      X: An [n_samples, n_features] array of data to cluster.\n",
    "      n_clusters: Number of clusters to form.\n",
    "      max_iter: Maximum number of EM iterations to perform.\n",
    "\n",
    "    Returns:\n",
    "      labels: An [n_samples] vector of integer labels.\n",
    "      centroids: An [n_clusters, n_features] array of cluster centroids.\n",
    "      X_hat: Copy of X with the missing values filled in.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize missing values to their column means\n",
    "    missing = ~np.isfinite(X)\n",
    "    mu = np.nanmean(X, 0, keepdims=1)\n",
    "    X_hat = np.where(missing, mu, X)\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        print(i)\n",
    "        if i > 0:\n",
    "            # initialize KMeans with the previous set of centroids. this is much\n",
    "            # faster and makes it easier to check convergence (since labels\n",
    "            # won't be permuted on every iteration), but might be more prone to\n",
    "            # getting stuck in local minima.\n",
    "            cls = KMeans(n_clusters, init=prev_centroids)\n",
    "        else:\n",
    "            # do multiple random initializations in parallel\n",
    "            cls = KMeans(n_clusters, n_jobs=-1)\n",
    "\n",
    "        # perform clustering on the filled-in data\n",
    "        labels = cls.fit_predict(X_hat)\n",
    "        centroids = cls.cluster_centers_\n",
    "\n",
    "        # fill in the missing values based on their cluster centroids\n",
    "        X_hat[missing] = centroids[labels][missing]\n",
    "\n",
    "        # when the labels have stopped changing then we have converged\n",
    "        if i > 0 and np.all(labels == prev_labels):\n",
    "            break\n",
    "\n",
    "        prev_labels = labels\n",
    "        prev_centroids = cls.cluster_centers_\n",
    "\n",
    "    return labels, centroids, X_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/annahilgard/anaconda/envs/python3/lib/python3.6/site-packages/sklearn/cluster/k_means_.py:893: RuntimeWarning: Explicit initial center position passed: performing only one init in k-means instead of n_init=10\n",
      "  return_n_iter=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[79 84 37 37 28 16 54 21 23 15 15 15  7 15  4 37 37  6 37  7 29 46 83 34 19\n",
      " 60 37 51  7 70 11 15 41 62 69 37  5 35 37 20 44 49 15 15 30 56 13 89 17  0\n",
      "  7  7 27 48 42 37  7 15 53 15 15  2 37 37 12 37 38 15 37 74 96  9  7 22  7\n",
      " 15 33 15 39 37  2  7  7 15  7  7 37 37 15  7 15 65 37  7  7 86 15 76 59 95\n",
      " 71  7 81 15 37 72 37 15 37 15 73  7 50 37 52  8 37 62  7  0 43 24 78 90 87\n",
      "  7 14 37 61 26 15  7 15 36 15 55 37 15 15 37 37  7 15 15 37  7 15 15  7 93\n",
      " 15  7 15 94  7 15 15  7 15 15  7 15 15 37 64 15 37 15 37 68  2 15 99  2  7\n",
      " 37 63 37 67 37  5 62  5 37  7 15 37 37 77 37  5  5 62 57 15  7 37  7  7 37\n",
      " 15 37  0 91 15 37 37 18  5 69  7 37 80  7  7  7 37 15 37 37 15 37 82  5 47\n",
      " 82  7 31 98 15 37 15 15 25 37 37 37  7 37 62 62  7 15 37  7 62  8 37 37 37\n",
      " 37 37  5  7  7 45 97  7 15 37 15  7 37  7  7 37  7  5 62 62 10 37 37 37  7\n",
      " 15 15 37  5 37  7 32 37 37 15 37 15  7 15 15 37 37 37 15  7 15 37 37 37 37\n",
      " 58 37 40 37 37 15 15 15 37 15  7 66 15  7  5  5  7 37 62 62 75  7  7 37  7\n",
      " 37 37 88 37  7  7 37  7  3 37 15 37 62  7 37 37 15  7 15  5 92 85 37  2 37\n",
      " 37  1 62 37 15 37 37 37  7 15 37 37  7 37  7 37 62 37 37 37 37 37 37  7 62\n",
      " 37 37 37  7 37 37  7 37 37 37 37 37 37 37]\n"
     ]
    }
   ],
   "source": [
    "labels, centroids, X_hat = kmeans_missing(features, 100, max_iter=5)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_cluster_matrix_dist(labels, delta_matrices_all):\n",
    "#     delta_matrices_new=np.zeros((len(np.unique(labels)), len(np.unique(labels)), delta_matrices_all[0][0].shape[0], \\\n",
    "#                                                               delta_matrices_all[0][0].shape[1]))\n",
    "    delta_matrices_new=np.empty(delta_matrices_all.shape)\n",
    "    for i in np.unique(labels):\n",
    "        if i%100==0:\n",
    "            print(i)\n",
    "        missing = ~np.isfinite(delta_matrices_all)\n",
    "        mu = np.nanmean(delta_matrices_all, 0, keepdims=0)\n",
    "        filled = np.where(missing, mu, delta_matrices_all)\n",
    "        cluster_i = np.average(filled[labels==i], axis=0)\n",
    "        for j in np.unique(labels): \n",
    "            if np.sum(np.sum(np.isnan(cluster_i), (1,2)))>0:\n",
    "                print(i,j)\n",
    "                missing = ~np.isfinite(deltas_used)\n",
    "                mu = np.nanmean(delta_matrices_all, 1, keepdims=1)\n",
    "                X_hat = np.where(missing, mu, X)\n",
    "            cluster_j = np.average(cluster_i[labels==j], axis=0)\n",
    "#             delta_matrices_new[i,j] = cluster_j\n",
    "            delta_matrices_new[np.ix_(labels==i,labels==j)] = cluster_j\n",
    "    return delta_matrices_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(389, 389, 4, 4)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta_matrices_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "K=4 score: 0.07798683683858106, 0.849352222276511\n"
     ]
    }
   ],
   "source": [
    "delta_matrices_clust = calc_cluster_matrix_dist(kmeans.labels_, delta_matrices_all)\n",
    "avg_dist = np.average([[pairwise_distances(delta_matrices_clust[i,j], delta_matrices_all[i,j]) for i in range(delta_matrices_all.shape[0])] for j in range(delta_matrices_all.shape[0])])\n",
    "max_dist = np.max([[pairwise_distances(delta_matrices_clust[i,j], delta_matrices_all[i,j]) for i in range(delta_matrices_all.shape[0])] for j in range(delta_matrices_all.shape[0])])\n",
    "print(\"K={} score: {}, {}\".format(k_, avg_dist, max_dist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "delta_matrices_clust_ref = calc_cluster_matrix_dist(labels, delta_matrices_all)\n",
    "# avg_dist = np.average([[pairwise_distances(delta_matrices_clust[i,j], delta_matrices_all[i,j]) for i in range(delta_matrices_all.shape[0])] for j in range(delta_matrices_all.shape[0])])\n",
    "# max_dist = np.max([[pairwise_distances(delta_matrices_clust[i,j], delta_matrices_all[i,j]) for i in range(delta_matrices_all.shape[0])] for j in range(delta_matrices_all.shape[0])])\n",
    "# print(\"K={} score: {}, {}\".format(k_, avg_dist, max_dist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "delta_matrices_clust = calc_cluster_matrix_dist(kmeans.labels_, delta_matrices_all)\n",
    "# print(np.average([[pairwise_distances(delta_matrices_clust[i,j], delta_matrices_all[i,j]) for i in range(1000)] for j in range(1000)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val: (0.6987190232629525, 0.5700266969052522)\n",
      "train: 0.5587561793864447 0.44557927274582065\n",
      "val: (0.47552665723685394, 0.38366568446049243)\n",
      "train: 0.4349172522847618 0.35672646145414594\n",
      "val: (0.40763345544557533, 0.34064426782586715)\n",
      "train: 0.38803979868488286 0.32854561751858297\n",
      "val: (0.3759200289668357, 0.3222991213493633)\n",
      "train: 0.36499612961060546 0.3153550618635834\n",
      "val: (0.3589411806939865, 0.31189319669916693)\n",
      "train: 0.35114515586270795 0.3053264139048679\n",
      "val: (0.346517363927786, 0.30045238237119964)\n",
      "train: 0.33806265307542965 0.290064672280638\n",
      "val: (0.33129667145267505, 0.28016804956587654)\n",
      "train: 0.3205448214671272 0.26721464876680173\n",
      "val: (0.31337363726686623, 0.25731300726060896)\n",
      "train: 0.30762496787012644 0.25132154614369356\n",
      "val: (0.30553574115434756, 0.24832530998355237)\n",
      "train: 0.3024787430806144 0.2456613460289817\n",
      "val: (0.3023188558894671, 0.24497890023656374)\n",
      "train: 0.2996086697996544 0.2432284078176894\n",
      "val: (0.2992738193297631, 0.2426309807977303)\n",
      "train: 0.2970762736538257 0.2411969233322483\n",
      "val: (0.2972108621329382, 0.2410527887084489)\n",
      "train: 0.29507538458734284 0.23954192666596125\n",
      "val: (0.2955151739684822, 0.23952130160222748)\n",
      "train: 0.29357578786132393 0.23828025038816655\n",
      "val: (0.29385241107325455, 0.2381729820527041)\n",
      "train: 0.2917525981117005 0.2368455966938703\n",
      "val: (0.2917997378701138, 0.2365410916613067)\n",
      "train: 0.2897406901438166 0.2351874790376772\n",
      "val: (0.2898457893848429, 0.23492374248233094)\n",
      "train: 0.2877520816636418 0.23349120775498566\n",
      "val: (0.2878135430886394, 0.2330934148490912)\n",
      "train: 0.28587362162492497 0.2317990666382071\n",
      "val: (0.2860458820799378, 0.23151284255112048)\n",
      "train: 0.2840286128506542 0.230177614700402\n",
      "val: (0.28402325157719316, 0.22979660034243296)\n",
      "train: 0.2817829940201311 0.22827077545010285\n",
      "val: (0.2814815267002408, 0.22754518730973664)\n",
      "train: 0.2794064349084471 0.22612301369556695\n",
      "val: (0.27927903464026516, 0.22552909335339066)\n",
      "train: 0.27731837123270314 0.22412399286897036\n",
      "val: (0.2772266004499918, 0.2235901916584714)\n",
      "train: 0.2753300085786576 0.2222221676540168\n",
      "val: (0.2752842882600363, 0.2216813603125382)\n",
      "train: 0.27322225459579563 0.2203179611553214\n",
      "val: (0.27263436745029296, 0.21941831532763556)\n",
      "train: 0.2705697146118072 0.21798701955377392\n",
      "val: (0.27034516441482614, 0.21721427159232756)\n",
      "train: 0.26865297708362157 0.21609777590966373\n",
      "val: (0.26857677434337857, 0.21542848418073715)\n",
      "train: 0.2669067452715228 0.21437441146025238\n",
      "val: (0.266799702440281, 0.2136839227818015)\n",
      "train: 0.2648289472604286 0.2125562865580252\n",
      "val: (0.26458624859648494, 0.2117492352912471)\n",
      "train: 0.2628332102842141 0.21071142729797487\n",
      "val: (0.26275715607552236, 0.2100519006373995)\n",
      "train: 0.2609038681516121 0.20894512996614217\n",
      "val: (0.26054724770037097, 0.20817442331084493)\n",
      "train: 0.2590536639776803 0.2071839973824111\n",
      "val: (0.25915143667343393, 0.20673574618337118)\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "K=2 score: 0.0782922658379942, 0.8496665788919354\n",
      "val: (0.7293899341864489, 0.5983257522118159)\n",
      "train: 0.5858973169237728 0.46527016590560155\n",
      "val: (0.4949899792906863, 0.3958681186198468)\n",
      "train: 0.45201467072459445 0.36651304944614244\n",
      "val: (0.42253050255383745, 0.3487569175810462)\n",
      "train: 0.4019737085508723 0.3359916633787607\n",
      "val: (0.3888098997820411, 0.3289361279316863)\n",
      "train: 0.37720762614473796 0.3217891057586341\n",
      "val: (0.3710946502558418, 0.31867254919865845)\n",
      "train: 0.3624054123229235 0.3132837143982978\n",
      "val: (0.35904368813312443, 0.31091333909807073)\n",
      "train: 0.3518486531634938 0.3054417517912446\n",
      "val: (0.3483455793843599, 0.3010286100832443)\n",
      "train: 0.34108136144206247 0.29147515475266295\n",
      "val: (0.33435184510446897, 0.28157283740761313)\n",
      "train: 0.32541406087959535 0.26742392673710813\n",
      "val: (0.3177143981468272, 0.25825325720344217)\n",
      "train: 0.31389158511286197 0.25359030576110575\n",
      "val: (0.3103852524814995, 0.2512033510096975)\n",
      "train: 0.30959433382978147 0.24948789978548122\n",
      "val: (0.30840410379347594, 0.2491030230097937)\n",
      "train: 0.3080804844582534 0.2480049405152557\n",
      "val: (0.307290596178315, 0.2479826255981964)\n",
      "train: 0.3066671308858289 0.24692315284432756\n",
      "val: (0.30520495536617726, 0.24654257213910863)\n",
      "train: 0.304748557246948 0.24546738672072277\n",
      "val: (0.303910895291613, 0.24547496806009708)\n",
      "train: 0.3034122662793309 0.2443510648600257\n",
      "val: (0.30259395300558467, 0.24444031639142758)\n",
      "train: 0.3018573268956389 0.24323270140276212\n",
      "val: (0.3009061439577771, 0.24324861097789183)\n",
      "train: 0.2995672652902315 0.24174254654963423\n",
      "val: (0.2986512804066812, 0.24182170214358556)\n",
      "train: 0.29731649041774294 0.24022642757930773\n",
      "val: (0.2951176843055251, 0.2395178875029414)\n",
      "train: 0.2937044603505009 0.2379462142512633\n",
      "val: (0.29320537871417773, 0.23790654960581617)\n",
      "train: 0.2919140275075916 0.2364369348211652\n",
      "val: (0.29192803553896696, 0.23670429865512846)\n",
      "train: 0.29071668847636406 0.23530614786694823\n",
      "val: (0.29075172195506377, 0.23563631973309093)\n",
      "train: 0.28939151134203983 0.2341459703285093\n",
      "val: (0.2893733125124386, 0.23441415555355674)\n",
      "train: 0.2878831552442759 0.23282057471099615\n",
      "val: (0.2879530699383042, 0.23311248137986446)\n",
      "train: 0.2866580203264207 0.23163631712465405\n",
      "val: (0.28676803795794814, 0.232035784243852)\n",
      "train: 0.2852946580672239 0.2304232986623205\n",
      "val: (0.28515076542590406, 0.2307887889687558)\n",
      "train: 0.28361882015885204 0.2289924953346546\n",
      "val: (0.28351722983592986, 0.22953493703042527)\n",
      "train: 0.2816429181279475 0.22733796936020473\n",
      "val: (0.2816605685113499, 0.22799200207952414)\n",
      "train: 0.2802382697649582 0.22601205623377796\n",
      "val: (0.2803871798283767, 0.2268244750684372)\n",
      "train: 0.2787685987853404 0.2247022436596304\n",
      "val: (0.27877245713749405, 0.2253769972080919)\n",
      "train: 0.27706970943101444 0.2231803263436958\n",
      "val: (0.2773120919389572, 0.22390284768260332)\n",
      "train: 0.27559758228716785 0.2218202586758264\n",
      "val: (0.2756292935182695, 0.2223585510999394)\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "K=3 score: 0.07752178859015961, 0.8052327104595812\n",
      "val: (0.7519602982760262, 0.6194406366363692)\n",
      "train: 0.6081012902254044 0.4814023677741702\n",
      "val: (0.5135709240893948, 0.4073118666780976)\n",
      "train: 0.4681996368198554 0.37544956098680443\n",
      "val: (0.4385624416268681, 0.3570605750595317)\n",
      "train: 0.4170076350162008 0.3436694663496694\n",
      "val: (0.4015913913600609, 0.3354261883385077)\n",
      "train: 0.38707760999032403 0.3265676505022205\n",
      "val: (0.37891901830855856, 0.32255574064690573)\n",
      "train: 0.36919914343581206 0.31497522183640525\n",
      "val: (0.36143658356648445, 0.3071483473852685)\n",
      "train: 0.3503845815712528 0.29008496886143265\n",
      "val: (0.3391051686485889, 0.2754525059479066)\n",
      "train: 0.3298877474357463 0.26621009655903055\n",
      "val: (0.3267344183294546, 0.26257051356180455)\n",
      "train: 0.32455879179755104 0.2607405941325487\n",
      "val: (0.3241048817904401, 0.2604183064888205)\n",
      "train: 0.32238784576465235 0.2589972229577558\n",
      "val: (0.32217082497794775, 0.25881195088395953)\n",
      "train: 0.3203930029638325 0.25753479393628925\n",
      "val: (0.3200052732673788, 0.2571842571887744)\n",
      "train: 0.3185487482031788 0.25610088308707457\n",
      "val: (0.3183659694460608, 0.2558908695075933)\n",
      "train: 0.3170381345496062 0.2549557646545186\n",
      "val: (0.31694193785642755, 0.25485483074798554)\n",
      "train: 0.3152145987089707 0.2537125357625059\n",
      "val: (0.3151549993639994, 0.2536860704537287)\n",
      "train: 0.3127899057672647 0.25205743592936447\n",
      "val: (0.3124004354870706, 0.2518240867131248)\n",
      "train: 0.31053706529710845 0.25044726854850313\n",
      "val: (0.31099608059253075, 0.25058264446744194)\n",
      "train: 0.3094319731150928 0.24946151590310187\n",
      "val: (0.30986095222325105, 0.249616645343509)\n",
      "train: 0.30792146324349196 0.24833732970777805\n",
      "val: (0.30791548249412826, 0.24832478397249763)\n",
      "train: 0.3061711248748977 0.2469978636042072\n",
      "val: (0.306409128094811, 0.24708145408081686)\n",
      "train: 0.30480092986169943 0.24585352208102582\n",
      "val: (0.3052557302504843, 0.24611629386931547)\n",
      "train: 0.3032757640203476 0.24469375304907573\n",
      "val: (0.3035870780105027, 0.2448954605574461)\n",
      "train: 0.3017489788203285 0.24351139810547504\n",
      "val: (0.3020775019490012, 0.2436678985397449)\n",
      "train: 0.30009348009268577 0.24228289639250125\n",
      "val: (0.299727335239609, 0.24210057002535312)\n",
      "train: 0.2981242034584032 0.2408621893916569\n",
      "val: (0.2983218797364224, 0.24102208184509114)\n",
      "train: 0.29695742058581703 0.239851784800863\n",
      "val: (0.29719297160664093, 0.2401070696353965)\n",
      "train: 0.2958892422007657 0.23891815369581848\n",
      "val: (0.2962161793472533, 0.2392831135582658)\n",
      "train: 0.29457082521285044 0.23786859054626916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val: (0.2949551887793111, 0.2382697740913897)\n",
      "train: 0.2934419785503004 0.23685293514803285\n",
      "val: (0.29390538064193994, 0.23739576067955714)\n",
      "train: 0.29219773222908857 0.23580339587910168\n",
      "val: (0.2924065425003174, 0.2362784227007358)\n",
      "train: 0.29014246383165987 0.23430113106462466\n",
      "val: (0.2904469815321397, 0.23476577823720296)\n",
      "train: 0.28885452025043257 0.23315416725344953\n",
      "val: (0.28946574334346686, 0.23379729138130712)\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "K=4 score: 0.07758545924960727, 0.8114071965300811\n",
      "val: (0.7667574496358466, 0.6344576256642014)\n",
      "train: 0.6223367388440045 0.49368091484489524\n",
      "val: (0.5316755941101969, 0.42035053703736036)\n",
      "train: 0.48546743422749616 0.38744647371578134\n",
      "val: (0.45549789231281673, 0.36807584677179256)\n",
      "train: 0.42839976244810685 0.35048034098107\n",
      "val: (0.41221191481658903, 0.3410076483772996)\n",
      "train: 0.3965320378478991 0.3310868235105353\n",
      "val: (0.3873613323021625, 0.32580447683483543)\n",
      "train: 0.3746452447761022 0.3159647030899158\n",
      "val: (0.36646552214422906, 0.3066878015804941)\n",
      "train: 0.3561246377082511 0.2912372363347804\n",
      "val: (0.34871716858583424, 0.28108840469443247)\n",
      "train: 0.34352655669109655 0.27572898968736187\n",
      "val: (0.3414551664652448, 0.27428682482086464)\n",
      "train: 0.33792211334510586 0.2712743910416305\n",
      "val: (0.33705709959075325, 0.2710367621828585)\n",
      "train: 0.33369975730572277 0.26846532963743047\n",
      "val: (0.3338458807890551, 0.2687634896762786)\n",
      "train: 0.33050123977610163 0.26623995023046193\n",
      "val: (0.3306460936101049, 0.2664834617643319)\n",
      "train: 0.32828641051084245 0.2644941017541847\n",
      "val: (0.3289480161805929, 0.2651296552106897)\n",
      "train: 0.32626816234392564 0.26295541987492277\n",
      "val: (0.3269397425036667, 0.2634731350213686)\n",
      "train: 0.3246410139361697 0.26153631645998937\n",
      "val: (0.32552744195257377, 0.26225649271913987)\n",
      "train: 0.32290340081134317 0.2601363614131976\n",
      "val: (0.323601510013919, 0.26082179434715885)\n",
      "train: 0.32146145598969533 0.25898701260955453\n",
      "val: (0.3223235259921838, 0.25981265878012183)\n",
      "train: 0.31996069416623824 0.2577964339592474\n",
      "val: (0.3212531166453247, 0.25876845202686855)\n",
      "train: 0.3186556866774972 0.25669461826136664\n",
      "val: (0.31942869000435514, 0.2574211542474025)\n",
      "train: 0.31665655991251285 0.2551712286855285\n",
      "val: (0.3181270847217278, 0.2562640174677264)\n",
      "train: 0.3156870300627105 0.2543142012930436\n",
      "val: (0.31678006025957367, 0.25527420443531923)\n",
      "train: 0.31380226375009956 0.2530283869097979\n",
      "val: (0.31497938350642707, 0.254041560101485)\n",
      "train: 0.31192730101762683 0.25179929815467966\n",
      "val: (0.3118545262203226, 0.25216320615238724)\n",
      "train: 0.3094036655937281 0.25007332138230187\n",
      "val: (0.310595988247748, 0.25110367768567576)\n",
      "train: 0.3083644982103851 0.2491849090567744\n",
      "val: (0.3091468672743662, 0.25008276297073473)\n",
      "train: 0.30713486192669043 0.2482174353921689\n",
      "val: (0.30843145085108786, 0.24936459592821167)\n",
      "train: 0.30628115915983845 0.24741816234958336\n",
      "val: (0.3074818433298415, 0.24851010951633473)\n",
      "train: 0.30538507409023297 0.24660220943812053\n",
      "val: (0.3066504998233537, 0.24777560937433216)\n",
      "train: 0.30415203721174017 0.2455949118785749\n",
      "val: (0.3049881808716196, 0.24647320809549308)\n",
      "train: 0.30249606508280363 0.24433709357700756\n",
      "val: (0.3032280448137458, 0.2452080894876722)\n",
      "train: 0.3009123491250183 0.24313090533753562\n",
      "val: (0.30203609738878257, 0.24422335794131123)\n",
      "train: 0.3000858906297627 0.24232010236411639\n",
      "val: (0.30123740538428667, 0.2434843210855509)\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "K=5 score: 0.07746310336814961, 0.8259443186154583\n",
      "val: (0.7900114755328784, 0.6565928412781212)\n",
      "train: 0.6420234436858604 0.5091165218911419\n",
      "val: (0.5490502970453617, 0.4318212582858076)\n",
      "train: 0.501238133906165 0.39699333046160046\n",
      "val: (0.4664403779139987, 0.37403519392706086)\n",
      "train: 0.43904822908699925 0.3563530935631322\n",
      "val: (0.42279287348712985, 0.347208678241941)\n",
      "train: 0.406434277719259 0.33727631590375906\n",
      "val: (0.39886175876578384, 0.3335180551305545)\n",
      "train: 0.3886744520968946 0.3273589830396704\n",
      "val: (0.3864527579840175, 0.3267076260769421)\n",
      "train: 0.3775073373463567 0.3214498973551354\n",
      "val: (0.37606025220271, 0.3212183156844418)\n",
      "train: 0.3680210591932992 0.31617394914927616\n",
      "val: (0.36747720729398736, 0.31574258374997854)\n",
      "train: 0.3565698331711044 0.30183489237721095\n",
      "val: (0.3448461089213958, 0.2808518354405786)\n",
      "train: 0.33629165448654863 0.271666291814584\n",
      "val: (0.3320800108585719, 0.26741516732163434)\n",
      "train: 0.3301816869413283 0.26552418895354\n",
      "val: (0.3303375195461187, 0.2658903632664709)\n",
      "train: 0.32835335309605507 0.2642709250255869\n",
      "val: (0.3279055560272625, 0.2645833722787159)\n",
      "train: 0.3254799727162262 0.2625178062966157\n",
      "val: (0.32579100048701615, 0.2629626108228285)\n",
      "train: 0.323236635064895 0.26094967303255123\n",
      "val: (0.3239003496192768, 0.2616220847292439)\n",
      "train: 0.3215813195831733 0.2597131375297937\n",
      "val: (0.3218171548239184, 0.2602272995129101)\n",
      "train: 0.31908917699434025 0.2580345701368293\n",
      "val: (0.31979923296613727, 0.2587498657792268)\n",
      "train: 0.31770656874140163 0.25683814345988604\n",
      "val: (0.3190228929092226, 0.25805502699669625)\n",
      "train: 0.3164586789921198 0.25586605644811783\n",
      "val: (0.3174400332364236, 0.25691185537146816)\n",
      "train: 0.3142742899184349 0.25449759710088155\n",
      "val: (0.314967693301496, 0.25537525849900206)\n",
      "train: 0.31271618564324605 0.25331593065519714\n",
      "val: (0.3140911064047809, 0.2545069028029315)\n",
      "train: 0.3118785207397261 0.2524965860068851\n",
      "val: (0.3132470662721837, 0.2537059240308206)\n",
      "train: 0.3111513376611422 0.2517862079351173\n",
      "val: (0.31255367799985284, 0.25305551985491126)\n",
      "train: 0.31017481012064685 0.25098770649639063\n",
      "val: (0.3114965245146576, 0.25218470223271783)\n",
      "train: 0.30889722709683426 0.250046357524069\n",
      "val: (0.3100203859687987, 0.25111850388502277)\n",
      "train: 0.30786877898935155 0.2491750382201679\n",
      "val: (0.30934270549251996, 0.2504474231311771)\n",
      "train: 0.3068952282776726 0.24836866868177007\n",
      "val: (0.307976301014395, 0.24937602242916537)\n",
      "train: 0.3057366499837272 0.24746113863519797\n",
      "val: (0.30680932450348625, 0.24858744925651555)\n",
      "train: 0.3039342582422127 0.24622247160713426\n",
      "val: (0.30495933095696504, 0.24722246537425732)\n",
      "train: 0.30288817115876276 0.24529532553845906\n",
      "val: (0.303873267443758, 0.24632211375046242)\n",
      "train: 0.30175458144524453 0.2444161317593233\n",
      "val: (0.3025729647285706, 0.2454091686636166)\n",
      "train: 0.30083846794986596 0.24361957129622736\n",
      "val: (0.30173506165781433, 0.24463227831381795)\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "K=6 score: 0.07738067569620717, 0.8450048832732695\n",
      "val: (0.8007849160709282, 0.6684452956673409)\n",
      "train: 0.6490807809645752 0.5133674492361334\n",
      "val: (0.5519758154898977, 0.43266241442605835)\n",
      "train: 0.5032659763339622 0.3960500407703933\n",
      "val: (0.4676930538498349, 0.3725290871158195)\n",
      "train: 0.4431791542590801 0.3565211548652581\n",
      "val: (0.42954552231496523, 0.3493003840889487)\n",
      "train: 0.411886161784677 0.3380535918409266\n",
      "val: (0.4029366412651032, 0.33270153901960386)\n",
      "train: 0.3894351400382796 0.322597780618974\n",
      "val: (0.3831178246039496, 0.31178412363764785)\n",
      "train: 0.37245720089622353 0.29709376498440626\n",
      "val: (0.36846163201849075, 0.2914228995870857)\n",
      "train: 0.3599033469862568 0.2867264667962354\n",
      "val: (0.3559570355889681, 0.2835489400720192)\n",
      "train: 0.3499441816376163 0.2801026560750443\n",
      "val: (0.3484353019057285, 0.2787344577027751)\n",
      "train: 0.3435720034038917 0.2759870515747858\n",
      "val: (0.34254960751782043, 0.27516777329619824)\n",
      "train: 0.3375039801560222 0.27195441423057715\n",
      "val: (0.3367355729868665, 0.271294650581339)\n",
      "train: 0.33252936466279903 0.2685760841992708\n",
      "val: (0.3331546619941075, 0.2687276708235702)\n",
      "train: 0.32995367855257235 0.26657253348592813\n",
      "val: (0.33115915284900993, 0.2669817322088668)\n",
      "train: 0.32840798246611275 0.2651480424928898\n",
      "val: (0.330335864246621, 0.26602697292131194)\n",
      "train: 0.32700268617592615 0.2639205686204722\n",
      "val: (0.3285146818772368, 0.2645363152434421)\n",
      "train: 0.3254896275537991 0.2626031154046964\n",
      "val: (0.32762547866877, 0.2636101865336374)\n",
      "train: 0.32434192079169266 0.2616129669465585\n",
      "val: (0.32613195586761656, 0.26249050872778207)\n",
      "train: 0.3228735070494289 0.260484095020141\n",
      "val: (0.3249266897878464, 0.26154364467562063)\n",
      "train: 0.32182558082684704 0.25955071925898293\n",
      "val: (0.3236372954520462, 0.2605702283795002)\n",
      "train: 0.31922414656747733 0.2579432275128025\n",
      "val: (0.3209289950665812, 0.2588591070939328)\n",
      "train: 0.31766308126825044 0.2567206451486301\n",
      "val: (0.31952547922406027, 0.2577161236535159)\n",
      "train: 0.3165088370637724 0.25573801314197364\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val: (0.3185912365730198, 0.2568602775912711)\n",
      "train: 0.31522144842513217 0.2547484975144194\n",
      "val: (0.3167973333588779, 0.2556423593375029)\n",
      "train: 0.31390536915517375 0.2537588278728625\n",
      "val: (0.3160913090822942, 0.2549506447822245)\n",
      "train: 0.31305066658829206 0.25300238552975785\n",
      "val: (0.3149386225274137, 0.25411759649090127)\n",
      "train: 0.3113081338011834 0.2518546485262163\n",
      "val: (0.31196095812316826, 0.2523439658212948)\n",
      "train: 0.3091595491342864 0.25040802216963165\n",
      "val: (0.3109048624455329, 0.2514460184977888)\n",
      "train: 0.3077523706394613 0.2493967609691217\n",
      "val: (0.3090168882121285, 0.25019495904777006)\n",
      "train: 0.306505124727177 0.2483934728356667\n",
      "val: (0.3084312331946184, 0.2495195232514999)\n",
      "train: 0.3059113098179588 0.24773954765406542\n",
      "val: (0.30787890733138834, 0.24889236377987303)\n",
      "train: 0.30529208666342855 0.2470908850984687\n",
      "val: (0.3072481368545186, 0.2482389756009523)\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "K=7 score: 0.07734997638804927, 0.8260714229701096\n",
      "val: (0.801995367724474, 0.6676676668710564)\n",
      "train: 0.650386106584583 0.5128017521735402\n",
      "val: (0.5621241455396109, 0.43894412426198304)\n",
      "train: 0.516327409948219 0.4048695918876577\n",
      "val: (0.4858401428764755, 0.3850025819711762)\n",
      "train: 0.4565805377713042 0.3654843323982891\n",
      "val: (0.441214295106707, 0.3568272578122297)\n",
      "train: 0.42252002361684976 0.3450683985400751\n",
      "val: (0.4139766621734623, 0.3408119149285178)\n",
      "train: 0.39760208506100214 0.3306562310559265\n",
      "val: (0.3927517644758408, 0.3281996004092281)\n",
      "train: 0.3805253007415789 0.3194907918877207\n",
      "val: (0.37331103286061285, 0.3109743301674887)\n",
      "train: 0.36203968574222783 0.29300251788645654\n",
      "val: (0.3569071732756177, 0.28682625078298724)\n",
      "train: 0.35144482707491764 0.28291875591088317\n",
      "val: (0.3476208359921503, 0.28123361447147516)\n",
      "train: 0.34444735370030843 0.27804873398227287\n",
      "val: (0.3438778227485113, 0.2783364969407936)\n",
      "train: 0.34108964241188516 0.27550236101267656\n",
      "val: (0.34125883195291623, 0.2763855837595385)\n",
      "train: 0.3356561351090586 0.2722152885034674\n",
      "val: (0.3353935899680034, 0.27244098827528246)\n",
      "train: 0.3327068722398795 0.26991065605156367\n",
      "val: (0.33399940702141895, 0.27105037318920305)\n",
      "train: 0.3311346883270681 0.2685308466359423\n",
      "val: (0.33236256317147744, 0.26961661046497926)\n",
      "train: 0.32894604025437385 0.2668905830654291\n",
      "val: (0.3305041552440346, 0.2681715884593556)\n",
      "train: 0.32718589304290413 0.26553577155192837\n",
      "val: (0.3291624441917679, 0.2670335568715068)\n",
      "train: 0.326156033214286 0.2645593055997254\n",
      "val: (0.3283321473195266, 0.26620669326430346)\n",
      "train: 0.3250926188721434 0.26357883039866886\n",
      "val: (0.3274911680083053, 0.26542071964998376)\n",
      "train: 0.3242605958915648 0.2628066618063414\n",
      "val: (0.32668975966529, 0.26477486154141555)\n",
      "train: 0.323096162188902 0.26191793760825605\n",
      "val: (0.32544455809450146, 0.2637545137598542)\n",
      "train: 0.32208755351968854 0.2610447798446053\n",
      "val: (0.3247733171732506, 0.26310695052897093)\n",
      "train: 0.32092732551090997 0.26018819150632094\n",
      "val: (0.3235575866176415, 0.26225923228226244)\n",
      "train: 0.31878005212500765 0.25903494037744096\n",
      "val: (0.3215946722302454, 0.2611277287800344)\n",
      "train: 0.3178447104737428 0.25825925627719193\n",
      "val: (0.32060117838425045, 0.2603359837121596)\n",
      "train: 0.3166960916669775 0.25739873842186745\n",
      "val: (0.3188578884343343, 0.2592177155845085)\n",
      "train: 0.31538019693606106 0.2564549205196121\n",
      "val: (0.31815498951981436, 0.2585308859151024)\n",
      "train: 0.3144321614933822 0.25566787361641524\n",
      "val: (0.3170030774932899, 0.25767768566705884)\n",
      "train: 0.31360346361202934 0.25494574493218813\n",
      "val: (0.3163815589468796, 0.2570819495763544)\n",
      "train: 0.3128170053740885 0.2542387949225769\n",
      "val: (0.3155402058127704, 0.25632587418409547)\n",
      "train: 0.3117986897851069 0.2533674597998575\n",
      "val: (0.3145585430184038, 0.2554862886825576)\n",
      "train: 0.3109097960261235 0.2526035642987651\n",
      "val: (0.31359911821178404, 0.2546758478114861)\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "K=8 score: 0.07745918960526957, 0.8848865260956845\n",
      "val: (0.8061676695088553, 0.6739927978984799)\n",
      "train: 0.6455730030554387 0.5104625449843436\n",
      "val: (0.5540888140487301, 0.4346134433710272)\n",
      "train: 0.5099570795354224 0.4015224035307533\n",
      "val: (0.475707325441045, 0.3789017453320452)\n",
      "train: 0.4506244808926728 0.3623050517767416\n",
      "val: (0.43602431929351415, 0.35401214875745224)\n",
      "train: 0.4186799658132627 0.3429554648376031\n",
      "val: (0.4116355480097629, 0.33937952843731534)\n",
      "train: 0.39854364068873455 0.3312391886821372\n",
      "val: (0.39519607491821107, 0.3294836166712751)\n",
      "train: 0.38645539883325 0.3241124661857152\n",
      "val: (0.38477688565711765, 0.32296057710687054)\n",
      "train: 0.37648519063590147 0.3178212069779992\n",
      "val: (0.37461263474664047, 0.3153163800035333)\n",
      "train: 0.36543184225408754 0.30378713332888496\n",
      "val: (0.3562423501282745, 0.2867937684180396)\n",
      "train: 0.3530991733247375 0.2823804286157438\n",
      "val: (0.3505300251541589, 0.2812329180572769)\n",
      "train: 0.3482903611928025 0.27885175591087064\n",
      "val: (0.3470248200637967, 0.2788633883964078)\n",
      "train: 0.34255019482611526 0.27574052770981106\n",
      "val: (0.34015562929790283, 0.2748464265749916)\n",
      "train: 0.3379499531769428 0.2728604050895647\n",
      "val: (0.33801046619822317, 0.2730849898995018)\n",
      "train: 0.3354282689166067 0.2710935171923806\n",
      "val: (0.3346766239940163, 0.2710738787411052)\n",
      "train: 0.33064438928198814 0.2683645583964179\n",
      "val: (0.32975954557157283, 0.2681582288707176)\n",
      "train: 0.3263606295456229 0.26577962792353027\n",
      "val: (0.32679379315397233, 0.2662476899630955)\n",
      "train: 0.3249562487963731 0.26457728184312385\n",
      "val: (0.32619219819040574, 0.2655231722477171)\n",
      "train: 0.32430997406924483 0.2638409937429211\n",
      "val: (0.32565792044437425, 0.26492337036962266)\n",
      "train: 0.323642698211081 0.2631635484664559\n",
      "val: (0.3250199521588701, 0.2643256882056232)\n",
      "train: 0.32255271949656344 0.26232145540664703\n",
      "val: (0.32336619241763787, 0.26326445084145095)\n",
      "train: 0.32098716791822396 0.26120123975459286\n",
      "val: (0.3224529641613755, 0.2624412952033419)\n",
      "train: 0.3202167243349305 0.2604976692193479\n",
      "val: (0.32111520277190436, 0.261482123934497)\n",
      "train: 0.3185888664371391 0.2592855323538376\n",
      "val: (0.31965202312603974, 0.2604061986233648)\n",
      "train: 0.317086588699005 0.25822764127393094\n",
      "val: (0.3183890358047964, 0.2594480320266975)\n",
      "train: 0.31591677573712823 0.2573300659583642\n",
      "val: (0.31742294964352624, 0.2586263246642385)\n",
      "train: 0.314498922180983 0.2563691536695918\n",
      "val: (0.3160822390883842, 0.2576859101797559)\n",
      "train: 0.3135165757793274 0.2555682204080693\n",
      "val: (0.31536769295778694, 0.2570089133909539)\n",
      "train: 0.3124662498407667 0.2547638461129593\n",
      "val: (0.3141319081548546, 0.256090451893926)\n",
      "train: 0.3116053635692488 0.2540371409766856\n",
      "val: (0.31358699713886423, 0.2554669362425403)\n",
      "train: 0.3109530443138851 0.25342082730156945\n",
      "val: (0.3128152524429319, 0.2548198034545963)\n",
      "train: 0.3095651249826184 0.25246426355294144\n",
      "val: (0.31073103952357634, 0.2535182655296333)\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "K=9 score: 0.0775127720211254, 0.8685120311677635\n",
      "val: (0.8230803759483776, 0.6907590997802333)\n",
      "train: 0.6711459561490023 0.5314696838473839\n",
      "val: (0.5849703054455971, 0.4563220601998776)\n",
      "train: 0.5419774716966437 0.4228583729799825\n",
      "val: (0.5087417848675113, 0.3993138729446818)\n",
      "train: 0.47575992571803366 0.3763277381824196\n",
      "val: (0.45488459447474044, 0.3631284534109246)\n",
      "train: 0.4344784920846782 0.3494888829285169\n",
      "val: (0.42375419617868193, 0.34320265592450316)\n",
      "train: 0.41137321784712466 0.3351479368793694\n",
      "val: (0.4065910959811902, 0.3325081345526607)\n",
      "train: 0.39567089931704497 0.32542911710249894\n",
      "val: (0.39378606719636317, 0.32413499988432265)\n",
      "train: 0.38539214305648667 0.3186266681170294\n",
      "val: (0.3842116504674512, 0.31769380540866127)\n",
      "train: 0.3760412290750381 0.31195692233194666\n",
      "val: (0.3744767357214079, 0.3102274840682384)\n",
      "train: 0.367758456838069 0.3053390091825931\n",
      "val: (0.36546059177705764, 0.30303781667586605)\n",
      "train: 0.3594646711385844 0.2983497392291782\n",
      "val: (0.35737459156558526, 0.29561156696451896)\n",
      "train: 0.3523149793534489 0.2913447741987229\n",
      "val: (0.3514888755601837, 0.2896955104926588)\n",
      "train: 0.3461928575063714 0.2851840572463984\n",
      "val: (0.3428483502536774, 0.2828518829604461)\n",
      "train: 0.3376617811096589 0.27801322321651406\n",
      "val: (0.33675295789060605, 0.27777397478547805)\n",
      "train: 0.3323906636780265 0.27358084239095465\n",
      "val: (0.3317663589099338, 0.27347438285541936)\n",
      "train: 0.3289067469678804 0.2704258487042638\n",
      "val: (0.3298339723933461, 0.27110832682791575)\n",
      "train: 0.3265877912390714 0.2682073359614136\n",
      "val: (0.3278015251564302, 0.2691006887049678)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 0.32331292506124537 0.2657472934009049\n",
      "val: (0.32419123879794853, 0.2666890371006366)\n",
      "train: 0.32097695243266433 0.2638924211319975\n",
      "val: (0.32247483442260083, 0.26526458928285607)\n",
      "train: 0.3190485979243796 0.262288048359859\n",
      "val: (0.32073307908675036, 0.26383211450031435)\n",
      "train: 0.317536683507393 0.2609325712053921\n",
      "val: (0.31962881035822616, 0.26274861977716807)\n",
      "train: 0.31631455885052556 0.25972686902271636\n",
      "val: (0.3184741732181556, 0.26162881915663183)\n",
      "train: 0.31496229631605255 0.2586323471119559\n",
      "val: (0.31704241950448064, 0.2605538703392519)\n",
      "train: 0.31355984369117157 0.25757265214842656\n",
      "val: (0.31598799535281513, 0.2596443682457096)\n",
      "train: 0.31294266448266206 0.2568672169919224\n",
      "val: (0.31546644696157855, 0.25898499118774054)\n",
      "train: 0.312263735152001 0.256175969627538\n",
      "val: (0.3146020807604743, 0.25823526023155463)\n",
      "train: 0.3108375199680039 0.2552138650785716\n",
      "val: (0.312880265021581, 0.25714074125606656)\n",
      "train: 0.3097551884039399 0.25431586414072027\n",
      "val: (0.31234560503802367, 0.25652839901503277)\n",
      "train: 0.30911825703866247 0.2536694264778989\n",
      "val: (0.31167985703277806, 0.255859606196665)\n",
      "train: 0.30836789398131287 0.2529662816560417\n",
      "val: (0.31107932481842687, 0.25524188900686906)\n",
      "train: 0.30777029676456685 0.2523556901527119\n",
      "val: (0.3105186482361903, 0.2546328722462)\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "K=10 score: 0.07757462869403448, 0.88573214740864\n"
     ]
    }
   ],
   "source": [
    "for k_ in range(2,11):\n",
    "    model = Model2(users, tasks, k=k_)\n",
    "    user_vec, user_bias = train(train_df, validate_df, test_df, model)\n",
    "    user_features = np.zeros((user_vec.numpy().shape[0], user_vec.numpy().shape[1]+1))\n",
    "    user_features[:, :user_vec.numpy().shape[1]] = user_vec.numpy()\n",
    "    user_features[:, -1] = user_bias.numpy().reshape(-1)\n",
    "    kmeans = KMeans(n_clusters=400, random_state=0).fit(user_features)\n",
    "    delta_matrices_clust = calc_cluster_matrix_dist(kmeans.labels_, delta_matrices_all)\n",
    "    pd = [[pairwise_distances(delta_matrices_clust[i,j], delta_matrices_all[i,j]) for i in range(delta_matrices_all.shape[0])] for j in range(delta_matrices_all.shape[0])]\n",
    "    avg_dist = np.average(pd)\n",
    "    max_dist = np.max(pd)\n",
    "    print(\"K={} score: {}, {}\".format(k_, avg_dist, max_dist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.isnan(user_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val: (0.4639765714452453, 0.3785863400954292)\n",
      "train: 0.3976220150586197 0.30140647537784343\n",
      "val: (0.3912996700760621, 0.2963129679616412)\n",
      "train: 0.3863972378421258 0.29263850707824146\n",
      "val: (0.38062887830351594, 0.28842775033556545)\n",
      "train: 0.3753557607375847 0.2843337796525777\n",
      "val: (0.3697756650199848, 0.28035144153860714)\n",
      "train: 0.3643174652003573 0.27597536014955326\n",
      "val: (0.3590453509619917, 0.2723341927365815)\n",
      "train: 0.3535626744462106 0.26779745229453233\n",
      "val: (0.34869543649570905, 0.264581396840703)\n",
      "train: 0.34330981102246094 0.2599818534112178\n",
      "val: (0.3389087563180287, 0.2572293087311685)\n",
      "train: 0.3337010423494056 0.25264784855715905\n",
      "val: (0.3297905049900379, 0.2503603597860713)\n",
      "train: 0.3248055021277796 0.24586431130356076\n",
      "val: (0.3213801578220503, 0.24403993506454463)\n",
      "train: 0.3166362003742733 0.2396479972276676\n",
      "val: (0.31367251720931305, 0.23824667026333188)\n",
      "train: 0.3091720462633454 0.23399006022432742\n",
      "val: (0.3066401457352175, 0.23295336697792132)\n",
      "train: 0.3023767937079633 0.22885895719219176\n",
      "val: (0.3002479145196121, 0.22815762218018776)\n",
      "train: 0.2962093342102625 0.22422436814756858\n",
      "val: (0.29445761829379974, 0.22382580373955874)\n",
      "train: 0.2906263380780068 0.22004541167852718\n",
      "val: (0.2892275640651235, 0.21992493155386955)\n",
      "train: 0.2855824049970066 0.21628881197484032\n",
      "val: (0.28451207556066554, 0.21641951143612204)\n",
      "train: 0.28103062824478703 0.21291412868304696\n",
      "val: (0.280262981547239, 0.2132636539937473)\n",
      "train: 0.27692402049155473 0.20988495860922968\n",
      "val: (0.2764324734642822, 0.2104250427074148)\n",
      "train: 0.27321737363634885 0.20716450759447802\n",
      "val: (0.2729756848259797, 0.20787362767180873)\n",
      "train: 0.2698688867755594 0.20472003365653577\n",
      "val: (0.2698521740077502, 0.20557676452465942)\n",
      "train: 0.2668410770407015 0.20251890633995664\n",
      "val: (0.2670262869784658, 0.20350422611326704)\n",
      "train: 0.26410091510597605 0.200533484191153\n",
      "val: (0.2644669085209692, 0.20163290711160103)\n",
      "train: 0.26161936748342196 0.19874220952497634\n",
      "val: (0.26214682109223414, 0.19994171638174926)\n",
      "train: 0.25937063650767356 0.1971249470148499\n",
      "val: (0.2600419220156859, 0.19841104263019121)\n",
      "train: 0.2573314202560684 0.19566402915267214\n",
      "val: (0.25813050511745367, 0.19702333832138227)\n",
      "train: 0.255480409285528 0.19434263010908095\n",
      "val: (0.2563927989808734, 0.19576303908936546)\n",
      "train: 0.25379807323558196 0.19314900158398043\n",
      "val: (0.2548107770201743, 0.1946209596866189)\n",
      "train: 0.25226664607469584 0.19206940602526076\n",
      "val: (0.25336813005889686, 0.193584710326676)\n",
      "train: 0.2508701722372939 0.19109063684980093\n",
      "val: (0.25205030088218, 0.19264201415621968)\n",
      "train: 0.24959451123389131 0.19020139832975547\n",
      "val: (0.25084444496044306, 0.19177946054740536)\n",
      "train: 0.24842726067062 0.18939136928084058\n",
      "val: (0.24973932849184832, 0.1909916480300388)\n",
      "train: 0.2473576191321601 0.1886536563722199\n",
      "val: (0.24872519253193107, 0.1902703371912059)\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "K=2 score: 0.07877282330287519, 0.9802137250832278\n",
      "val: (0.4703552103176433, 0.38246591113760764)\n",
      "train: 0.4033145440000841 0.30426724423542983\n",
      "val: (0.397846758477839, 0.299940861763185)\n",
      "train: 0.3935202271267529 0.2964976240378478\n",
      "val: (0.388759241233882, 0.29323709013582455)\n",
      "train: 0.3839321448658858 0.28930543848756896\n",
      "val: (0.379361496985835, 0.2862361628668172)\n",
      "train: 0.3741803294688622 0.2819417758238957\n",
      "val: (0.36987847194033074, 0.27915878160794655)\n",
      "train: 0.36447703274119275 0.27459452990169203\n",
      "val: (0.3605052462998501, 0.27213768423162366)\n",
      "train: 0.3549971940259997 0.2673959241341013\n",
      "val: (0.3514050260481682, 0.265308428650456)\n",
      "train: 0.34588166120029556 0.2604610848775992\n",
      "val: (0.3427077762365239, 0.2587484342611527)\n",
      "train: 0.33723713261152694 0.2538844668726727\n",
      "val: (0.33450481339600585, 0.25254525918507587)\n",
      "train: 0.3291337187428399 0.24771501272651766\n",
      "val: (0.3268475464889087, 0.24674104600336272)\n",
      "train: 0.3216059347345928 0.24199228257225747\n",
      "val: (0.31975295523689873, 0.24134883997845116)\n",
      "train: 0.3146585806478616 0.23672111342686344\n",
      "val: (0.31321230520669985, 0.23637499349743715)\n",
      "train: 0.30827467690879007 0.23188692938392297\n",
      "val: (0.3072002924670821, 0.2318123584920952)\n",
      "train: 0.3024227685478872 0.22747256527640333\n",
      "val: (0.30168268108667046, 0.22763732434252543)\n",
      "train: 0.29706346381908155 0.22345244640168904\n",
      "val: (0.2966217705741517, 0.22383134583472533)\n",
      "train: 0.2921550270617185 0.21979178860617934\n",
      "val: (0.2919798681238112, 0.22035572364801556)\n",
      "train: 0.2876571605044927 0.2164553236009254\n",
      "val: (0.28772126354342914, 0.2171828993969105)\n",
      "train: 0.28353288834955465 0.21341591835247245\n",
      "val: (0.28381311305180107, 0.21428090093678562)\n",
      "train: 0.2797491996103253 0.21064440942986004\n",
      "val: (0.2802257364413627, 0.21162675544699816)\n",
      "train: 0.2762770834933866 0.2081133925060714\n",
      "val: (0.27693267702068264, 0.2091959153972686)\n",
      "train: 0.2730912059246357 0.2058002385064436\n",
      "val: (0.2739105008313804, 0.2069731045126447)\n",
      "train: 0.2701692665729054 0.203684922458464\n",
      "val: (0.2711382984201955, 0.20494492260974576)\n",
      "train: 0.26749114803679425 0.20175096784043758\n",
      "val: (0.2685969374221763, 0.2030909229842502)\n",
      "train: 0.26503807663874457 0.199987174393986\n",
      "val: (0.26626835121884096, 0.20140041082014032)\n",
      "train: 0.2627920423479248 0.19838018902225885\n",
      "val: (0.26413510819314134, 0.19986011948895807)\n",
      "train: 0.26073556207184406 0.19691523644044537\n",
      "val: (0.262180341570524, 0.1984604238417054)\n",
      "train: 0.25885173168940245 0.1955793161142291\n",
      "val: (0.26038789271357654, 0.1971845183450211)\n",
      "train: 0.2571244262345002 0.19436229528215737\n",
      "val: (0.2587425374403642, 0.19601932697554963)\n",
      "train: 0.25553855003408815 0.1932502523952455\n",
      "val: (0.2572301573553171, 0.19495575941462243)\n",
      "train: 0.25408021808951353 0.19223427840435223\n",
      "val: (0.2558378761450943, 0.1939852255328834)\n",
      "train: 0.25273686708063625 0.19130666953171555\n",
      "val: (0.25455408008164593, 0.1930975154662871)\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "K=3 score: 0.0787646798987359, 0.912367894807308\n",
      "val: (0.4886040660910888, 0.3980651841716661)\n",
      "train: 0.4149160509266538 0.31249234334740866\n",
      "val: (0.41011809979701336, 0.307920447428203)\n",
      "train: 0.40701595917139605 0.3059416417299164\n",
      "val: (0.4031407271605519, 0.30272291370855026)\n",
      "train: 0.3995229459977225 0.3003254148888821\n",
      "val: (0.3957683982235824, 0.2971568579498193)\n",
      "train: 0.3917032445729872 0.2944046177662389\n",
      "val: (0.3881272831852003, 0.29135075059908894)\n",
      "train: 0.383691217872575 0.28828765970533454\n",
      "val: (0.3803487671382227, 0.28541744805630953)\n",
      "train: 0.3756175094878218 0.2820996879129979\n",
      "val: (0.37255827714084017, 0.2794481910004681)\n",
      "train: 0.36759980063355513 0.27593614173769\n",
      "val: (0.3648649832369932, 0.27355689067782757)\n",
      "train: 0.3597362376343945 0.2698761406952988\n",
      "val: (0.35735590700953246, 0.2678047762050723)\n",
      "train: 0.35210385929280186 0.263984856775081\n",
      "val: (0.35009571849742294, 0.2622436126217115)\n",
      "train: 0.3447595597756592 0.2583156367138578\n",
      "val: (0.34312944296830344, 0.25692889161489174)\n",
      "train: 0.3377413426402135 0.25291362753334234\n",
      "val: (0.3364858011373984, 0.2518882598868811)\n",
      "train: 0.3310711074032666 0.24779814556936733\n",
      "val: (0.33018062230025125, 0.24713235663152122)\n",
      "train: 0.32475882740130185 0.24297357776950548\n",
      "val: (0.32421952276340626, 0.24265608742804165)\n",
      "train: 0.3188059912696988 0.23844741656879362\n",
      "val: (0.3186002325197567, 0.23845327438885697)\n",
      "train: 0.31320769538856935 0.23421037281259213\n",
      "val: (0.31331511355627256, 0.2345081391828022)\n",
      "train: 0.3079543965572331 0.2302552002795594\n",
      "val: (0.30835339832032754, 0.23081873643627235)\n",
      "train: 0.3030337148436183 0.2265688614259698\n",
      "val: (0.30370241854192087, 0.22737350627149394)\n",
      "train: 0.2984317691666632 0.22314014799293277\n",
      "val: (0.2993482371158551, 0.22416598827877324)\n",
      "train: 0.29413372749793365 0.21995522051870744\n",
      "val: (0.2952760908956793, 0.22118189236761712)\n",
      "train: 0.2901239279749771 0.21699875647350383\n",
      "val: (0.2914708578982157, 0.21840319492066992)\n",
      "train: 0.2863861753380719 0.21425718524923532\n",
      "val: (0.2879175078167603, 0.21581628098151892)\n",
      "train: 0.28290423145499166 0.21171614813766199\n",
      "val: (0.28460142415906153, 0.21341666818005803)\n",
      "train: 0.2796622083185169 0.20936302430579906\n",
      "val: (0.28150846926968526, 0.21119285178998934)\n",
      "train: 0.27664470751721204 0.2071841252663762\n",
      "val: (0.2786248905817204, 0.209127320160325)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-172-25a7c494148a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0musers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0muser_vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_bias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0muser_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0muser_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0muser_vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muser_vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-169-25ffd40bdceb>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_iter, val_iter, test_iter, model)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;31m#if i % 1000==0:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;31m#    print (loss.data[0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0mavg_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mavg_l1\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcrit2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrating\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/python3/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \"\"\"\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/python3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 98\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for k_ in range(2,11):\n",
    "    model = Model(users, tasks, k=k_)\n",
    "    user_vec, user_bias = train(train_df, validate_df, test_df, model)\n",
    "    user_features = np.zeros((user_vec.numpy().shape[0], user_vec.numpy().shape[1]+1))\n",
    "    user_features[:, :user_vec.numpy().shape[1]] = user_vec.numpy()\n",
    "    user_features[:, -1] = user_bias.numpy().reshape(-1)\n",
    "    kmeans = KMeans(n_clusters=400, random_state=0).fit(user_features)\n",
    "    delta_matrices_clust = calc_cluster_matrix_dist(kmeans.labels_, delta_matrices_all)\n",
    "    pd = [[pairwise_distances(delta_matrices_clust[i,j], delta_matrices_all[i,j]) for i in range(delta_matrices_all.shape[0])] for j in range(delta_matrices_all.shape[0])]\n",
    "    avg_dist = np.average(pd)\n",
    "    max_dist = np.max(pd)\n",
    "    print(\"K={} score: {}, {}\".format(k_, avg_dist, max_dist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kmeans = SpectralClustering(n_clusters=200).fit(user_features)\n",
    "# delta_matrices_clust = calc_cluster_matrix_dist(kmeans.labels_, delta_matrices_all)\n",
    "# avg_dist = np.average([[pairwise_distances(delta_matrices_clust[i,j], delta_matrices_all[i,j]) for i in range(1000)] for j in range(1000)])\n",
    "# print(\"K={} score: {}\".format(k_, avg_dist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 44, 168, 196,  93, 168, 166, 182, 113, 166,  25, 110,  63,   5,\n",
       "       173, 125, 155,  81, 101,  28,  75,  89,  22,  98, 166, 193,  92,\n",
       "         6, 119,   6,  70, 166, 156,  91, 174, 195,  69, 152,  45,  12,\n",
       "        40, 195,  54, 194, 161,  90, 109, 169,   8,   9,  52, 103, 194,\n",
       "         4, 168,   9, 170, 138, 100, 154,  31,  33,  55,   9,   1, 162,\n",
       "         9,  29, 127, 172,  56,  50,  78, 185, 111,  18, 142,  30,  94,\n",
       "        96,   9,   1, 143, 158, 166,   1,   4,  67, 175,  74, 102,   6,\n",
       "        88,  58, 106, 161,  38, 126,  37,  46,  83, 164,   3, 181,  43,\n",
       "       124,  77, 188, 192, 140,   9, 134, 193, 163,  14,  19, 179, 184,\n",
       "       178, 116, 129,  79, 169,  47,  72,  15, 121, 193, 149,   0,  59,\n",
       "        10,  60,  42,  11,  61,  26, 170,  39,  86, 152,  82, 192, 160,\n",
       "         2, 190,  73,  35, 118, 177, 168, 191, 196, 173, 168, 167, 148,\n",
       "       184, 191, 168,  97, 177, 137, 199, 186,  71,  80, 192, 174, 199,\n",
       "       168, 152, 150, 117, 181, 183, 114, 152, 192,  16, 152, 130,   4,\n",
       "       176, 136, 197,  20, 188, 189, 187, 171,  62,  51, 195,  17, 153,\n",
       "       187, 176, 131, 192,  87,  49,   1, 175,  76,  24,  53,   2, 195,\n",
       "       120, 135, 191, 177, 128,   9,   9, 191,   6, 188,   9, 147, 112,\n",
       "        68, 187, 152, 157, 152,   1, 198,  85,  65, 177, 188, 146, 107,\n",
       "       145,   9, 179, 193, 189,   1, 104, 190,  64,  84, 151, 195, 195,\n",
       "       183, 187,  66,   9, 171, 132,   9, 144, 169, 193, 177, 123, 198,\n",
       "       173, 196, 192,  34,   6,   1, 178,  48, 179, 159,   4, 175,   9,\n",
       "         9,   2, 176,  32, 198, 194,   9, 198, 168, 191, 115,  27, 194,\n",
       "        13, 181, 174,  57, 171,  41, 191, 172, 165,   2,   9, 188, 141,\n",
       "       190, 193, 174,   9, 168, 196,  21, 197,  23, 187, 173, 152,   4,\n",
       "         6, 190,  36,  95, 184,   9, 152, 179, 169, 170,  99, 133, 174,\n",
       "         9, 122, 170, 168, 175,   1,   9, 139, 193, 199,   7,   9, 105,\n",
       "         9, 172, 178, 173, 196, 108, 170,   9,   9,   9, 192,   4,   9,\n",
       "       180, 195,   9, 171, 197, 197, 171,   9,   9, 178,   9, 183, 186,\n",
       "         9, 152, 181,   9,   9,   9, 161,   9, 180,   9, 161,   9, 161,\n",
       "         9,   1,   9, 175, 186,   9, 179,   9,   9,   9, 181, 176], dtype=int32)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [-1.26082933]\n",
      "1 [[-0.39793867  0.67628181  1.1686765  -0.7427668 ]\n",
      " [ 0.71048403 -0.78146476 -0.29446998 -0.21856749]\n",
      " [-1.30848157  0.59841329  0.31170163 -0.07004142]\n",
      " ..., \n",
      " [-0.12140504 -0.25585377 -0.11752333  0.05447096]\n",
      " [ 0.2422477  -0.65883136 -0.6978479  -0.34982434]\n",
      " [-0.92756301 -1.06573153 -1.64558303 -0.55912602]]\n",
      "2 [[ 0.3497223   0.11227955  0.42520446  0.30817065]\n",
      " [ 0.0609895  -0.10946952  0.16272642 -0.17210275]\n",
      " [-0.10992495 -0.036178    0.12413357 -0.42531428]\n",
      " ..., \n",
      " [ 0.59692997  0.90453321  0.41128317  0.60188913]\n",
      " [ 0.5215283   0.32531875 -1.06333375 -0.93270755]\n",
      " [ 1.04257977  1.01841938  1.00617087 -0.80307019]]\n",
      "3 [[ 0.94279188]\n",
      " [-0.91361088]\n",
      " [-1.17880964]\n",
      " ..., \n",
      " [ 0.34308255]\n",
      " [ 0.81045347]\n",
      " [-0.61459929]]\n",
      "4 [[-1.00959754]\n",
      " [-0.78255588]\n",
      " [-0.95955992]\n",
      " ..., \n",
      " [ 0.12014688]\n",
      " [ 0.3620224 ]\n",
      " [ 0.47548157]]\n"
     ]
    }
   ],
   "source": [
    "for i, param in enumerate(model.parameters()):\n",
    "    np.savetxt('./test_{}'.format(i), param.data.numpy())\n",
    "    print(i, param.data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./test_pd.pkl', 'wb') as f:\n",
    "    pickle.dump(pd, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.03850098 -0.00070619 -0.00251558  0.00492479 -0.00810864]\n",
      " [-0.01367145  0.00419112  0.0025168   0.00543623  0.00199566]\n",
      " [-0.00511168  0.00354283  0.00876775  0.00368143  0.0028533 ]\n",
      " [-0.00517798  0.00571605  0.00293797  0.0069204   0.00460551]\n",
      " [-0.01270993  0.00311497  0.00271329  0.00537838  0.01720596]]\n",
      "[[ 0.18454485 -0.05865035 -0.00266644 -0.11463426 -0.01521456]\n",
      " [-0.05865035  0.15605807 -0.00128753 -0.08659634 -0.010848  ]\n",
      " [-0.00266644 -0.00128753  0.02146133 -0.00528638  0.00181503]\n",
      " [-0.11463426 -0.08659634 -0.00528638  0.21334369 -0.02351102]\n",
      " [-0.01521456 -0.010848    0.00181503 -0.02351102  0.05835177]]\n"
     ]
    }
   ],
   "source": [
    "print(delta_matrices_clust[0,0]) \n",
    "print(delta_matrices_all[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.25971494,  0.18266406,  0.04541026,  0.23844652,  0.07674275],\n",
       "       [ 0.24078773,  0.18367389,  0.02500055,  0.24974557,  0.06513833],\n",
       "       [ 0.23300941,  0.18591987,  0.01649054,  0.25489434,  0.06462982],\n",
       "       [ 0.23523939,  0.18564499,  0.02356406,  0.25292491,  0.06473865],\n",
       "       [ 0.24126224,  0.18645239,  0.02867682,  0.2518097 ,  0.05224576]])"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairwise_distances(delta_matrices_clust[0,0], delta_matrices_all[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.021972585625869595"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.average(np.abs(delta_matrices_clust - delta_matrices_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZQAAAD8CAYAAABQFVIjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1128c7c88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(np.abs(delta_matrices_clust - delta_matrices_all).flatten())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZQAAAD8CAYAAABQFVIjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUadjSeIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x116aeeef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(np.abs(delta_matrices_clust_ref - delta_matrices_all).flatten(), alpha=.5)\n",
    "plt.hist(np.abs(delta_matrices_clust - delta_matrices_all).flatten(), alpha=.5, color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.60110056142430368"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(np.abs(delta_matrices_clust - delta_matrices_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.68299466548434962"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(np.abs(delta_matrices_clust_ref - delta_matrices_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 0.19249209, -0.07486806, -0.05794249, -0.06922621],\n",
       "         [-0.07486806,  0.09484536, -0.00765522, -0.00936041],\n",
       "         [-0.05794249, -0.00765522,  0.07604623, -0.00699209],\n",
       "         [-0.06922621, -0.00936041, -0.00699209,  0.08870529]],\n",
       "\n",
       "        [[-0.07679419,  0.00593934,  0.02490498,  0.00268276],\n",
       "         [-0.0766963 ,  0.01976156,  0.01926943,  0.01926943],\n",
       "         [-0.03249219,  0.02030838,  0.01992561,  0.04214783],\n",
       "         [-0.069369  ,  0.01994383,  0.04171038,  0.01948816]],\n",
       "\n",
       "        [[-0.23537528, -0.0121463 ,  0.00132952, -0.04707506],\n",
       "         [ 0.05794702,  0.0209412 ,  0.03557094,  0.02825607],\n",
       "         [-0.01048565,  0.02554686,  0.03692555,  0.0312362 ],\n",
       "         [-0.02041943,  0.02247642,  0.03602248,  0.02924945]],\n",
       "\n",
       "        ..., \n",
       "        [[-0.07948675, -0.07948675, -0.07948675, -0.22147351],\n",
       "         [ 0.04104305,  0.04104305,  0.04104305,  0.01958609],\n",
       "         [ 0.04581126,  0.04581126,  0.04581126,  0.02912252],\n",
       "         [ 0.04263245,  0.04263245,  0.04263245,  0.0227649 ]],\n",
       "\n",
       "        [[-0.26019717, -0.1311183 , -0.00203943, -0.06657887],\n",
       "         [ 0.0137342 ,  0.03324052,  0.05274684,  0.04299368],\n",
       "         [ 0.02457104,  0.03974262,  0.05491421,  0.04732842],\n",
       "         [ 0.01734648,  0.03540789,  0.0534693 ,  0.04443859]],\n",
       "\n",
       "        [[-0.34317644, -0.03891911, -0.03891911, -0.03891911],\n",
       "         [ 0.00119442,  0.0471736 ,  0.0471736 ,  0.0471736 ],\n",
       "         [ 0.01481788,  0.05057947,  0.05057947,  0.05057947],\n",
       "         [ 0.00573557,  0.04830889,  0.04830889,  0.04830889]]],\n",
       "\n",
       "\n",
       "       [[[-0.07679419, -0.0766963 , -0.03249219, -0.069369  ],\n",
       "         [ 0.00593934,  0.01976156,  0.02030838,  0.01994383],\n",
       "         [ 0.02490498,  0.01926943,  0.01992561,  0.04171038],\n",
       "         [ 0.00268276,  0.01926943,  0.04214783,  0.01948816]],\n",
       "\n",
       "        [[ 0.01313003, -0.01687371, -0.02110315, -0.02110315],\n",
       "         [-0.01687371,  0.02511498,  0.00364224,  0.00364224],\n",
       "         [-0.02110315,  0.00364224,  0.02915702,  0.00351599],\n",
       "         [-0.02110315,  0.00364224,  0.00351599,  0.02915702]],\n",
       "\n",
       "        [[-0.36197575, -0.03526004, -0.01667312, -0.07953801],\n",
       "         [ 0.12852228,  0.03128351,  0.03441112,  0.03284731],\n",
       "         [ 0.01851245,  0.03039735,  0.03415048,  0.03227392],\n",
       "         [ 0.01851245,  0.03039735,  0.03415048,  0.0679882 ]],\n",
       "\n",
       "        ..., \n",
       "        [[-0.12190367, -0.12190367, -0.12190367, -0.30630734],\n",
       "         [ 0.05791284,  0.05791284,  0.05791284,  0.05332569],\n",
       "         [ 0.05699541,  0.05699541,  0.05699541,  0.05149083],\n",
       "         [ 0.05699541,  0.05699541,  0.05699541,  0.05149083]],\n",
       "\n",
       "        [[-0.35659925, -0.18895955, -0.02131985, -0.1051397 ],\n",
       "         [ 0.05207465,  0.05624479,  0.06041493,  0.05832986],\n",
       "         [ 0.04998957,  0.05499374,  0.05999791,  0.05749583],\n",
       "         [ 0.04998957,  0.05499374,  0.05999791,  0.05749583]],\n",
       "\n",
       "        [[-0.46436763, -0.06921691, -0.06921691, -0.06921691],\n",
       "         [ 0.04939384,  0.05922346,  0.05922346,  0.05922346],\n",
       "         [ 0.04677261,  0.05856815,  0.05856815,  0.05856815],\n",
       "         [ 0.04677261,  0.05856815,  0.05856815,  0.05856815]]],\n",
       "\n",
       "\n",
       "       [[[-0.23537528,  0.05794702, -0.01048565, -0.02041943],\n",
       "         [-0.0121463 ,  0.0209412 ,  0.02554686,  0.02247642],\n",
       "         [ 0.00132952,  0.03557094,  0.03692555,  0.03602248],\n",
       "         [-0.04707506,  0.02825607,  0.0312362 ,  0.02924945]],\n",
       "\n",
       "        [[-0.36197575,  0.12852228,  0.01851245,  0.01851245],\n",
       "         [-0.03526004,  0.03128351,  0.03039735,  0.03039735],\n",
       "         [-0.01667312,  0.03441112,  0.03415048,  0.03415048],\n",
       "         [-0.07953801,  0.03284731,  0.03227392,  0.0679882 ]],\n",
       "\n",
       "        [[ 0.14783654, -0.11112325, -0.02589598, -0.06850962],\n",
       "         [-0.11112325,  0.13575771, -0.00136086, -0.01453234],\n",
       "         [-0.02589598, -0.00136086,  0.054464  ,  0.00251311],\n",
       "         [-0.06850962, -0.01453234,  0.00251311,  0.09975962]],\n",
       "\n",
       "        ..., \n",
       "        [[-0.0625    , -0.0625    , -0.0625    , -0.1875    ],\n",
       "         [ 0.02386364,  0.02386364,  0.02386364, -0.01477273],\n",
       "         [ 0.05113636,  0.05113636,  0.05113636,  0.03977273],\n",
       "         [ 0.0375    ,  0.0375    ,  0.0375    ,  0.0125    ]],\n",
       "\n",
       "        [[-0.22159091, -0.10795455,  0.00568182, -0.05113636],\n",
       "         [-0.02530992,  0.00981405,  0.04493802,  0.02737603],\n",
       "         [ 0.03667355,  0.04700413,  0.05733471,  0.05216942],\n",
       "         [ 0.00568182,  0.02840909,  0.05113636,  0.03977273]],\n",
       "\n",
       "        [[-0.29464286, -0.02678571, -0.02678571, -0.02678571],\n",
       "         [-0.04788961,  0.0349026 ,  0.0349026 ,  0.0349026 ],\n",
       "         [ 0.03003247,  0.05438312,  0.05438312,  0.05438312],\n",
       "         [-0.00892857,  0.04464286,  0.04464286,  0.04464286]]],\n",
       "\n",
       "\n",
       "       ..., \n",
       "       [[[-0.07948675,  0.04104305,  0.04581126,  0.04263245],\n",
       "         [-0.07948675,  0.04104305,  0.04581126,  0.04263245],\n",
       "         [-0.07948675,  0.04104305,  0.04581126,  0.04263245],\n",
       "         [-0.22147351,  0.01958609,  0.02912252,  0.0227649 ]],\n",
       "\n",
       "        [[-0.12190367,  0.05791284,  0.05699541,  0.05699541],\n",
       "         [-0.12190367,  0.05791284,  0.05699541,  0.05699541],\n",
       "         [-0.12190367,  0.05791284,  0.05699541,  0.05699541],\n",
       "         [-0.30630734,  0.05332569,  0.05149083,  0.05149083]],\n",
       "\n",
       "        [[-0.0625    ,  0.02386364,  0.05113636,  0.0375    ],\n",
       "         [-0.0625    ,  0.02386364,  0.05113636,  0.0375    ],\n",
       "         [-0.0625    ,  0.02386364,  0.05113636,  0.0375    ],\n",
       "         [-0.1875    , -0.01477273,  0.03977273,  0.0125    ]],\n",
       "\n",
       "        ..., \n",
       "        [[ 0.0552381 ,  0.00761905,  0.00761905, -0.03238095],\n",
       "         [ 0.00761905,  0.0552381 ,  0.00761905, -0.03238095],\n",
       "         [ 0.00761905,  0.00761905,  0.0552381 , -0.03238095],\n",
       "         [-0.03238095, -0.03238095, -0.03238095, -0.01714286]],\n",
       "\n",
       "        [[-0.02840909,  0.00795455,  0.04431818,  0.02613636],\n",
       "         [-0.02840909,  0.00795455,  0.04431818,  0.02613636],\n",
       "         [-0.02840909,  0.00795455,  0.04431818,  0.02613636],\n",
       "         [-0.11931818, -0.04659091,  0.02613636, -0.01022727]],\n",
       "\n",
       "        [[-0.05178571,  0.03392857,  0.03392857,  0.03392857],\n",
       "         [-0.05178571,  0.03392857,  0.03392857,  0.03392857],\n",
       "         [-0.05178571,  0.03392857,  0.03392857,  0.03392857],\n",
       "         [-0.16607143,  0.00535714,  0.00535714,  0.00535714]]],\n",
       "\n",
       "\n",
       "       [[[-0.26019717,  0.0137342 ,  0.02457104,  0.01734648],\n",
       "         [-0.1311183 ,  0.03324052,  0.03974262,  0.03540789],\n",
       "         [-0.00203943,  0.05274684,  0.05491421,  0.0534693 ],\n",
       "         [-0.06657887,  0.04299368,  0.04732842,  0.04443859]],\n",
       "\n",
       "        [[-0.35659925,  0.05207465,  0.04998957,  0.04998957],\n",
       "         [-0.18895955,  0.05624479,  0.05499374,  0.05499374],\n",
       "         [-0.02131985,  0.06041493,  0.05999791,  0.05999791],\n",
       "         [-0.1051397 ,  0.05832986,  0.05749583,  0.05749583]],\n",
       "\n",
       "        [[-0.22159091, -0.02530992,  0.03667355,  0.00568182],\n",
       "         [-0.10795455,  0.00981405,  0.04700413,  0.02840909],\n",
       "         [ 0.00568182,  0.04493802,  0.05733471,  0.05113636],\n",
       "         [-0.05113636,  0.02737603,  0.05216942,  0.03977273]],\n",
       "\n",
       "        ..., \n",
       "        [[-0.02840909, -0.02840909, -0.02840909, -0.11931818],\n",
       "         [ 0.00795455,  0.00795455,  0.00795455, -0.04659091],\n",
       "         [ 0.04431818,  0.04431818,  0.04431818,  0.02613636],\n",
       "         [ 0.02613636,  0.02613636,  0.02613636, -0.01022727]],\n",
       "\n",
       "        [[ 0.01561065, -0.08692991, -0.00428528, -0.04560759],\n",
       "         [-0.08692991,  0.07376798,  0.01224365, -0.01254974],\n",
       "         [-0.00428528,  0.01224365,  0.06580961,  0.02050811],\n",
       "         [-0.04560759, -0.01254974,  0.02050811,  0.07805326]],\n",
       "\n",
       "        [[-0.19724026, -0.00243506, -0.00243506, -0.00243506],\n",
       "         [-0.09334416,  0.02353896,  0.02353896,  0.02353896],\n",
       "         [ 0.01055195,  0.04951299,  0.04951299,  0.04951299],\n",
       "         [-0.0413961 ,  0.03652597,  0.03652597,  0.03652597]]],\n",
       "\n",
       "\n",
       "       [[[-0.34317644,  0.00119442,  0.01481788,  0.00573557],\n",
       "         [-0.03891911,  0.0471736 ,  0.05057947,  0.04830889],\n",
       "         [-0.03891911,  0.0471736 ,  0.05057947,  0.04830889],\n",
       "         [-0.03891911,  0.0471736 ,  0.05057947,  0.04830889]],\n",
       "\n",
       "        [[-0.46436763,  0.04939384,  0.04677261,  0.04677261],\n",
       "         [-0.06921691,  0.05922346,  0.05856815,  0.05856815],\n",
       "         [-0.06921691,  0.05922346,  0.05856815,  0.05856815],\n",
       "         [-0.06921691,  0.05922346,  0.05856815,  0.05856815]],\n",
       "\n",
       "        [[-0.29464286, -0.04788961,  0.03003247, -0.00892857],\n",
       "         [-0.02678571,  0.0349026 ,  0.05438312,  0.04464286],\n",
       "         [-0.02678571,  0.0349026 ,  0.05438312,  0.04464286],\n",
       "         [-0.02678571,  0.0349026 ,  0.05438312,  0.04464286]],\n",
       "\n",
       "        ..., \n",
       "        [[-0.05178571, -0.05178571, -0.05178571, -0.16607143],\n",
       "         [ 0.03392857,  0.03392857,  0.03392857,  0.00535714],\n",
       "         [ 0.03392857,  0.03392857,  0.03392857,  0.00535714],\n",
       "         [ 0.03392857,  0.03392857,  0.03392857,  0.00535714]],\n",
       "\n",
       "        [[-0.19724026, -0.09334416,  0.01055195, -0.0413961 ],\n",
       "         [-0.00243506,  0.02353896,  0.04951299,  0.03652597],\n",
       "         [-0.00243506,  0.02353896,  0.04951299,  0.03652597],\n",
       "         [-0.00243506,  0.02353896,  0.04951299,  0.03652597]],\n",
       "\n",
       "        [[-0.10913931, -0.03815439, -0.03815439, -0.03815439],\n",
       "         [-0.03815439,  0.06654836,  0.0230701 ,  0.0230701 ],\n",
       "         [-0.03815439,  0.0230701 ,  0.06654836,  0.0230701 ],\n",
       "         [-0.03815439,  0.0230701 ,  0.0230701 ,  0.06654836]]]])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta_matrices_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[-0.2655 , -0.03537,  0.00279, -0.0243 ],\n",
       "         [-0.03537,  0.04028,  0.04537,  0.03118],\n",
       "         [ 0.00279,  0.04537,  0.04608,  0.04373],\n",
       "         [-0.0243 ,  0.03118,  0.04373,  0.05236]],\n",
       "\n",
       "        [[-0.34638, -0.00418,  0.02688,  0.00879],\n",
       "         [-0.04779,  0.04207,  0.04922,  0.03923],\n",
       "         [-0.00454,  0.04639,  0.04593,  0.04762],\n",
       "         [-0.03618,  0.03563,  0.04795,  0.04937]],\n",
       "\n",
       "        [[-0.25658, -0.07774,  0.00408, -0.03749],\n",
       "         [-0.016  ,  0.02873,  0.04735,  0.03275],\n",
       "         [ 0.01512,  0.04348,  0.04953,  0.04693],\n",
       "         [-0.00691,  0.03005,  0.04836,  0.04833]],\n",
       "\n",
       "        ..., \n",
       "        [[-0.06677, -0.0718 , -0.0718 , -0.20506],\n",
       "         [ 0.03007,  0.03143,  0.03317,  0.00005],\n",
       "         [ 0.04749,  0.05059,  0.04749,  0.03352],\n",
       "         [ 0.04007,  0.04143,  0.04007,  0.02004]],\n",
       "\n",
       "        [[-0.11694, -0.11993, -0.06721, -0.10574],\n",
       "         [ 0.02238,  0.01866,  0.03239,  0.02142],\n",
       "         [ 0.04338,  0.04353,  0.04678,  0.04276],\n",
       "         [ 0.03029,  0.03061,  0.03927,  0.03834]],\n",
       "\n",
       "        [[-0.28337, -0.06656, -0.02211, -0.03814],\n",
       "         [-0.01787,  0.03297,  0.04283,  0.03773],\n",
       "         [ 0.02466,  0.04875,  0.05152,  0.05106],\n",
       "         [ 0.00342,  0.0397 ,  0.04791,  0.04751]]],\n",
       "\n",
       "\n",
       "       [[[-0.34638, -0.04779, -0.00454, -0.03618],\n",
       "         [-0.00418,  0.04207,  0.04639,  0.03563],\n",
       "         [ 0.02688,  0.04922,  0.04593,  0.04795],\n",
       "         [ 0.00879,  0.03923,  0.04762,  0.04937]],\n",
       "\n",
       "        [[-0.38047, -0.02238,  0.017  , -0.00158],\n",
       "         [-0.02238,  0.04798,  0.04309,  0.03755],\n",
       "         [ 0.017  ,  0.04309,  0.04454,  0.04538],\n",
       "         [-0.00158,  0.03755,  0.04538,  0.04982]],\n",
       "\n",
       "        [[-0.32396, -0.09862, -0.00429, -0.0535 ],\n",
       "         [ 0.01108,  0.03529,  0.04835,  0.03728],\n",
       "         [ 0.03528,  0.04946,  0.05098,  0.05056],\n",
       "         [ 0.02059,  0.04037,  0.05093,  0.05022]],\n",
       "\n",
       "        ..., \n",
       "        [[-0.09142, -0.09498, -0.09625, -0.2535 ],\n",
       "         [ 0.03833,  0.03959,  0.03833,  0.01438],\n",
       "         [ 0.05396,  0.05523,  0.05523,  0.0507 ],\n",
       "         [ 0.04927,  0.05054,  0.05306,  0.03753]],\n",
       "\n",
       "        [[-0.1503 , -0.15053, -0.0876 , -0.13428],\n",
       "         [ 0.03555,  0.02737,  0.03822,  0.03004],\n",
       "         [ 0.05599,  0.05174,  0.05055,  0.05112],\n",
       "         [ 0.04497,  0.04344,  0.04844,  0.04528]],\n",
       "\n",
       "        [[-0.33294, -0.088  , -0.0357 , -0.05719],\n",
       "         [ 0.00409,  0.03661,  0.04543,  0.0409 ],\n",
       "         [ 0.04446,  0.05351,  0.05339,  0.05457],\n",
       "         [ 0.02758,  0.04694,  0.05239,  0.05395]]],\n",
       "\n",
       "\n",
       "       [[[-0.25658, -0.016  ,  0.01512, -0.00691],\n",
       "         [-0.07774,  0.02873,  0.04348,  0.03005],\n",
       "         [ 0.00408,  0.04735,  0.04953,  0.04836],\n",
       "         [-0.03749,  0.03275,  0.04693,  0.04833]],\n",
       "\n",
       "        [[-0.32396,  0.01108,  0.03528,  0.02059],\n",
       "         [-0.09862,  0.03529,  0.04946,  0.04037],\n",
       "         [-0.00429,  0.04835,  0.05098,  0.05093],\n",
       "         [-0.0535 ,  0.03728,  0.05056,  0.05022]],\n",
       "\n",
       "        [[-0.19668, -0.06067,  0.01116, -0.02295],\n",
       "         [-0.06067,  0.0232 ,  0.0404 ,  0.02291],\n",
       "         [ 0.01116,  0.0404 ,  0.05309,  0.04512],\n",
       "         [-0.02295,  0.02291,  0.04512,  0.04846]],\n",
       "\n",
       "        ..., \n",
       "        [[-0.04859, -0.05099, -0.05099, -0.16358],\n",
       "         [ 0.01655,  0.01895,  0.01655, -0.02849],\n",
       "         [ 0.04649,  0.04649,  0.04888,  0.03377],\n",
       "         [ 0.03435,  0.03435,  0.03675,  0.0095 ]],\n",
       "\n",
       "        [[-0.09163, -0.09263, -0.04799, -0.0787 ],\n",
       "         [ 0.0019 ,  0.00127,  0.02053,  0.00534],\n",
       "         [ 0.04085,  0.04339,  0.04662,  0.04197],\n",
       "         [ 0.02349,  0.02335,  0.03418,  0.02807]],\n",
       "\n",
       "        [[-0.22929, -0.04819, -0.00912, -0.02183],\n",
       "         [-0.0561 ,  0.0223 ,  0.03384,  0.02824],\n",
       "         [ 0.02163,  0.0472 ,  0.05161,  0.05061],\n",
       "         [-0.01101,  0.03448,  0.04462,  0.04103]]],\n",
       "\n",
       "\n",
       "       ..., \n",
       "       [[[-0.06677,  0.03007,  0.04749,  0.04007],\n",
       "         [-0.0718 ,  0.03143,  0.05059,  0.04143],\n",
       "         [-0.0718 ,  0.03317,  0.04749,  0.04007],\n",
       "         [-0.20506,  0.00005,  0.03352,  0.02004]],\n",
       "\n",
       "        [[-0.09142,  0.03833,  0.05396,  0.04927],\n",
       "         [-0.09498,  0.03959,  0.05523,  0.05054],\n",
       "         [-0.09625,  0.03833,  0.05523,  0.05306],\n",
       "         [-0.2535 ,  0.01438,  0.0507 ,  0.03753]],\n",
       "\n",
       "        [[-0.04859,  0.01655,  0.04649,  0.03435],\n",
       "         [-0.05099,  0.01895,  0.04649,  0.03435],\n",
       "         [-0.05099,  0.01655,  0.04888,  0.03675],\n",
       "         [-0.16358, -0.02849,  0.03377,  0.0095 ]],\n",
       "\n",
       "        ..., \n",
       "        [[ 0.05524,  0.00762,  0.00762, -0.03238],\n",
       "         [ 0.00762,  0.05524,  0.00762, -0.03238],\n",
       "         [ 0.00762,  0.00762,  0.05524, -0.03238],\n",
       "         [-0.03238, -0.03238, -0.03238, -0.01714]],\n",
       "\n",
       "        [[ 0.00702,  0.00672,  0.02349,  0.01277],\n",
       "         [ 0.00702,  0.00672,  0.02349,  0.01277],\n",
       "         [ 0.00702,  0.00672,  0.02349,  0.01277],\n",
       "         [-0.04845, -0.04906, -0.01552, -0.03697]],\n",
       "\n",
       "        [[-0.03363,  0.02157,  0.03598,  0.03165],\n",
       "         [-0.04475,  0.03268,  0.03598,  0.03165],\n",
       "         [-0.04475,  0.02157,  0.03598,  0.03165],\n",
       "         [-0.1506 , -0.01798,  0.01084,  0.00218]]],\n",
       "\n",
       "\n",
       "       [[[-0.11694,  0.02238,  0.04338,  0.03029],\n",
       "         [-0.11993,  0.01866,  0.04353,  0.03061],\n",
       "         [-0.06721,  0.03239,  0.04678,  0.03927],\n",
       "         [-0.10574,  0.02142,  0.04276,  0.03834]],\n",
       "\n",
       "        [[-0.1503 ,  0.03555,  0.05599,  0.04497],\n",
       "         [-0.15053,  0.02737,  0.05174,  0.04344],\n",
       "         [-0.0876 ,  0.03822,  0.05055,  0.04844],\n",
       "         [-0.13428,  0.03004,  0.05112,  0.04528]],\n",
       "\n",
       "        [[-0.09163,  0.0019 ,  0.04085,  0.02349],\n",
       "         [-0.09263,  0.00127,  0.04339,  0.02335],\n",
       "         [-0.04799,  0.02053,  0.04662,  0.03418],\n",
       "         [-0.0787 ,  0.00534,  0.04197,  0.02807]],\n",
       "\n",
       "        ..., \n",
       "        [[ 0.00702,  0.00702,  0.00702, -0.04845],\n",
       "         [ 0.00672,  0.00672,  0.00672, -0.04906],\n",
       "         [ 0.02349,  0.02349,  0.02349, -0.01552],\n",
       "         [ 0.01277,  0.01277,  0.01277, -0.03697]],\n",
       "\n",
       "        [[ 0.01539, -0.02462, -0.00136, -0.01624],\n",
       "         [-0.02462,  0.02041, -0.00166, -0.01661],\n",
       "         [-0.00136, -0.00166,  0.04818,  0.00424],\n",
       "         [-0.01624, -0.01661,  0.00424,  0.02852]],\n",
       "\n",
       "        [[-0.0828 ,  0.00654,  0.02652,  0.02315],\n",
       "         [-0.08623,  0.00624,  0.02633,  0.02029],\n",
       "         [-0.04185,  0.02282,  0.04187,  0.03265],\n",
       "         [-0.0676 ,  0.01221,  0.03013,  0.02975]]],\n",
       "\n",
       "\n",
       "       [[[-0.28337, -0.01787,  0.02466,  0.00342],\n",
       "         [-0.06656,  0.03297,  0.04875,  0.0397 ],\n",
       "         [-0.02211,  0.04283,  0.05152,  0.04791],\n",
       "         [-0.03814,  0.03773,  0.05106,  0.04751]],\n",
       "\n",
       "        [[-0.33294,  0.00409,  0.04446,  0.02758],\n",
       "         [-0.088  ,  0.03661,  0.05351,  0.04694],\n",
       "         [-0.0357 ,  0.04543,  0.05339,  0.05239],\n",
       "         [-0.05719,  0.0409 ,  0.05457,  0.05395]],\n",
       "\n",
       "        [[-0.22929, -0.0561 ,  0.02163, -0.01101],\n",
       "         [-0.04819,  0.0223 ,  0.0472 ,  0.03448],\n",
       "         [-0.00912,  0.03384,  0.05161,  0.04462],\n",
       "         [-0.02183,  0.02824,  0.05061,  0.04103]],\n",
       "\n",
       "        ..., \n",
       "        [[-0.03363, -0.04475, -0.04475, -0.1506 ],\n",
       "         [ 0.02157,  0.03268,  0.02157, -0.01798],\n",
       "         [ 0.03598,  0.03598,  0.03598,  0.01084],\n",
       "         [ 0.03165,  0.03165,  0.03165,  0.00218]],\n",
       "\n",
       "        [[-0.0828 , -0.08623, -0.04185, -0.0676 ],\n",
       "         [ 0.00654,  0.00624,  0.02282,  0.01221],\n",
       "         [ 0.02652,  0.02633,  0.04187,  0.03013],\n",
       "         [ 0.02315,  0.02029,  0.03265,  0.02975]],\n",
       "\n",
       "        [[-0.16653, -0.04676, -0.01243, -0.02389],\n",
       "         [-0.04676,  0.04303,  0.02924,  0.02496],\n",
       "         [-0.01243,  0.02924,  0.05272,  0.03938],\n",
       "         [-0.02389,  0.02496,  0.03938,  0.0498 ]]]])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.set_printoptions(precision=5, suppress=True)\n",
    "delta_matrices_clust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.5337\n",
      " 0.1677\n",
      " 0.3572\n",
      "       \n",
      " 0.3238\n",
      " 0.2746\n",
      " 0.3138\n",
      "[torch.FloatTensor of size 2052x1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.user_add_bias.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
