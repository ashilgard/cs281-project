{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    "from itertools import product\n",
    "from itertools import combinations_with_replacement\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import time\n",
    "import math\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, SpectralClustering, AffinityPropagation\n",
    "from sklearn.metrics.pairwise import pairwise_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gj = pd.read_csv('./gj_df.csv')\n",
    "# gj = gj[['ifp_id', 'ctt', 'cond', 'training', 'team', 'user_id', 'value', 'fcast_date']]\n",
    "# gj['fcast_year'] = pd.to_datetime(gj['fcast_date']).dt.year\n",
    "# gj['fcast_week'] = pd.to_datetime(gj['fcast_date']).dt.week\n",
    "# gj['ifp_week'] = gj['fcast_year'].map(str) + gj['fcast_week'].map(str) + gj['ifp_id']\n",
    "# gj = gj.drop('fcast_date', axis=1)\n",
    "# gj = gj.drop_duplicates()\n",
    "# gj.to_csv('./gj_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ifp_id</th>\n",
       "      <th>ctt</th>\n",
       "      <th>cond</th>\n",
       "      <th>training</th>\n",
       "      <th>team</th>\n",
       "      <th>user_id</th>\n",
       "      <th>value</th>\n",
       "      <th>fcast_year</th>\n",
       "      <th>fcast_week</th>\n",
       "      <th>ifp_week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1244-0</td>\n",
       "      <td>1a</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51</td>\n",
       "      <td>0.20</td>\n",
       "      <td>2015</td>\n",
       "      <td>3</td>\n",
       "      <td>201531244-0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1244-0</td>\n",
       "      <td>1a</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51</td>\n",
       "      <td>0.20</td>\n",
       "      <td>2015</td>\n",
       "      <td>4</td>\n",
       "      <td>201541244-0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1244-0</td>\n",
       "      <td>1a</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51</td>\n",
       "      <td>0.20</td>\n",
       "      <td>2015</td>\n",
       "      <td>5</td>\n",
       "      <td>201551244-0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1244-0</td>\n",
       "      <td>1a</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51</td>\n",
       "      <td>0.20</td>\n",
       "      <td>2015</td>\n",
       "      <td>6</td>\n",
       "      <td>201561244-0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1244-0</td>\n",
       "      <td>1a</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51</td>\n",
       "      <td>0.20</td>\n",
       "      <td>2015</td>\n",
       "      <td>7</td>\n",
       "      <td>201571244-0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1244-0</td>\n",
       "      <td>1a</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51</td>\n",
       "      <td>0.20</td>\n",
       "      <td>2015</td>\n",
       "      <td>8</td>\n",
       "      <td>201581244-0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1244-0</td>\n",
       "      <td>1a</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51</td>\n",
       "      <td>0.09</td>\n",
       "      <td>2015</td>\n",
       "      <td>8</td>\n",
       "      <td>201581244-0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1244-0</td>\n",
       "      <td>1a</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51</td>\n",
       "      <td>0.09</td>\n",
       "      <td>2015</td>\n",
       "      <td>9</td>\n",
       "      <td>201591244-0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1394-0</td>\n",
       "      <td>1a</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51</td>\n",
       "      <td>0.75</td>\n",
       "      <td>2014</td>\n",
       "      <td>42</td>\n",
       "      <td>2014421394-0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1394-0</td>\n",
       "      <td>1a</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51</td>\n",
       "      <td>0.75</td>\n",
       "      <td>2014</td>\n",
       "      <td>43</td>\n",
       "      <td>2014431394-0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ifp_id ctt  cond training  team  user_id  value  fcast_year  fcast_week  \\\n",
       "0  1244-0  1a     1        a   NaN       51   0.20        2015           3   \n",
       "1  1244-0  1a     1        a   NaN       51   0.20        2015           4   \n",
       "2  1244-0  1a     1        a   NaN       51   0.20        2015           5   \n",
       "3  1244-0  1a     1        a   NaN       51   0.20        2015           6   \n",
       "4  1244-0  1a     1        a   NaN       51   0.20        2015           7   \n",
       "5  1244-0  1a     1        a   NaN       51   0.20        2015           8   \n",
       "6  1244-0  1a     1        a   NaN       51   0.09        2015           8   \n",
       "7  1244-0  1a     1        a   NaN       51   0.09        2015           9   \n",
       "8  1394-0  1a     1        a   NaN       51   0.75        2014          42   \n",
       "9  1394-0  1a     1        a   NaN       51   0.75        2014          43   \n",
       "\n",
       "       ifp_week  \n",
       "0   201531244-0  \n",
       "1   201541244-0  \n",
       "2   201551244-0  \n",
       "3   201561244-0  \n",
       "4   201571244-0  \n",
       "5   201581244-0  \n",
       "6   201581244-0  \n",
       "7   201591244-0  \n",
       "8  2014421394-0  \n",
       "9  2014431394-0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# adult = pd.read_csv('../labels.txt', delimiter='\\t', header=0, names=['user_id','website','rating'])\n",
    "# trec = pd.read_csv('../trec-rf10-crowd/trec-rf10-data.txt', delimiter='\\t')\n",
    "# gj = pd.read_csv('./filled_active_df.csv')\n",
    "\n",
    "# best_users = trec.groupby('workerID').count().sort_values('docID', ascending=False)[:150].index\n",
    "# trec = trec[trec['workerID'].isin(best_users)]\n",
    "\n",
    "# r = pd.Series([2,3,2,3], index=[1,2,0,-2])\n",
    "# trec['label_bin'] = trec['label'].map(r)\n",
    "gj.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "ifp_var = gj.groupby('ifp_week')['value'].var().sort_values()\n",
    "ifps_less_var = (ifp_var<.1).index\n",
    "ifps_less_var\n",
    "testframe = testframe[testframe['ifp_week'].isin(ifps_less_var)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ifp_id</th>\n",
       "      <th>ctt</th>\n",
       "      <th>cond</th>\n",
       "      <th>training</th>\n",
       "      <th>team</th>\n",
       "      <th>user_id</th>\n",
       "      <th>value</th>\n",
       "      <th>fcast_year</th>\n",
       "      <th>fcast_week</th>\n",
       "      <th>ifp_week</th>\n",
       "      <th>bin</th>\n",
       "      <th>task_id</th>\n",
       "      <th>uid</th>\n",
       "      <th>bin_levels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1244-0</td>\n",
       "      <td>1a</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2015</td>\n",
       "      <td>3</td>\n",
       "      <td>201531244-0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1244-0</td>\n",
       "      <td>1a</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2015</td>\n",
       "      <td>4</td>\n",
       "      <td>201541244-0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1244-0</td>\n",
       "      <td>1a</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2015</td>\n",
       "      <td>5</td>\n",
       "      <td>201551244-0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1244-0</td>\n",
       "      <td>1a</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2015</td>\n",
       "      <td>6</td>\n",
       "      <td>201561244-0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1244-0</td>\n",
       "      <td>1a</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2015</td>\n",
       "      <td>7</td>\n",
       "      <td>201571244-0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ifp_id ctt  cond training  team  user_id  value  fcast_year  fcast_week  \\\n",
       "0  1244-0  1a     1        a   NaN       51    0.2        2015           3   \n",
       "1  1244-0  1a     1        a   NaN       51    0.2        2015           4   \n",
       "2  1244-0  1a     1        a   NaN       51    0.2        2015           5   \n",
       "3  1244-0  1a     1        a   NaN       51    0.2        2015           6   \n",
       "4  1244-0  1a     1        a   NaN       51    0.2        2015           7   \n",
       "\n",
       "      ifp_week  bin  task_id  uid bin_levels  \n",
       "0  201531244-0  0.2        0    0          2  \n",
       "1  201541244-0  0.2        1    0          2  \n",
       "2  201551244-0  0.2        2    0          2  \n",
       "3  201561244-0  0.2        3    0          2  \n",
       "4  201571244-0  0.2        4    0          2  "
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testframe = create_user_task_ids(adult, 'user_id', 'website', 'rating')\n",
    "testframe = create_user_task_ids(gj, 'user_id', 'ifp_week', 'value', False, True)\n",
    "testframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testframe['value'] = testframe['value'] - .5\n",
    "testframe['value'] = testframe['value']*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = pd.cut(\n",
    "    testframe['value'],\n",
    "    [-np.inf, .2, .4, .6, .8, np.inf],\n",
    "    labels=[2,3,5,7,11]\n",
    ")\n",
    "testframe['bin_levels'] = c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testframe = testframe[testframe['uid']<1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batcher(seq, size):\n",
    "    return (seq[pos:pos + size] for pos in range(0, len(seq), size))\n",
    "\n",
    "def split(df):\n",
    "    train_df, validate_df, test_df = np.split(df.sample(frac=1), [int(.6*len(df)), int(.8*len(df))])\n",
    "    return train_df, validate_df, test_df\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, users, tasks, k=2):\n",
    "        super(Model, self).__init__()\n",
    "        self.user_lut = nn.Embedding(users, k)\n",
    "        self.task_lut = nn.Embedding(tasks, k)\n",
    "\n",
    "        self.user_bias = nn.Embedding(users, 1)\n",
    "        self.task_bias = nn.Embedding(tasks, 1)\n",
    "        self.global_bias = nn.Parameter(torch.FloatTensor(1))\n",
    "        \n",
    "    def forward(self, users, jokes):\n",
    "        user_vectors = self.user_lut(users)\n",
    "        task_vectors = self.task_lut(jokes)\n",
    "        user_bias = self.user_bias(users)\n",
    "        task_bias = self.task_bias(jokes)\n",
    "\n",
    "        return torch.bmm(user_vectors.unsqueeze(1),\n",
    "                         task_vectors.unsqueeze(2)).squeeze() \\\n",
    "                         + user_bias.squeeze() + task_bias.squeeze() + self.global_bias.expand_as(user_bias.squeeze())\n",
    "        \n",
    "class Model2(nn.Module):\n",
    "    def __init__(self, users, tasks, k=2):\n",
    "        super(Model2, self).__init__()\n",
    "        self.user_lut = nn.Embedding(users, k)\n",
    "        self.task_lut = nn.Embedding(tasks, k)\n",
    "\n",
    "        self.user_bias = nn.Embedding(users, 1)\n",
    "        self.task_bias = nn.Embedding(tasks, 1)\n",
    "        self.global_bias = nn.Parameter(torch.FloatTensor(1))\n",
    "        \n",
    "    def forward(self, users, jokes):\n",
    "        user_vectors = self.user_lut(users)\n",
    "        task_vectors = self.task_lut(jokes)\n",
    "        user_bias = self.user_bias(users)\n",
    "        task_bias = self.task_bias(jokes)\n",
    "\n",
    "        preds = torch.bmm(user_vectors.unsqueeze(1), task_vectors.unsqueeze(2)).squeeze() \\\n",
    "                          + task_bias.squeeze() + self.global_bias.expand_as(user_bias.squeeze())\n",
    "        m = nn.Tanh()\n",
    "        return m((user_bias.squeeze() * (preds - .5)) + .5)\n",
    "            \n",
    "\n",
    "def val(df, model):\n",
    "    crit = nn.MSELoss(size_average=False)\n",
    "    crit2 = nn.L1Loss(size_average=False)\n",
    "    total_loss = 0.\n",
    "    total_l1 = 0.\n",
    "    total_num = 0\n",
    "    for batch in batcher(df, 100):\n",
    "        true_rating = Variable(torch.Tensor(batch.bin.values.astype(float)))\n",
    "        total_num = total_num + true_rating.size(0)\n",
    "        users = Variable(torch.LongTensor(batch.uid.values))\n",
    "        tasks = Variable(torch.LongTensor(batch.task_id.values))\n",
    "        scores = model.forward(users, tasks)\n",
    "        total_loss += crit(scores, true_rating).data[0]\n",
    "        total_l1 += crit2(scores,true_rating).data[0]\n",
    "    return math.sqrt(total_loss/total_num), total_l1/total_num\n",
    "\n",
    "\n",
    "def train(train_iter, val_iter, test_iter, model):\n",
    "    opt = optim.SGD(model.parameters(), lr=0.3)\n",
    "    crit = nn.MSELoss()\n",
    "    crit2 = nn.L1Loss()\n",
    "\n",
    "    print(\"val:\", val(validate_df, model))\n",
    "    for epochs in range(30):\n",
    "        avg_loss = 0\n",
    "        avg_l1 = 0\n",
    "        total = 0\n",
    "        for i,batch in enumerate(batcher(train_df, 100)):\n",
    "            opt.zero_grad()\n",
    "            rating = Variable(torch.Tensor(batch.bin.values.astype(float)))\n",
    "#             print(rating)\n",
    "            users = Variable(torch.LongTensor(batch.uid.values))\n",
    "            tasks = Variable(torch.LongTensor(batch.task_id.values))\n",
    "            scores = model.forward(users, tasks) \n",
    "#             + torch.sum(model.user_lut.weight.data.pow(2)) + \\\n",
    "#             torch.sum(model.task_lut.weight.data.pow(2)) + torch.sum(model.user_bias.weight.data.pow(2)) + \\\n",
    "#             torch.sum(model.task_bias.weight.data.pow(2))\n",
    "            loss = crit(scores, rating)\n",
    "            #if i % 1000==0:\n",
    "            #    print (loss.data[0])\n",
    "            loss.backward()\n",
    "            avg_loss += loss.data[0]\n",
    "            avg_l1 += crit2(scores,rating).data[0]\n",
    "            total += 1\n",
    "            opt.step()\n",
    "        print(\"train:\", math.sqrt(avg_loss / float(total)), avg_l1/ float(total))\n",
    "        print(\"val:\", val(validate_df, model))\n",
    "#         print(model.user_bias.weight.data)\n",
    "    return model.user_lut.weight.data, model.user_bias.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val: (0.7473103289996134, 0.6151419616596224)\n",
      "train: 0.606835178930439 0.4805361428138626\n",
      "val: (0.520822475750004, 0.41220385770541484)\n",
      "train: 0.4757892324542197 0.38006537861164646\n",
      "val: (0.44208866948033193, 0.35802764813449545)\n",
      "train: 0.4180096816202666 0.3426151462412771\n",
      "val: (0.40410267910046516, 0.3335855035564734)\n",
      "train: 0.3875334633946526 0.32308996881893765\n",
      "val: (0.37868319465825706, 0.3166363542452884)\n",
      "train: 0.3661391437356122 0.30695524819861153\n",
      "val: (0.3604964695735768, 0.30179046293685113)\n",
      "train: 0.3485468263748942 0.2879042638231379\n",
      "val: (0.3402293697318645, 0.2753946540632052)\n",
      "train: 0.33365334783670486 0.26922908909834187\n",
      "val: (0.330631677097293, 0.2654078644965046)\n",
      "train: 0.32733389363980764 0.2626523983599796\n",
      "val: (0.32700851024657934, 0.2627324770601625)\n",
      "train: 0.32354824948900557 0.2600798686645783\n",
      "val: (0.3224022426817918, 0.25977926799171963)\n",
      "train: 0.31940089727608695 0.2572212044104914\n",
      "val: (0.31915272446458454, 0.25739049644503675)\n",
      "train: 0.31636031402446024 0.2549835349759853\n",
      "val: (0.31576149457610564, 0.2549814377138155)\n",
      "train: 0.313187186234339 0.2526605734133678\n",
      "val: (0.31331564540360507, 0.2530695350721801)\n",
      "train: 0.31157341337396344 0.25122402311067094\n",
      "val: (0.312118430121688, 0.251900132452944)\n",
      "train: 0.31020715041233415 0.25002926559568034\n",
      "val: (0.3105479892815207, 0.25049371631167605)\n",
      "train: 0.30921128474947196 0.24908177451081093\n",
      "val: (0.30969510602715883, 0.2496591152459983)\n",
      "train: 0.3080238028573639 0.24815908111979718\n",
      "val: (0.30817120714646346, 0.24856784257776876)\n",
      "train: 0.3063379140662168 0.2468633367309389\n",
      "val: (0.3068124869182265, 0.24738044615115223)\n",
      "train: 0.3049976267475204 0.24575497527396006\n",
      "val: (0.30536539454619815, 0.24614720205257437)\n",
      "train: 0.30351641695883663 0.24458287230441442\n",
      "val: (0.30365135996001025, 0.24479246482215455)\n",
      "train: 0.3018735378755555 0.24325343208009\n",
      "val: (0.3023786373319534, 0.24386961167301202)\n",
      "train: 0.3003311547979293 0.24202008881320844\n",
      "val: (0.30071361518467904, 0.24269147550068748)\n",
      "train: 0.2989581141701586 0.2409036365574124\n",
      "val: (0.29981176963820333, 0.241533189308412)\n",
      "train: 0.2978717529206594 0.23992812248893503\n",
      "val: (0.2986333081258491, 0.24041065629975625)\n",
      "train: 0.2966034207667195 0.23885836595194823\n",
      "val: (0.29731858605612954, 0.23918993149708728)\n",
      "train: 0.29549155571356883 0.23784397469428498\n",
      "val: (0.29632152658773864, 0.23822117130092443)\n",
      "train: 0.29446035925643643 0.23687246494680947\n",
      "val: (0.2952726445106357, 0.23728076926774377)\n",
      "train: 0.29339899041319506 0.23586663785492012\n",
      "val: (0.29436225401820443, 0.2363023456718017)\n",
      "train: 0.292466405904689 0.2349504817222531\n",
      "val: (0.2934821489708132, 0.23539736123500526)\n",
      "train: 0.2915176012830986 0.23403722557817738\n",
      "val: (0.2925331353200977, 0.2345222007926998)\n",
      "train: 0.29017496874613297 0.23289354038687746\n",
      "val: (0.2911502722211347, 0.23340942963338948)\n"
     ]
    }
   ],
   "source": [
    "train_df, validate_df, test_df = split(testframe)\n",
    "users = len(train_df.uid.unique())\n",
    "tasks = len(train_df.task_id.unique())\n",
    "model = Model2(users, tasks, k=4)\n",
    "user_vec, user_bias = train(train_df, validate_df, test_df, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_vec, user_bias = model.user_lut.weight.data, model.user_bias.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_features = np.zeros((user_vec.numpy().shape[0], user_vec.numpy().shape[1]+1))\n",
    "user_features[:, :user_vec.numpy().shape[1]] = user_vec.numpy()\n",
    "user_features[:, -1] = user_bias.numpy().reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.01259172,  0.08032684,  0.22459139],\n",
       "       [ 0.03830139,  0.04006179, -0.15898089],\n",
       "       [ 1.5714947 , -0.86501276, -0.0041402 ],\n",
       "       ..., \n",
       "       [ 0.11534408, -0.09271911, -0.02346769],\n",
       "       [ 0.49681965, -0.66163236, -0.05740545],\n",
       "       [-0.15671408,  0.41284984,  0.02198376]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savetxt('./user_id_mapping.csv', testframe[['uid','user_id']].drop_duplicates().values)\n",
    "np.savetxt('./cluster_array.csv', kmeans.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[127  59 114 ..., 156  93 144]\n"
     ]
    }
   ],
   "source": [
    "kmeans = KMeans(n_clusters=200, random_state=0).fit(user_features)\n",
    "print(kmeans.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "completed, values, ind = compute_individual_dist(testframe, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "653.079264163971\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "features = np.empty((ind.shape[0], ind.shape[1]**2*ind.shape[0]))\n",
    "delta_matrices_all = np.empty((ind.shape[0], ind.shape[0], ind.shape[1], ind.shape[1]))\n",
    "score_matrices_all = np.empty((ind.shape[0], ind.shape[0], ind.shape[1], ind.shape[1]))\n",
    "for user_index in range(values.shape[0]):\n",
    "    if user_index%100==0:\n",
    "        print(user_index)\n",
    "    #check for full joint distribution or add a prior later\n",
    "    if np.sum(ind[user_index]==0) > 0:\n",
    "        continue\n",
    "    #compute delta matrices with all other users where applicable\n",
    "    else:\n",
    "        #create a mask so that other half of tasks can be used later to find score matrix\n",
    "#         mask = np.random.randint(0,2,values.shape).astype(bool)\n",
    "        mask = np.ones((values.shape)).astype(bool)\n",
    "        delta_matrices, t_m_i_1, cluster_img = compute_deltas(user_index, completed, values, ind, mask, False, 20)\n",
    "        features[user_index,:] = cluster_img.flatten()\n",
    "        delta_matrices_all[user_index,:,:,:] = delta_matrices\n",
    "#         score_matrices, t_m_i_2 = compute_deltas(user_index, completed, values, ind, ~mask, True, 20)\n",
    "#         score_matrices_all[user_index,:,:,:] = score_matrices\n",
    "#         print(np.sum(t_m_i_1), np.sum(t_m_i_2))\n",
    "#         if len(np.intersect1d(np.array(np.where(t_m_i_1==True)), np.array(np.where(t_m_i_2==True))))>0:\n",
    "#             print(np.intersect1d(np.array(np.where(t_m_i_1==True)), np.array(np.where(t_m_i_2==True))))\n",
    "#             print(regret(score_matrices, delta_matrices, \\\n",
    "#                      np.logical_and((t_m_i_1==True), (t_m_i_2==True))))\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110825"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.isnan(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def kmeans_missing(X, n_clusters, max_iter=10):\n",
    "    \"\"\"Perform K-Means clustering on data with missing values.\n",
    "\n",
    "    Args:\n",
    "      X: An [n_samples, n_features] array of data to cluster.\n",
    "      n_clusters: Number of clusters to form.\n",
    "      max_iter: Maximum number of EM iterations to perform.\n",
    "\n",
    "    Returns:\n",
    "      labels: An [n_samples] vector of integer labels.\n",
    "      centroids: An [n_clusters, n_features] array of cluster centroids.\n",
    "      X_hat: Copy of X with the missing values filled in.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize missing values to their column means\n",
    "    missing = ~np.isfinite(X)\n",
    "    mu = np.nanmean(X, 0, keepdims=1)\n",
    "    X_hat = np.where(missing, mu, X)\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        print(i)\n",
    "        if i > 0:\n",
    "            # initialize KMeans with the previous set of centroids. this is much\n",
    "            # faster and makes it easier to check convergence (since labels\n",
    "            # won't be permuted on every iteration), but might be more prone to\n",
    "            # getting stuck in local minima.\n",
    "            cls = KMeans(n_clusters, init=prev_centroids)\n",
    "        else:\n",
    "            # do multiple random initializations in parallel\n",
    "            cls = KMeans(n_clusters, n_jobs=-1)\n",
    "\n",
    "        # perform clustering on the filled-in data\n",
    "        labels = cls.fit_predict(X_hat)\n",
    "        centroids = cls.cluster_centers_\n",
    "\n",
    "        # fill in the missing values based on their cluster centroids\n",
    "        X_hat[missing] = centroids[labels][missing]\n",
    "\n",
    "        # when the labels have stopped changing then we have converged\n",
    "        if i > 0 and np.all(labels == prev_labels):\n",
    "            break\n",
    "\n",
    "        prev_labels = labels\n",
    "        prev_centroids = cls.cluster_centers_\n",
    "\n",
    "    return labels, centroids, X_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/annahilgard/anaconda/envs/python3/lib/python3.6/site-packages/sklearn/cluster/k_means_.py:893: RuntimeWarning: Explicit initial center position passed: performing only one init in k-means instead of n_init=10\n",
      "  return_n_iter=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "[220  99 170 ..., 266 343 331]\n"
     ]
    }
   ],
   "source": [
    "labels, centroids, X_hat = kmeans_missing(features, 400, max_iter=5)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_cluster_matrix_dist(labels, delta_matrices_all):\n",
    "#     delta_matrices_new=np.zeros((len(np.unique(labels)), len(np.unique(labels)), delta_matrices_all[0][0].shape[0], \\\n",
    "#                                                               delta_matrices_all[0][0].shape[1]))\n",
    "    delta_matrices_new=np.empty(delta_matrices_all.shape)\n",
    "    for i in np.unique(labels):\n",
    "        if i%100==0:\n",
    "            print(i)\n",
    "        missing = ~np.isfinite(delta_matrices_all)\n",
    "        mu = np.nanmean(delta_matrices_all, 0, keepdims=0)\n",
    "        filled = np.where(missing, mu, delta_matrices_all)\n",
    "        cluster_i = np.average(filled[labels==i], axis=0)\n",
    "        for j in np.unique(labels): \n",
    "            if np.sum(np.sum(np.isnan(cluster_i), (1,2)))>0:\n",
    "                print(i,j)\n",
    "                missing = ~np.isfinite(deltas_used)\n",
    "                mu = np.nanmean(delta_matrices_all, 1, keepdims=1)\n",
    "                X_hat = np.where(missing, mu, X)\n",
    "            cluster_j = np.average(cluster_i[labels==j], axis=0)\n",
    "#             delta_matrices_new[i,j] = cluster_j\n",
    "            delta_matrices_new[np.ix_(labels==i,labels==j)] = cluster_j\n",
    "    return delta_matrices_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "delta_matrices_clust = calc_cluster_matrix_dist(labels, delta_matrices_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0776220916645\n"
     ]
    }
   ],
   "source": [
    "print(np.average([[pairwise_distances(score_matrices_clust[i,j], delta_matrices_all[i,j]) for i in range(1000)] for j in range(1000)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "0.0770279382517\n"
     ]
    }
   ],
   "source": [
    "delta_matrices_clust = calc_cluster_matrix_dist(kmeans.labels_, delta_matrices_all)\n",
    "print(np.average([[pairwise_distances(delta_matrices_clust[i,j], delta_matrices_all[i,j]) for i in range(1000)] for j in range(1000)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val: 2.0777278104227674\n",
      "train: 1.2116290182206695\n",
      "val: 0.7037363884206135\n",
      "train: 0.5415771868267919\n",
      "val: 0.43095153492469124\n",
      "train: 0.37284708612514905\n",
      "val: 0.3362389822791087\n",
      "train: 0.3094920255758393\n",
      "val: 0.29511266203916064\n",
      "train: 0.2803961135279256\n",
      "val: 0.2742695826174684\n",
      "train: 0.26502186941334716\n",
      "val: 0.2624492080619879\n",
      "train: 0.2560319880641963\n",
      "val: 0.25516313670855634\n",
      "train: 0.2503637469994149\n",
      "val: 0.25037620876514394\n",
      "train: 0.24657596241028473\n",
      "val: 0.24706860682050535\n",
      "train: 0.24392504133559736\n",
      "val: 0.24468791332265538\n",
      "train: 0.24199866034805284\n",
      "val: 0.24291570638787846\n",
      "train: 0.24055453016340203\n",
      "val: 0.24155883453926144\n",
      "train: 0.23944327906537716\n",
      "val: 0.2404949847923139\n",
      "train: 0.2385690068180709\n",
      "val: 0.23964382042786905\n",
      "train: 0.23786796768224575\n",
      "val: 0.23895085063834187\n",
      "train: 0.23729649213738355\n",
      "val: 0.23837808474724526\n",
      "train: 0.2368238659764394\n",
      "val: 0.237898370021001\n",
      "train: 0.23642797699748727\n",
      "val: 0.2374918730891948\n",
      "train: 0.23609257782100876\n",
      "val: 0.23714382638434262\n",
      "train: 0.2358055065860778\n",
      "val: 0.23684303526376987\n",
      "train: 0.23555750824296656\n",
      "val: 0.23658088508690306\n",
      "train: 0.2353414352722948\n",
      "val: 0.23635064703582406\n",
      "train: 0.23515169175210365\n",
      "val: 0.23614699891543084\n",
      "train: 0.23498384095796956\n",
      "val: 0.23596568302681348\n",
      "train: 0.23483432844891994\n",
      "val: 0.23580324962772417\n",
      "train: 0.23470027365933682\n",
      "val: 0.2356568864836254\n",
      "train: 0.2345793203082262\n",
      "val: 0.2355242688066955\n",
      "train: 0.23446952343987784\n",
      "val: 0.23540346548704846\n",
      "train: 0.23436926490112822\n",
      "val: 0.23529285697300847\n",
      "train: 0.23427718623091057\n",
      "val: 0.23519107010882326\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "K=2 score: 0.07654269036872563, 0.7182761111990762\n",
      "val: 2.284823605002805\n",
      "train: 1.3192521727418582\n",
      "val: 0.7598190936701738\n",
      "train: 0.5834714315680728\n",
      "val: 0.4633628517182594\n",
      "train: 0.3985266149800948\n",
      "val: 0.35743634631153415\n",
      "train: 0.3267762996568664\n",
      "val: 0.3098624134576283\n",
      "train: 0.2926735850449051\n",
      "val: 0.2850395548947366\n",
      "train: 0.27410599702433486\n",
      "val: 0.27062000141393666\n",
      "train: 0.26296682381124886\n",
      "val: 0.2615535799070949\n",
      "train: 0.2557876157199241\n",
      "val: 0.25549959285335666\n",
      "train: 0.25090029808564407\n",
      "val: 0.251262259751218\n",
      "train: 0.24742679224661\n",
      "val: 0.24818217335449672\n",
      "train: 0.24487086200093547\n",
      "val: 0.24587278292588385\n",
      "train: 0.2429355859151735\n",
      "val: 0.24409588914011093\n",
      "train: 0.24143476854388918\n",
      "val: 0.24269846699229336\n",
      "train: 0.24024696625730466\n",
      "val: 0.24157866789863458\n",
      "train: 0.23929027816758489\n",
      "val: 0.24066662601844474\n",
      "train: 0.2385078594115449\n",
      "val: 0.239913128103617\n",
      "train: 0.23785925761297802\n",
      "val: 0.23928269793247756\n",
      "train: 0.2373150500688997\n",
      "val: 0.23874923617823116\n",
      "train: 0.2368534196232571\n",
      "val: 0.2382931907929601\n",
      "train: 0.23645790876210462\n",
      "val: 0.23789967608131365\n",
      "train: 0.2361159121759225\n",
      "val: 0.23755719171237027\n",
      "train: 0.2358176391111538\n",
      "val: 0.23725673332122818\n",
      "train: 0.23555539038100617\n",
      "val: 0.2369911697961889\n",
      "train: 0.23532304176325056\n",
      "val: 0.23675478595711194\n",
      "train: 0.235115671384508\n",
      "val: 0.23654295969624928\n",
      "train: 0.23492928246026618\n",
      "val: 0.23635191593786067\n",
      "train: 0.23476060134307342\n",
      "val: 0.23617854173922334\n",
      "train: 0.23460692451076387\n",
      "val: 0.23602025189508405\n",
      "train: 0.23446599901331996\n",
      "val: 0.2358748807815867\n",
      "train: 0.2343359345792994\n",
      "val: 0.2357406029498996\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "K=3 score: 0.0763494527771713, 0.7420566918702398\n",
      "val: 2.505235067381982\n",
      "train: 1.4290318799971589\n",
      "val: 0.8229067372862081\n",
      "train: 0.6310843306075239\n",
      "val: 0.5041243394238104\n",
      "train: 0.42997215376663594\n",
      "val: 0.38675472798526717\n",
      "train: 0.348997514485874\n",
      "val: 0.3319140455562486\n",
      "train: 0.3090523749576862\n",
      "val: 0.30227044747069487\n",
      "train: 0.28665409153640214\n",
      "val: 0.2845353855768607\n",
      "train: 0.272905041878892\n",
      "val: 0.2730974126821188\n",
      "train: 0.26387530301789713\n",
      "val: 0.2652835295637152\n",
      "train: 0.2576271478850066\n",
      "val: 0.25969807992390326\n",
      "train: 0.2531209860679155\n",
      "val: 0.2555574330749011\n",
      "train: 0.2497603623914104\n",
      "val: 0.2523949373454326\n",
      "train: 0.24718384942893135\n",
      "val: 0.24991888051663266\n",
      "train: 0.24516228715022959\n",
      "val: 0.247939330848948\n",
      "train: 0.2435447287697022\n",
      "val: 0.24632828273198698\n",
      "train: 0.24222846278622798\n",
      "val: 0.24499687736473402\n",
      "train: 0.24114160917079677\n",
      "val: 0.2438818048737211\n",
      "train: 0.24023262640929766\n",
      "val: 0.24293694280386854\n",
      "train: 0.23946375494247002\n",
      "val: 0.242127997727252\n",
      "train: 0.23880680276825766\n",
      "val: 0.24142902973508917\n",
      "train: 0.23824036233232376\n",
      "val: 0.24082009070857624\n",
      "train: 0.23774793172383032\n",
      "val: 0.24028562528315525\n",
      "train: 0.2373166143882409\n",
      "val: 0.23981334369242358\n",
      "train: 0.2369362073344491\n",
      "val: 0.23939341746009837\n",
      "train: 0.23659854335236885\n",
      "val: 0.23901790837295325\n",
      "train: 0.2362970230756593\n",
      "val: 0.23868033283225432\n",
      "train: 0.23602626004639798\n",
      "val: 0.2383753548818164\n",
      "train: 0.23578181780170784\n",
      "val: 0.23809854531155583\n",
      "train: 0.23556001438758842\n",
      "val: 0.23784619195245277\n",
      "train: 0.23535777242963804\n",
      "val: 0.23761516793384022\n",
      "train: 0.2351724968003901\n",
      "val: 0.23740282081797703\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "K=4 score: 0.07593253687397433, 0.7426020551790387\n",
      "val: 2.705978011314385\n",
      "train: 1.5362559582449407\n",
      "val: 0.8748255199824563\n",
      "train: 0.6709997040525059\n",
      "val: 0.536005042326824\n",
      "train: 0.45449901939965526\n",
      "val: 0.4086303193842743\n",
      "train: 0.36543544465966454\n",
      "val: 0.34758085560801116\n",
      "train: 0.3205689922198168\n",
      "val: 0.31385386894773715\n",
      "train: 0.2949876871496289\n",
      "val: 0.29335732057517044\n",
      "train: 0.2791028017519213\n",
      "val: 0.2800082763163212\n",
      "train: 0.268596778924714\n",
      "val: 0.2708405013503752\n",
      "train: 0.26130012845689576\n",
      "val: 0.26427303765478277\n",
      "train: 0.25603025530406864\n",
      "val: 0.25940382690065944\n",
      "train: 0.25210034618962246\n",
      "val: 0.25568936651467256\n",
      "train: 0.2490905556053456\n",
      "val: 0.2527871256767334\n",
      "train: 0.24673305907356596\n",
      "val: 0.25047287935461693\n",
      "train: 0.24485072522589219\n",
      "val: 0.24859498377115732\n",
      "train: 0.24332270515578164\n",
      "val: 0.2470479311034462\n",
      "train: 0.24206429263266924\n",
      "val: 0.24575647342565848\n",
      "train: 0.2410146969187149\n",
      "val: 0.2446657585105971\n",
      "train: 0.24012936865876916\n",
      "val: 0.24373501077991466\n",
      "train: 0.23937505225279984\n",
      "val: 0.2429333993424164\n",
      "train: 0.23872650954444788\n",
      "val: 0.2422372423153998\n",
      "train: 0.23816430404787906\n",
      "val: 0.24162809614943204\n",
      "train: 0.2376732625119267\n",
      "val: 0.24109141515892465\n",
      "train: 0.23724140066944405\n",
      "val: 0.24061559174514274\n",
      "train: 0.23685914362993188\n",
      "val: 0.24019127325163478\n",
      "train: 0.23651876994739385\n",
      "val: 0.23981084277752557\n",
      "train: 0.2362139901389765\n",
      "val: 0.2394680499700798\n",
      "train: 0.23593964304019444\n",
      "val: 0.23915772091533208\n",
      "train: 0.23569145591109833\n",
      "val: 0.23887553911244325\n",
      "train: 0.23546586680718243\n",
      "val: 0.23861788126799077\n",
      "train: 0.2352598880040451\n",
      "val: 0.23838168339739263\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "K=5 score: 0.0756826747631, 0.7349616313738704\n",
      "val: 2.845067765667384\n",
      "train: 1.6130925316622624\n",
      "val: 0.9166315288117693\n",
      "train: 0.703781890249164\n",
      "val: 0.5644345074627684\n",
      "train: 0.47777147429502054\n",
      "val: 0.43002452775707256\n",
      "train: 0.3829550112796568\n",
      "val: 0.36440312293094834\n",
      "train: 0.33418660873946937\n",
      "val: 0.32751893701675455\n",
      "train: 0.3058549865315476\n",
      "val: 0.3047332992020564\n",
      "train: 0.28795838973235255\n",
      "val: 0.2896566047982491\n",
      "train: 0.27593313906467626\n",
      "val: 0.2791448004857504\n",
      "train: 0.267459960513839\n",
      "val: 0.27150790172625866\n",
      "train: 0.2612608086483487\n",
      "val: 0.2657728075828623\n",
      "train: 0.2565850322629543\n",
      "val: 0.26134724928344494\n",
      "train: 0.25296833430538257\n",
      "val: 0.25785386828406426\n",
      "train: 0.2501109852371527\n",
      "val: 0.25504285899583906\n",
      "train: 0.24781242494177982\n",
      "val: 0.25274338202596275\n",
      "train: 0.2459342983017374\n",
      "val: 0.25083529961527\n",
      "train: 0.24437862851617181\n",
      "val: 0.24923206328397937\n",
      "train: 0.2430744406284774\n",
      "val: 0.24786999356062883\n",
      "train: 0.2419692846811476\n",
      "val: 0.24670135550761507\n",
      "train: 0.24102371139282988\n",
      "val: 0.2456897667514014\n",
      "train: 0.24020758063928602\n",
      "val: 0.24480708670211834\n",
      "train: 0.23949753748466507\n",
      "val: 0.2440312555068034\n",
      "train: 0.23887525319351005\n",
      "val: 0.2433447695819797\n",
      "train: 0.2383261766820845\n",
      "val: 0.24273359054422272\n",
      "train: 0.23783863237218805\n",
      "val: 0.24218634380072537\n",
      "train: 0.23740316228826677\n",
      "val: 0.2416937265097894\n",
      "train: 0.237012034111084\n",
      "val: 0.24124806639005306\n",
      "train: 0.2366588761756516\n",
      "val: 0.24084298739043328\n",
      "train: 0.2363383972502196\n",
      "val: 0.24047315100888655\n",
      "train: 0.23604616912142604\n",
      "val: 0.2401340533261744\n",
      "train: 0.2357784648258805\n",
      "val: 0.2398218776284166\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "K=6 score: 0.07570947052524746, 0.7833414463016155\n",
      "val: 3.060241538590184\n",
      "train: 1.7150541093094864\n",
      "val: 0.973086770953058\n",
      "train: 0.7416402835834014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val: 0.5979119211726214\n",
      "train: 0.5010595408194991\n",
      "val: 0.4538116255161054\n",
      "train: 0.39934205657254496\n",
      "val: 0.3825611433077871\n",
      "train: 0.34635565289352627\n",
      "val: 0.3419400636195742\n",
      "train: 0.31518187334284775\n",
      "val: 0.31651896121910467\n",
      "train: 0.2952847477721761\n",
      "val: 0.2995167080574935\n",
      "train: 0.28181112939753883\n",
      "val: 0.28755905959853967\n",
      "train: 0.2722634799855486\n",
      "val: 0.2788098085295374\n",
      "train: 0.26524854139250786\n",
      "val: 0.2721996359413626\n",
      "train: 0.2599395658947294\n",
      "val: 0.2670715384187277\n",
      "train: 0.25582116521817966\n",
      "val: 0.26300377728905894\n",
      "train: 0.2525587221990473\n",
      "val: 0.2597154946690391\n",
      "train: 0.24992741627249682\n",
      "val: 0.25701370432973375\n",
      "train: 0.24777169049116016\n",
      "val: 0.2547621452760772\n",
      "train: 0.2459811378737559\n",
      "val: 0.2528623085063793\n",
      "train: 0.2444756289580709\n",
      "val: 0.2512414721467619\n",
      "train: 0.24319585078115669\n",
      "val: 0.24984494856713665\n",
      "train: 0.24209712377480808\n",
      "val: 0.24863093145147605\n",
      "train: 0.2411452577017321\n",
      "val: 0.24756698048329648\n",
      "train: 0.24031371610020275\n",
      "val: 0.24662758518998684\n",
      "train: 0.23958163950988218\n",
      "val: 0.24579244432089561\n",
      "train: 0.2389324358775456\n",
      "val: 0.24504521777941043\n",
      "train: 0.2383527636170915\n",
      "val: 0.24437261983604186\n",
      "train: 0.23783179094168833\n",
      "val: 0.24376375357076724\n",
      "train: 0.23736063909771218\n",
      "val: 0.24320959394557257\n",
      "train: 0.2369319706288964\n",
      "val: 0.24270261256191736\n",
      "train: 0.2365396674703799\n",
      "val: 0.24223647708844884\n",
      "train: 0.23617859467983499\n",
      "val: 0.24180582624919095\n",
      "train: 0.23584440577360657\n",
      "val: 0.24140608898727076\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "K=7 score: 0.07553421840452237, 0.7266164358477504\n",
      "val: 3.171818936715936\n",
      "train: 1.784740413528142\n",
      "val: 1.0164545401564944\n",
      "train: 0.7749288559855632\n",
      "val: 0.6265621395326941\n",
      "train: 0.523197682453337\n",
      "val: 0.47451398158574354\n",
      "train: 0.4153024702761832\n",
      "val: 0.39845283598614767\n",
      "train: 0.358513241920608\n",
      "val: 0.35466344157074614\n",
      "train: 0.3247922901989164\n",
      "val: 0.32700806030545726\n",
      "train: 0.3030752841756806\n",
      "val: 0.3083517324408094\n",
      "train: 0.2882419863473991\n",
      "val: 0.2951277870545019\n",
      "train: 0.2776467657178373\n",
      "val: 0.2853852479097802\n",
      "train: 0.2698064100517048\n",
      "val: 0.2779809072771807\n",
      "train: 0.2638354719655214\n",
      "val: 0.27220757285729413\n",
      "train: 0.259178290007234\n",
      "val: 0.2676081342428755\n",
      "train: 0.2554716563099125\n",
      "val: 0.26387617965810495\n",
      "train: 0.2524699381463551\n",
      "val: 0.26079987453055303\n",
      "train: 0.2500021357814665\n",
      "val: 0.2582288536253208\n",
      "train: 0.24794619695710512\n",
      "val: 0.2560538949320878\n",
      "train: 0.24621308452450344\n",
      "val: 0.2541940371834637\n",
      "train: 0.24473658630500816\n",
      "val: 0.2525881850784341\n",
      "train: 0.24346661454568133\n",
      "val: 0.25118949236477106\n",
      "train: 0.24236469736981814\n",
      "val: 0.2499615162814071\n",
      "train: 0.24140087488943926\n",
      "val: 0.2488755396785131\n",
      "train: 0.24055152173176725\n",
      "val: 0.2479086587247195\n",
      "train: 0.23979779738683712\n",
      "val: 0.2470424167963671\n",
      "train: 0.23912451558977374\n",
      "val: 0.24626178153815723\n",
      "train: 0.238519319878886\n",
      "val: 0.24555440648607002\n",
      "train: 0.2379720651210402\n",
      "val: 0.24491006501896373\n",
      "train: 0.23747435177357332\n",
      "val: 0.244320206787329\n",
      "train: 0.2370191701794628\n",
      "val: 0.2437776414308559\n",
      "train: 0.23660062767687642\n",
      "val: 0.24327627670392699\n",
      "train: 0.2362137350317527\n",
      "val: 0.24281090938771552\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "K=8 score: 0.07555365872987167, 0.7658296065565633\n",
      "val: 3.352369035229583\n",
      "train: 1.880277184668858\n",
      "val: 1.0568423573106145\n",
      "train: 0.808676406514758\n",
      "val: 0.6506842243886083\n",
      "train: 0.5440786496947263\n",
      "val: 0.49133010690134643\n",
      "train: 0.42945484763064307\n",
      "val: 0.4106651376899486\n",
      "train: 0.36845561708170216\n",
      "val: 0.3637662187149724\n",
      "train: 0.33197453633236906\n",
      "val: 0.33396311214075725\n",
      "train: 0.30839887542980243\n",
      "val: 0.3137911193586677\n",
      "train: 0.29228038951685215\n",
      "val: 0.2994717006560859\n",
      "train: 0.28077242186807605\n",
      "val: 0.28891848913092355\n",
      "train: 0.2722672930111209\n",
      "val: 0.28090127958602884\n",
      "train: 0.26580114515230946\n",
      "val: 0.27465599895700926\n",
      "train: 0.26076776293155673\n",
      "val: 0.2696874681545107\n",
      "train: 0.25677047886948856\n",
      "val: 0.2656631398148538\n",
      "train: 0.25354095141510574\n",
      "val: 0.2623528158477909\n",
      "train: 0.25089240474960917\n",
      "val: 0.25959293333276745\n",
      "train: 0.2486915874354367\n",
      "val: 0.25726460006494456\n",
      "train: 0.24684136312133642\n",
      "val: 0.2552796482843034\n",
      "train: 0.2452695647820503\n",
      "val: 0.2535715209279231\n",
      "train: 0.24392166119154282\n",
      "val: 0.2520891822459659\n",
      "train: 0.24275582050112623\n",
      "val: 0.2507929331971588\n",
      "train: 0.24173951158685672\n",
      "val: 0.24965149379786591\n",
      "train: 0.24084712304878794\n",
      "val: 0.24863993014165883\n",
      "train: 0.24005825995432445\n",
      "val: 0.2477381576275008\n",
      "train: 0.23935651618087556\n",
      "val: 0.24692983061436538\n",
      "train: 0.23872856580703827\n",
      "val: 0.24620153733420924\n",
      "train: 0.23816349283941682\n",
      "val: 0.24554217786533736\n",
      "train: 0.23765227904150293\n",
      "val: 0.24494248752117764\n",
      "train: 0.2371874172945831\n",
      "val: 0.24439469016355725\n",
      "train: 0.23676261520845854\n",
      "val: 0.24389220516864085\n",
      "train: 0.23637256184950614\n",
      "val: 0.24342943431992214\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "K=9 score: 0.07513841283896183, 0.7498469736449609\n",
      "val: 3.50789782427653\n",
      "train: 1.9492994878625418\n",
      "val: 1.1000909472564953\n",
      "train: 0.8348320468630878\n",
      "val: 0.6775255125419228\n",
      "train: 0.5601937171428705\n",
      "val: 0.5100671002772516\n",
      "train: 0.44044460698191673\n",
      "val: 0.42463606917650903\n",
      "train: 0.37637718323888714\n",
      "val: 0.37462254532901557\n",
      "train: 0.33787988683837883\n",
      "val: 0.3426512297220072\n",
      "train: 0.3129065852561707\n",
      "val: 0.32091235494496506\n",
      "train: 0.2957878105014067\n",
      "val: 0.3054318790370595\n",
      "train: 0.283547798839972\n",
      "val: 0.2940010801838826\n",
      "train: 0.27449704865620966\n",
      "val: 0.28530875257080085\n",
      "train: 0.26761750387560324\n",
      "val: 0.27853553909369305\n",
      "train: 0.26226613414136013\n",
      "val: 0.2731477612240069\n",
      "train: 0.2580208307817874\n",
      "val: 0.26878566599268217\n",
      "train: 0.2545954086422208\n",
      "val: 0.2651995488954658\n",
      "train: 0.25179038329397607\n",
      "val: 0.26221166725734063\n",
      "train: 0.24946332523486897\n",
      "val: 0.25969268341477414\n",
      "train: 0.24751037010168203\n",
      "val: 0.2575466243806292\n",
      "train: 0.24585434502307696\n",
      "val: 0.25570103438410624\n",
      "train: 0.24443694466528731\n",
      "val: 0.2541003543019675\n",
      "train: 0.24321345408800085\n",
      "val: 0.25270138199099085\n",
      "train: 0.2421491122328665\n",
      "val: 0.2514700948263576\n",
      "train: 0.24121656574768996\n",
      "val: 0.2503793921571691\n",
      "train: 0.24039404633763933\n",
      "val: 0.2494074565995276\n",
      "train: 0.23966405316684744\n",
      "val: 0.24853655519141712\n",
      "train: 0.23901238339282357\n",
      "val: 0.2477521468780561\n",
      "train: 0.23842741176621338\n",
      "val: 0.24704220639265306\n",
      "train: 0.237899544349551\n",
      "val: 0.24639672518324515\n",
      "train: 0.2374208045530395\n",
      "val: 0.24580729810272156\n",
      "train: 0.236984514144404\n",
      "val: 0.24526682831895186\n",
      "train: 0.2365850453338128\n",
      "val: 0.2447692868484567\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "K=10 score: 0.07492495392362156, 0.7440194997134518\n"
     ]
    }
   ],
   "source": [
    "for k_ in range(2,11):\n",
    "    model = Model(users, tasks, k=k_)\n",
    "    user_vec, user_bias = train(train_df, validate_df, test_df, model)\n",
    "    user_features = np.zeros((user_vec.numpy().shape[0], user_vec.numpy().shape[1]+1))\n",
    "    user_features[:, :user_vec.numpy().shape[1]] = user_vec.numpy()\n",
    "    user_features[:, -1] = user_bias.numpy().reshape(-1)\n",
    "    kmeans = KMeans(n_clusters=400, random_state=0).fit(user_features)\n",
    "    delta_matrices_clust = calc_cluster_matrix_dist(kmeans.labels_, delta_matrices_all)\n",
    "    avg_dist = np.average([[pairwise_distances(delta_matrices_clust[i,j], delta_matrices_all[i,j]) for i in range(1000)] for j in range(1000)])\n",
    "    max_dist = np.max([[pairwise_distances(delta_matrices_clust[i,j], delta_matrices_all[i,j]) for i in range(1000)] for j in range(1000)])\n",
    "    print(\"K={} score: {}, {}\".format(k_, avg_dist, max_dist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.isnan(user_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/annahilgard/anaconda/envs/python3/lib/python3.6/site-packages/numpy/core/_methods.py:116: RuntimeWarning: overflow encountered in multiply\n",
      "  x = um.multiply(x, x, out=x)\n",
      "/Users/annahilgard/anaconda/envs/python3/lib/python3.6/site-packages/sklearn/metrics/pairwise.py:248: RuntimeWarning: invalid value encountered in add\n",
      "  distances += XX\n",
      "/Users/annahilgard/anaconda/envs/python3/lib/python3.6/site-packages/sklearn/cluster/k_means_.py:400: RuntimeWarning: overflow encountered in square\n",
      "  max_iter=max_iter, verbose=verbose)\n",
      "/Users/annahilgard/anaconda/envs/python3/lib/python3.6/site-packages/sklearn/cluster/k_means_.py:400: RuntimeWarning: invalid value encountered in subtract\n",
      "  max_iter=max_iter, verbose=verbose)\n",
      "/Users/annahilgard/anaconda/envs/python3/lib/python3.6/site-packages/sklearn/cluster/k_means_.py:401: RuntimeWarning: overflow encountered in square\n",
      "  inertia = np.sum((X - centers[labels]) ** 2, dtype=np.float64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "K=2 score: 0.07221198567612774, 0.7124813399972646\n"
     ]
    }
   ],
   "source": [
    "# for k_ in range(2,3):\n",
    "#     model = Model(users, tasks, k=k_)\n",
    "#     user_vec, user_bias = train(train_df, validate_df, test_df, model)\n",
    "#     user_features = np.empty((user_vec.numpy().shape[0], user_vec.numpy().shape[1]+1))\n",
    "#     user_features[:, :user_vec.numpy().shape[1]] = user_vec.numpy()\n",
    "#     user_features[:, :-1] = user_bias.numpy()\n",
    "sc = KMeans(n_clusters=200, random_state=0).fit(user_features)\n",
    "delta_matrices_clust = calc_cluster_matrix_dist(sc.labels_, delta_matrices_all)\n",
    "avg_dist = np.average([[pairwise_distances(delta_matrices_clust[i,j], delta_matrices_all[i,j]) for i in range(1000)] for j in range(1000)])\n",
    "max_dist = np.max([[pairwise_distances(delta_matrices_clust[i,j], delta_matrices_all[i,j]) for i in range(1000)] for j in range(1000)])\n",
    "print(\"K={} score: {}, {}\".format(k_, avg_dist, max_dist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = SpectralClustering(n_clusters=200).fit(user_features)\n",
    "# delta_matrices_clust = calc_cluster_matrix_dist(kmeans.labels_, delta_matrices_all)\n",
    "# avg_dist = np.average([[pairwise_distances(delta_matrices_clust[i,j], delta_matrices_all[i,j]) for i in range(1000)] for j in range(1000)])\n",
    "# print(\"K={} score: {}\".format(k_, avg_dist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([190,   1, 197,  85,   3,   1,   1, 199,   1, 190,  22,   3,   1,\n",
       "       188, 192,   1, 192,   1, 188,   1, 192, 190,   1, 191,   1, 166,\n",
       "         3, 188, 196, 188, 190,   1,   3,  14,   1,   1,   1, 188,  21,\n",
       "         1, 133, 143,   1, 108, 177, 196,   1,   1, 197,  25,   1,   1,\n",
       "         1, 188,   1, 157, 188, 193, 104, 193, 188, 190,   1,   1,   1,\n",
       "       188, 188,   0, 188, 188, 188, 188, 188, 190,   1,   1,   1,  53,\n",
       "         3,   1,   1,   1,   1,   3,   1,   1,   1, 188,   3, 193,   0,\n",
       "         3,   1,   1,   1, 188,   1,   1,   1,   1,   1, 190,   1,   1,\n",
       "         1, 127, 188, 190,   3, 188,   1, 188,   1,   1,   1, 195,   1,\n",
       "       193, 188, 199,   1, 159,   1, 197,   1,   1, 188, 155, 196,   1,\n",
       "       188, 132,   1,   1,  91, 193, 188, 164, 188, 141,   1,   1, 196,\n",
       "         1,   1, 190,   1, 193,   1, 188,   1,   3,   1,   1, 196, 188,\n",
       "        36,  92,   1,   1,  20,   1, 193, 106, 197,   1,   1,   1, 197,\n",
       "         1,   1,   1, 188,   3,   0, 188, 188,   1, 188,   1,   1,   1,\n",
       "         1, 195,   1,   3,   0,   0,   1,   1,   1,   1, 173,   1, 188,\n",
       "       188, 188,   1,   0,   1,   3,   3,   1, 188,   3,   3,   3, 192,\n",
       "         1, 193, 198, 162, 196, 197,   1, 124,   0, 188, 136, 188,   1,\n",
       "         1, 188, 193,   3, 197, 188, 198, 188, 165, 188, 190,   1,   1,\n",
       "         1,  43, 188,   1,   1,  61, 188,  64, 184, 188,  83, 147, 188,\n",
       "       199,  97, 190,  65, 123, 190, 188,   1, 188, 188,   1,   3,   1,\n",
       "        95,   1,   1,   1, 197, 186,   1, 183,   1, 126,   1,  63, 188,\n",
       "       193, 188, 188, 167,   3,  52, 175, 188, 139,   1, 172,   9, 153,\n",
       "       125, 188, 198,   3, 195,   1, 174,  46,   1,   1, 197,  75,   1,\n",
       "        76,   3,   1,   1,   1, 196,   1,   1,   3, 188, 128, 193,   1,\n",
       "       188,  44, 193,   4, 193, 188,   6,   1,   3, 188,   0,   1, 190,\n",
       "         1, 188,   3,   1, 188,   3, 188,   1,   3, 190,   1,  96, 190,\n",
       "       119, 190,   1, 190, 188, 110, 113,   1, 190, 193, 199, 188,   1,\n",
       "       197,   1,   1, 154, 188, 196, 193,   3, 190, 199,   3, 190,   3,\n",
       "       135,   1, 196,  48, 188,   1,   1,   1,   1,   3, 188, 188, 188,\n",
       "       195, 197,  12,   1,   1, 193, 188,   1, 161, 197, 131, 188,   1,\n",
       "       151, 188,   3, 188, 188, 198, 188,  49, 180, 196, 188, 188, 196,\n",
       "       188,  57,   3,   1,   1,   3, 189, 188, 188, 188,  99,   3, 188,\n",
       "       188,   1, 188, 197,   1,   1, 142, 197,   1,   1,   3, 111,   1,\n",
       "         3,   1,   1,   1, 188, 134,   3,   3,   1,  60,   1, 188, 188,\n",
       "         1,   1, 188,   0,  54,   1, 199, 158,   1,   3, 105,  81, 190,\n",
       "        28, 193,   3, 197,   1,   3, 188, 188, 188,   0,   3,   3,  18,\n",
       "       190,  82, 188, 188,   3,   1,   1,  89,  16, 190, 188, 188,  39,\n",
       "       182,   1, 188, 160, 190, 188,   1, 190, 190, 197,   1, 188, 190,\n",
       "       192,  68, 194,  71,   1, 190,  62, 188, 188, 188, 197, 188, 193,\n",
       "         1, 188, 150,   3,   3,   1, 169,   1,   1,   1,   1,   1, 188,\n",
       "       130, 188,  27,   3,   3,   1,  72, 188,  42, 188,   3,   1, 188,\n",
       "       188,   1, 188, 196,   3, 188,  84,   1,  56, 188, 188, 190, 195,\n",
       "       188,  67, 188, 190,   0, 188, 188, 190, 188, 190, 115,  94, 198,\n",
       "       188, 190, 188,   3, 190, 188,   3, 190,  88, 149, 188, 190, 190,\n",
       "        98,   7, 148, 190,  11, 188, 188,   1,   2, 188, 190,   1, 190,\n",
       "         3, 190,  51, 179,  32, 156, 192, 188,   1, 188,   1, 190, 188,\n",
       "       190,  31,   0, 101, 193, 188,  58,  86,   3,   0, 188, 192, 152,\n",
       "        77, 146, 196, 188, 190, 196,   3,   1,   1,  13, 196, 188, 188,\n",
       "         3,   3, 193, 190, 188, 187, 197, 190,   3,   1,   3, 185, 188,\n",
       "       145, 190, 190, 168,  24,  19,   1, 199, 188, 190, 190, 196,   1,\n",
       "         3,   3,   1, 195, 190,   3, 190,   1, 188, 190, 193,   0, 190,\n",
       "       188, 188, 197, 188, 188,   3, 188, 198, 190, 188,   1, 196, 190,\n",
       "         0,   3, 198, 193, 195,   0, 196, 190, 188, 190, 114,   3, 188,\n",
       "         1, 190,   3,   3,   3,   0, 188, 190,   1, 188, 188, 190, 188,\n",
       "       197,  40,   1,   1,   1, 188, 192, 188, 188, 197, 171,   1, 188,\n",
       "       188,   1, 196, 195,   1,   1,   1, 188, 138, 188, 188,   1, 137,\n",
       "       188,  15,   3, 188, 188, 198,  23, 197,   1, 196, 188,   3,   1,\n",
       "       193,   3,   1,  78, 117,   1, 196,  34, 197,   3,   3, 193, 188,\n",
       "       176,  26,   1,   3,   1, 197, 190, 198,   3,  29,   1, 188,   1,\n",
       "       188, 188,   3,   3, 188, 197, 102,   1,   3,  87, 188,   1, 118,\n",
       "       188,   5, 198, 196, 129,   3, 197, 195,   1, 188, 197, 188,  93,\n",
       "         1,   3, 196, 188,  66, 188,   1, 188, 188, 196,   3,   0, 181,\n",
       "         1,   1,  73, 188,   1,   0, 188, 193,   8, 193, 188, 188,   1,\n",
       "         3, 103,  80,  90, 188, 193, 170, 188, 188, 188,   3, 144,   3,\n",
       "       122, 190, 196, 112,   1,   1,  69, 188, 193,   3,   1,  30, 188,\n",
       "         3,   3, 192,   3,   1,   1,  10, 188, 188, 109, 188,   3, 190,\n",
       "       188,   0, 197,   1,   1, 190,   3, 193,   1,   3, 188, 188,  17,\n",
       "       195, 188,   1, 197, 188,   1, 100, 188, 193,   1, 188, 107, 188,\n",
       "       188, 188, 190,  70,   3, 188, 197, 197,   1, 190, 198, 188,   0,\n",
       "         1, 178,   1, 188,  47, 197,  38, 188, 190, 188, 163,   1, 188,\n",
       "       188,   0, 197,  74, 188, 188,   1,   3, 188,  55, 196, 190,   1,\n",
       "       190, 188, 188, 190, 190,   3, 188, 195, 188, 188,   1, 188, 188,\n",
       "       192,   3, 188,   1,   1,   3, 192, 188, 188, 197,   0, 190,   1,\n",
       "       188, 188,  41,   1,  79, 196, 188, 121,  50,   3,   1,  59,   3,\n",
       "       188,   1, 196,   1, 193,  35,   0, 197,   3,   1,   1,  37, 199,\n",
       "         0,   3, 190, 190,   1, 188, 190, 116, 120, 190,   1, 190, 188,\n",
       "       190, 196, 188,   3, 188, 188,  45,   3, 190, 190, 140,  33], dtype=int32)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
