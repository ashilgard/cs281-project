{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.cm as cm\n",
    "from itertools import product\n",
    "from itertools import combinations_with_replacement\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import time\n",
    "import math\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, SpectralClustering, AffinityPropagation\n",
    "from sklearn.metrics.pairwise import pairwise_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from utils import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "gj = pd.read_csv('./gj_df.csv')\n",
    "# gj = gj[['ifp_id', 'ctt', 'cond', 'training', 'team', 'user_id', 'value', 'fcast_date']]\n",
    "# gj['fcast_year'] = pd.to_datetime(gj['fcast_date']).dt.year\n",
    "# gj['fcast_week'] = pd.to_datetime(gj['fcast_date']).dt.week\n",
    "# gj['ifp_week'] = gj['fcast_year'].map(str) + gj['fcast_week'].map(str) + gj['ifp_id']\n",
    "# gj = gj.drop('fcast_date', axis=1)\n",
    "# gj = gj.drop_duplicates()\n",
    "# gj.to_csv('./gj_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['G', 'P', 'R', 'X'], dtype=object)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# adult = pd.read_csv('../labels.txt', delimiter='\\t', header=0, names=['user_id','website','rating'])\n",
    "# trec = pd.read_csv('../trec-rf10-crowd/trec-rf10-data.txt', delimiter='\\t')\n",
    "# gj = pd.read_csv('./filled_active_df.csv')\n",
    "\n",
    "# best_users = trec.groupby('workerID').count().sort_values('docID', ascending=False)[:150].index\n",
    "# trec = trec[trec['workerID'].isin(best_users)]\n",
    "\n",
    "# r = pd.Series([2,3,2,3], index=[1,2,0,-2])\n",
    "# trec['label_bin'] = trec['label'].map(r)\n",
    "# adult.rating.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'2014311399-2', u'2014311244-0', u'2014311394-0', u'2015241543-0',\n",
       "       u'2015231543-0', u'2015221520-0', u'2015211520-0', u'2015221543-0',\n",
       "       u'2015201520-0', u'2015101520-0',\n",
       "       ...\n",
       "       u'2015141459-0', u'2015181417-0', u'2015151459-0', u'2015161459-0',\n",
       "       u'2015171459-0', u'2015181459-0', u'2015191459-0', u'2015201459-0',\n",
       "       u'2015211459-0', u'2015221459-0'],\n",
       "      dtype='object', name=u'ifp_week', length=1548)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ifp_var = gj.groupby('ifp_week')['value'].var().sort_values()\n",
    "ifps_less_var = (ifp_var<.1).index\n",
    "ifps_less_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         ifp_id ctt  cond training  team  user_id  value  fcast_year  \\\n",
      "0        1244-0  1a     1        a   NaN       51   0.20        2015   \n",
      "1        1244-0  1a     1        a   NaN       51   0.20        2015   \n",
      "2        1244-0  1a     1        a   NaN       51   0.20        2015   \n",
      "3        1244-0  1a     1        a   NaN       51   0.20        2015   \n",
      "4        1244-0  1a     1        a   NaN       51   0.20        2015   \n",
      "5        1244-0  1a     1        a   NaN       51   0.20        2015   \n",
      "6        1244-0  1a     1        a   NaN       51   0.09        2015   \n",
      "7        1244-0  1a     1        a   NaN       51   0.09        2015   \n",
      "8        1394-0  1a     1        a   NaN       51   0.75        2014   \n",
      "9        1394-0  1a     1        a   NaN       51   0.75        2014   \n",
      "10       1394-0  1a     1        a   NaN       51   0.75        2014   \n",
      "11       1394-0  1a     1        a   NaN       51   0.75        2014   \n",
      "12       1394-0  1a     1        a   NaN       51   0.75        2014   \n",
      "13       1394-0  1a     1        a   NaN       51   0.75        2014   \n",
      "14       1394-0  1a     1        a   NaN       51   0.75        2014   \n",
      "15       1394-0  1a     1        a   NaN       51   0.75        2014   \n",
      "16       1394-0  1a     1        a   NaN       51   0.75        2014   \n",
      "17       1394-0  1a     1        a   NaN       51   0.75        2014   \n",
      "18       1394-0  1a     1        a   NaN       51   0.75        2014   \n",
      "19       1394-0  1a     1        a   NaN       51   0.75        2014   \n",
      "20       1411-2  1a     1        a   NaN       51   0.30        2015   \n",
      "21       1411-2  1a     1        a   NaN       51   0.30        2015   \n",
      "22       1411-2  1a     1        a   NaN       51   0.30        2015   \n",
      "23       1411-2  1a     1        a   NaN       51   0.30        2015   \n",
      "24       1411-2  1a     1        a   NaN       51   0.30        2015   \n",
      "25       1411-2  1a     1        a   NaN       51   0.30        2015   \n",
      "26       1411-2  1a     1        a   NaN       51   0.30        2015   \n",
      "27       1411-2  1a     1        a   NaN       51   0.30        2015   \n",
      "28       1411-2  1a     1        a   NaN       51   0.30        2015   \n",
      "29       1411-2  1a     1        a   NaN       51   0.30        2015   \n",
      "...         ...  ..   ...      ...   ...      ...    ...         ...   \n",
      "1499264  1515-0  1q     1        q   NaN   145836   0.80        2015   \n",
      "1499265  1515-0  1q     1        q   NaN   145836   0.80        2015   \n",
      "1499266  1515-0  1q     1        q   NaN   145836   0.80        2015   \n",
      "1499267  1515-0  1q     1        q   NaN   145836   0.80        2015   \n",
      "1499268  1515-0  1q     1        q   NaN   145836   0.80        2015   \n",
      "1499269  1515-0  1q     1        q   NaN   145836   0.80        2015   \n",
      "1499270  1515-0  1q     1        q   NaN   145836   0.80        2015   \n",
      "1499271  1515-0  1q     1        q   NaN   145836   0.80        2015   \n",
      "1499272  1515-0  1q     1        q   NaN   145836   0.80        2015   \n",
      "1499273  1515-0  1q     1        q   NaN   145836   0.80        2015   \n",
      "1499274  1515-0  1q     1        q   NaN   145836   0.80        2015   \n",
      "1499275  1515-0  1q     1        q   NaN   145836   0.80        2015   \n",
      "1499276  1515-0  1q     1        q   NaN   145836   0.80        2015   \n",
      "1499277  1515-0  1q     1        q   NaN   145836   0.80        2015   \n",
      "1499278  6413-0  1q     1        q   NaN   145836   0.30        2015   \n",
      "1499279  6413-0  1q     1        q   NaN   145836   0.30        2015   \n",
      "1499280  6413-0  1q     1        q   NaN   145836   0.30        2015   \n",
      "1499281  6413-0  1q     1        q   NaN   145836   0.30        2015   \n",
      "1499282  6413-0  1q     1        q   NaN   145836   0.30        2015   \n",
      "1499283  6413-0  1q     1        q   NaN   145836   0.30        2015   \n",
      "1499284  6413-0  1q     1        q   NaN   145836   0.30        2015   \n",
      "1499285  6413-0  1q     1        q   NaN   145836   0.30        2015   \n",
      "1499286  6413-0  1q     1        q   NaN   145836   0.30        2015   \n",
      "1499287  6413-0  1q     1        q   NaN   145836   0.30        2015   \n",
      "1499288  6413-0  1q     1        q   NaN   145836   0.30        2015   \n",
      "1499289  6413-0  1q     1        q   NaN   145836   0.30        2015   \n",
      "1499290  6413-0  1q     1        q   NaN   145836   0.30        2015   \n",
      "1499291  6413-0  1q     1        q   NaN   145836   0.30        2015   \n",
      "1499292  6413-0  1q     1        q   NaN   145836   0.30        2015   \n",
      "1499293  6413-0  1q     1        q   NaN   145836   0.30        2015   \n",
      "\n",
      "         fcast_week      ifp_week   bin  task_id   uid  \n",
      "0                 3   201531244-0  0.20        0     0  \n",
      "1                 4   201541244-0  0.20        1     0  \n",
      "2                 5   201551244-0  0.20        2     0  \n",
      "3                 6   201561244-0  0.20        3     0  \n",
      "4                 7   201571244-0  0.20        4     0  \n",
      "5                 8   201581244-0  0.20        5     0  \n",
      "6                 8   201581244-0  0.09        5     0  \n",
      "7                 9   201591244-0  0.09        6     0  \n",
      "8                42  2014421394-0  0.75        7     0  \n",
      "9                43  2014431394-0  0.75        8     0  \n",
      "10               44  2014441394-0  0.75        9     0  \n",
      "11               45  2014451394-0  0.75       10     0  \n",
      "12               46  2014461394-0  0.75       11     0  \n",
      "13               47  2014471394-0  0.75       12     0  \n",
      "14               48  2014481394-0  0.75       13     0  \n",
      "15               49  2014491394-0  0.75       14     0  \n",
      "16               50  2014501394-0  0.75       15     0  \n",
      "17               51  2014511394-0  0.75       16     0  \n",
      "18               52  2014521394-0  0.75       17     0  \n",
      "19                1   201411394-0  0.75       18     0  \n",
      "20                3   201531411-2  0.30       19     0  \n",
      "21                4   201541411-2  0.30       20     0  \n",
      "22                5   201551411-2  0.30       21     0  \n",
      "23                6   201561411-2  0.30       22     0  \n",
      "24                7   201571411-2  0.30       23     0  \n",
      "25                8   201581411-2  0.30       24     0  \n",
      "26                9   201591411-2  0.30       25     0  \n",
      "27               10  2015101411-2  0.30       26     0  \n",
      "28               11  2015111411-2  0.30       27     0  \n",
      "29               12  2015121411-2  0.30       28     0  \n",
      "...             ...           ...   ...      ...   ...  \n",
      "1499264           9   201591515-0  0.80     1530  2051  \n",
      "1499265          10  2015101515-0  0.80     1286  2051  \n",
      "1499266          11  2015111515-0  0.80     1287  2051  \n",
      "1499267          12  2015121515-0  0.80     1288  2051  \n",
      "1499268          13  2015131515-0  0.80     1289  2051  \n",
      "1499269          14  2015141515-0  0.80     1290  2051  \n",
      "1499270          15  2015151515-0  0.80     1291  2051  \n",
      "1499271          16  2015161515-0  0.80     1292  2051  \n",
      "1499272          17  2015171515-0  0.80     1293  2051  \n",
      "1499273          18  2015181515-0  0.80     1294  2051  \n",
      "1499274          19  2015191515-0  0.80     1295  2051  \n",
      "1499275          20  2015201515-0  0.80      242  2051  \n",
      "1499276          21  2015211515-0  0.80      243  2051  \n",
      "1499277          22  2015221515-0  0.80      244  2051  \n",
      "1499278           9   201596413-0  0.30     1426  2051  \n",
      "1499279          10  2015106413-0  0.30     1427  2051  \n",
      "1499280          11  2015116413-0  0.30     1428  2051  \n",
      "1499281          12  2015126413-0  0.30     1429  2051  \n",
      "1499282          13  2015136413-0  0.30     1430  2051  \n",
      "1499283          14  2015146413-0  0.30     1431  2051  \n",
      "1499284          15  2015156413-0  0.30     1432  2051  \n",
      "1499285          16  2015166413-0  0.30     1433  2051  \n",
      "1499286          17  2015176413-0  0.30     1434  2051  \n",
      "1499287          18  2015186413-0  0.30     1435  2051  \n",
      "1499288          19  2015196413-0  0.30     1436  2051  \n",
      "1499289          20  2015206413-0  0.30     1437  2051  \n",
      "1499290          21  2015216413-0  0.30     1438  2051  \n",
      "1499291          22  2015226413-0  0.30     1439  2051  \n",
      "1499292          23  2015236413-0  0.30     1440  2051  \n",
      "1499293          24  2015246413-0  0.30     1441  2051  \n",
      "\n",
      "[1499294 rows x 13 columns]\n"
     ]
    }
   ],
   "source": [
    "print gj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "389"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ratingcts = adult.groupby('user_id')['rating'].nunique()\n",
    "# ratingcts = ratingcts[ratingcts==4]\n",
    "# len(ratingcts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ifp_id</th>\n",
       "      <th>ctt</th>\n",
       "      <th>cond</th>\n",
       "      <th>training</th>\n",
       "      <th>team</th>\n",
       "      <th>user_id</th>\n",
       "      <th>value</th>\n",
       "      <th>fcast_year</th>\n",
       "      <th>fcast_week</th>\n",
       "      <th>ifp_week</th>\n",
       "      <th>bin</th>\n",
       "      <th>task_id</th>\n",
       "      <th>uid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1244-0</td>\n",
       "      <td>1a</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2015</td>\n",
       "      <td>3</td>\n",
       "      <td>201531244-0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1244-0</td>\n",
       "      <td>1a</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2015</td>\n",
       "      <td>4</td>\n",
       "      <td>201541244-0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1244-0</td>\n",
       "      <td>1a</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2015</td>\n",
       "      <td>5</td>\n",
       "      <td>201551244-0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1244-0</td>\n",
       "      <td>1a</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2015</td>\n",
       "      <td>6</td>\n",
       "      <td>201561244-0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1244-0</td>\n",
       "      <td>1a</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2015</td>\n",
       "      <td>7</td>\n",
       "      <td>201571244-0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ifp_id ctt  cond training  team  user_id  value  fcast_year  fcast_week  \\\n",
       "0  1244-0  1a     1        a   NaN       51    0.2        2015           3   \n",
       "1  1244-0  1a     1        a   NaN       51    0.2        2015           4   \n",
       "2  1244-0  1a     1        a   NaN       51    0.2        2015           5   \n",
       "3  1244-0  1a     1        a   NaN       51    0.2        2015           6   \n",
       "4  1244-0  1a     1        a   NaN       51    0.2        2015           7   \n",
       "\n",
       "      ifp_week  bin  task_id  uid  \n",
       "0  201531244-0  0.2        0    0  \n",
       "1  201541244-0  0.2        1    0  \n",
       "2  201551244-0  0.2        2    0  \n",
       "3  201561244-0  0.2        3    0  \n",
       "4  201571244-0  0.2        4    0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(len(adult['user_id'].unique()))\n",
    "# ratingcts = adult.groupby('user_id')['rating'].nunique()\n",
    "# ratingcts = ratingcts[ratingcts==4]\n",
    "# uids = ratingcts.index\n",
    "# print(len(uids))\n",
    "# adult = adult[adult['user_id'].isin(uids)]\n",
    "# testframe = create_user_task_ids(adult, 'user_id', 'website', 'rating', prime=True)\n",
    "# print(np.max(testframe['uid'].values))\n",
    "testframe = create_user_task_ids(gj, 'user_id', 'ifp_week', 'value', False, True)\n",
    "testframe = testframe[testframe['ifp_week'].isin(ifps_less_var)]\n",
    "testframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1499294, 13)\n",
      "Index([u'ifp_id', u'ctt', u'cond', u'training', u'team', u'user_id', u'value',\n",
      "       u'fcast_year', u'fcast_week', u'ifp_week', u'bin', u'task_id', u'uid'],\n",
      "      dtype='object')\n",
      "2052\n",
      "1548\n"
     ]
    }
   ],
   "source": [
    "print(testframe.shape)\n",
    "print testframe.columns\n",
    "print(len(testframe['uid'].unique()))\n",
    "print(len(testframe['task_id'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "testframe['value'] = (testframe['value'] - .5)*2\n",
    "# testframe['value'] = testframe['value']\n",
    "# testframe['value'] = (testframe['value']*.5) + .5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ifp_id</th>\n",
       "      <th>ctt</th>\n",
       "      <th>cond</th>\n",
       "      <th>training</th>\n",
       "      <th>team</th>\n",
       "      <th>user_id</th>\n",
       "      <th>value</th>\n",
       "      <th>fcast_year</th>\n",
       "      <th>fcast_week</th>\n",
       "      <th>ifp_week</th>\n",
       "      <th>bin</th>\n",
       "      <th>task_id</th>\n",
       "      <th>uid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1244-0</td>\n",
       "      <td>1a</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>2015</td>\n",
       "      <td>3</td>\n",
       "      <td>201531244-0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1244-0</td>\n",
       "      <td>1a</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>2015</td>\n",
       "      <td>4</td>\n",
       "      <td>201541244-0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1244-0</td>\n",
       "      <td>1a</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>2015</td>\n",
       "      <td>5</td>\n",
       "      <td>201551244-0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1244-0</td>\n",
       "      <td>1a</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>2015</td>\n",
       "      <td>6</td>\n",
       "      <td>201561244-0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1244-0</td>\n",
       "      <td>1a</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>2015</td>\n",
       "      <td>7</td>\n",
       "      <td>201571244-0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ifp_id ctt  cond training  team  user_id  value  fcast_year  fcast_week  \\\n",
       "0  1244-0  1a     1        a   NaN       51   -0.6        2015           3   \n",
       "1  1244-0  1a     1        a   NaN       51   -0.6        2015           4   \n",
       "2  1244-0  1a     1        a   NaN       51   -0.6        2015           5   \n",
       "3  1244-0  1a     1        a   NaN       51   -0.6        2015           6   \n",
       "4  1244-0  1a     1        a   NaN       51   -0.6        2015           7   \n",
       "\n",
       "      ifp_week  bin  task_id  uid  \n",
       "0  201531244-0  0.2        0    0  \n",
       "1  201541244-0  0.2        1    0  \n",
       "2  201551244-0  0.2        2    0  \n",
       "3  201561244-0  0.2        3    0  \n",
       "4  201571244-0  0.2        4    0  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "c = pd.cut(\n",
    "    testframe['value'],\n",
    "    [-np.inf, .2, .4, .6, .8, np.inf],\n",
    "    labels=[2,3,5,7,11]\n",
    ")\n",
    "testframe['bin_levels'] = c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# testframe = testframe[testframe['uid']<1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def batcher(seq, size):\n",
    "    return (seq[pos:pos + size] for pos in range(0, len(seq), size))\n",
    "\n",
    "def split(df):\n",
    "    train_df, validate_df, test_df = np.split(df.sample(frac=1), [int(.6*len(df)), int(.8*len(df))])\n",
    "    return train_df, validate_df, test_df\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, users, tasks, k=2):\n",
    "        super(Model, self).__init__()\n",
    "        self.user_lut = nn.Embedding(users, k)\n",
    "        self.task_lut = nn.Embedding(tasks, k)\n",
    "\n",
    "        self.user_bias = nn.Embedding(users, 1)\n",
    "        self.task_bias = nn.Embedding(tasks, 1)\n",
    "        self.global_bias = nn.Parameter(torch.FloatTensor(1))\n",
    "        \n",
    "    def forward(self, users, jokes):\n",
    "        user_vectors = self.user_lut(users)\n",
    "        task_vectors = self.task_lut(jokes)\n",
    "        user_bias = self.user_bias(users)\n",
    "        task_bias = self.task_bias(jokes)\n",
    "        preds = torch.bmm(user_vectors.unsqueeze(1), task_vectors.unsqueeze(2)).squeeze() \\\n",
    "                         + user_bias.squeeze() + task_bias.squeeze() + self.global_bias.expand_as(user_bias.squeeze())\n",
    "#         m = nn.Sigmoid()\n",
    "        return preds\n",
    "        \n",
    "class Model2(nn.Module):\n",
    "    def __init__(self, users, tasks, k=2):\n",
    "        super(Model2, self).__init__()\n",
    "        self.user_lut = nn.Embedding(users, k)\n",
    "        self.task_lut = nn.Embedding(tasks, k)\n",
    "\n",
    "        self.user_bias = nn.Embedding(users, 1)\n",
    "        self.user_add_bias = nn.Embedding(users, 1)\n",
    "        self.task_bias = nn.Embedding(tasks, 1)\n",
    "        self.global_bias = nn.Parameter(torch.FloatTensor(1))\n",
    "        \n",
    "    def forward(self, users, jokes):\n",
    "        user_vectors = self.user_lut(users)\n",
    "        task_vectors = self.task_lut(jokes)\n",
    "        user_bias = self.user_bias(users)\n",
    "        task_bias = self.task_bias(jokes)\n",
    "        user_add_bias = self.user_add_bias(users)\n",
    "\n",
    "        preds = torch.bmm(user_vectors.unsqueeze(1), task_vectors.unsqueeze(2)).squeeze() \\\n",
    "                          + task_bias.squeeze() + self.global_bias.expand_as(user_bias.squeeze())\n",
    "        m = nn.Tanh()\n",
    "#         return m((user_bias.squeeze() * (preds - .5)) + .5)\n",
    "#         return m(user_bias.squeeze() * preds )\n",
    "        return m(user_bias.squeeze() * preds + user_add_bias.squeeze() )\n",
    "            \n",
    "\n",
    "def val(df, model):\n",
    "    crit = nn.MSELoss(size_average=False)\n",
    "    crit2 = nn.L1Loss(size_average=False)\n",
    "    total_loss = 0.\n",
    "    total_l1 = 0.\n",
    "    total_num = 0\n",
    "    for batch in batcher(df, 100):\n",
    "        true_rating = Variable(torch.Tensor(batch.bin.values.astype(float)))\n",
    "        total_num = total_num + true_rating.size(0)\n",
    "        users = Variable(torch.LongTensor(batch.uid.values))\n",
    "        tasks = Variable(torch.LongTensor(batch.task_id.values))\n",
    "#         print(users, tasks)\n",
    "        scores = model.forward(users, tasks)\n",
    "        total_loss += crit(scores, true_rating).data[0]\n",
    "        total_l1 += crit2(scores,true_rating).data[0]\n",
    "    return math.sqrt(total_loss/total_num), total_l1/total_num\n",
    "\n",
    "\n",
    "def train(train_iter, val_iter, test_iter, model):\n",
    "    opt = optim.SGD(model.parameters(), lr=.9)\n",
    "    crit = nn.MSELoss()\n",
    "    crit2 = nn.L1Loss()\n",
    "\n",
    "    print(\"val:\", val(validate_df, model))\n",
    "    for epochs in range(1):\n",
    "        avg_loss = 0\n",
    "        avg_l1 = 0\n",
    "        total = 0\n",
    "        for i,batch in enumerate(batcher(train_df, 100)):\n",
    "            opt.zero_grad()\n",
    "            if total == 0:\n",
    "                print batch.bin.values\n",
    "            rating = Variable(torch.Tensor(batch.bin.values.astype(float)))\n",
    "#             print(rating)\n",
    "            users = Variable(torch.LongTensor(batch.uid.values))\n",
    "            tasks = Variable(torch.LongTensor(batch.task_id.values))\n",
    "#             print(users, tasks)\n",
    "            scores = model.forward(users, tasks) \n",
    "#             + torch.sum(model.user_lut.weight.data.pow(2)) + \\\n",
    "#             torch.sum(model.task_lut.weight.data.pow(2)) + torch.sum(model.user_bias.weight.data.pow(2)) + \\\n",
    "#             torch.sum(model.task_bias.weight.data.pow(2))\n",
    "            loss = crit(scores, rating)\n",
    "            #if i % 1000==0:\n",
    "            #    print (loss.data[0])\n",
    "            loss.backward()\n",
    "            avg_loss += loss.data[0]\n",
    "            avg_l1 += crit2(scores,rating).data[0]\n",
    "            total += 1\n",
    "            opt.step()\n",
    "        print(\"train:\", math.sqrt(avg_loss / float(total)), avg_l1/ float(total))\n",
    "        print(\"val:\", val(validate_df, model))\n",
    "#         print(model.user_bias.weight.data)\n",
    "    return model.user_lut.weight.data, model.user_bias.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2051, 1547)\n",
      "2051\n",
      "1547\n",
      "('val:', (0.8875000975499351, 0.758736459664383))\n",
      "         ifp_id   ctt  cond training  team  user_id  value  fcast_year  \\\n",
      "200164   1428-0    1p     1        p   NaN     8142  -0.34        2014   \n",
      "278478   1467-2    1b     1        b   NaN     9735  -1.00        2015   \n",
      "833967   1459-0    1z     1        z   NaN   125407   0.30        2015   \n",
      "433576   1459-0    1a     1        a   NaN    22231   0.14        2014   \n",
      "376032   1428-0    1a     1        a   NaN    21150  -0.34        2014   \n",
      "539241   1419-2    1h     1        h   NaN    22599  -0.20        2014   \n",
      "231171   1244-0   4b7     4        b   7.0     8811  -0.80        2014   \n",
      "588708   1414-0  4b22     4        b  22.0    22871  -0.50        2014   \n",
      "1443525  1529-0  4w74     4        w  74.0   145238  -1.00        2015   \n",
      "537973   1480-0    1h     1        h   NaN    22580   0.14        2014   \n",
      "1276099  1438-0    1z     1        z   NaN   130640   0.50        2014   \n",
      "1295975  1417-0    1z     1        z   NaN   130782  -0.50        2015   \n",
      "967002   1394-0   1r1     1        r   1.0   127310  -0.60        2014   \n",
      "76494    1502-0   5s3     5        s   3.0     3001  -0.60        2015   \n",
      "296977   1440-0  4b11     4        b  11.0    10212  -1.00        2015   \n",
      "978689   1244-0   1r1     1        r   1.0   127448  -0.60        2014   \n",
      "834324   1421-0    1z     1        z   NaN   125461   0.00        2014   \n",
      "825905   1449-0   1r1     1        r   1.0   125044   0.30        2014   \n",
      "341307   1477-0   5s2     5        s   2.0    15063  -1.00        2015   \n",
      "1090979  1435-0    1n     1        n   NaN   128570  -0.80        2015   \n",
      "1387326  1475-0    1z     1        z   NaN   132770  -0.76        2015   \n",
      "756836   1466-0   1r1     1        r   1.0   124396  -0.94        2015   \n",
      "141333   1478-0  4h46     4        h  46.0     4938  -0.80        2015   \n",
      "1157101  1427-0    1z     1        z   NaN   129689  -1.00        2014   \n",
      "772110   1482-0   1r1     1        r   1.0   124544  -0.08        2014   \n",
      "472035   1459-0  4h48     4        h  48.0    22299  -0.50        2014   \n",
      "210309   1514-0  4b19     4        b  19.0     8224  -1.00        2015   \n",
      "19541    1482-0   5s2     5        s   2.0      803  -0.70        2014   \n",
      "709308   1487-0    1z     1        z   NaN   123651  -0.70        2015   \n",
      "649661   1440-0  4h31     4        h  31.0    23278  -0.50        2014   \n",
      "...         ...   ...   ...      ...   ...      ...    ...         ...   \n",
      "778788   1417-0   1r1     1        r   1.0   124619  -0.20        2014   \n",
      "953334   1455-0   1r1     1        r   1.0   127165  -0.30        2015   \n",
      "925934   1413-0   1r1     1        r   1.0   126972  -0.98        2014   \n",
      "7315     1430-0    1a     1        a   NaN      362   0.50        2014   \n",
      "544822   1434-0    1h     1        h   NaN    22641  -0.52        2014   \n",
      "198762   1435-0  4b17     4        b  17.0     8124  -0.30        2014   \n",
      "812396   1414-0   1r1     1        r   1.0   124891   0.82        2014   \n",
      "1065138  1244-0   1r1     1        r   1.0   128308   0.44        2014   \n",
      "229541   1409-0  4h47     4        h  47.0     8809   0.80        2014   \n",
      "1332693  1535-0    1z     1        z   NaN   131131  -1.00        2015   \n",
      "180350   1466-0   5s5     5        s   5.0     6372  -0.98        2015   \n",
      "1363191  1394-0    1z     1        z   NaN   131434  -0.70        2014   \n",
      "1200440  1420-0    1z     1        z   NaN   129994   0.40        2015   \n",
      "917782   1420-0   1r1     1        r   1.0   126876   0.10        2014   \n",
      "919239   1399-2   1r1     1        r   1.0   126899  -0.70        2014   \n",
      "514916   1455-0    1b     1        b   NaN    22432   0.20        2014   \n",
      "1213574  1409-0    1z     1        z   NaN   130072   0.20        2014   \n",
      "1375479  1411-2    1z     1        z   NaN   132023  -0.70        2015   \n",
      "640370   1467-2  4h48     4        h  48.0    23211  -0.20        2014   \n",
      "1372018  1413-0    1z     1        z   NaN   131496  -0.78        2014   \n",
      "306048   1482-0   5s2     5        s   2.0    13190  -0.50        2014   \n",
      "790548   1434-0   1r1     1        r   1.0   124756  -0.30        2014   \n",
      "532347   1417-0    1p     1        p   NaN    22540  -0.60        2014   \n",
      "67242    1472-2   5s8     5        s   8.0     2880  -0.98        2015   \n",
      "367437   1430-0    1a     1        a   NaN    21046  -0.68        2015   \n",
      "274554   1449-0    1p     1        p   NaN     9657  -0.40        2014   \n",
      "101000   1410-0  4h52     4        h  52.0     3626  -0.72        2014   \n",
      "1484697  1535-0  4u26     4        u  26.0   145576   0.34        2015   \n",
      "3886     1483-0   5s1     5        s   1.0      149   0.50        2015   \n",
      "1282799  1482-0    1z     1        z   NaN   130690  -0.60        2015   \n",
      "\n",
      "         fcast_week      ifp_week   bin  task_id   uid  \n",
      "200164           46  2014461428-0  0.33       76   249  \n",
      "278478           23  2015231467-2  0.00      170   342  \n",
      "833967            8   201581459-0  0.65      803  1095  \n",
      "433576           50  2014501459-0  0.57      792   547  \n",
      "376032           42  2014421428-0  0.33       72   478  \n",
      "539241           40  2014401419-2  0.40      448   675  \n",
      "231171           42  2014421244-0  0.10      302   287  \n",
      "588708           37  2014371414-0  0.25       42   750  \n",
      "1443525          21  2015211529-0  0.00     1345  1957  \n",
      "537973            1   201411480-0  0.57     1052   672  \n",
      "1276099          49  2014491438-0  0.75      699  1682  \n",
      "1295975           7   201571417-0  0.25      427  1711  \n",
      "967002           49  2014491394-0  0.20       14  1274  \n",
      "76494             7   201571502-0  0.20     1180    86  \n",
      "296977            8   201581440-0  0.00      727   367  \n",
      "978689           46  2014461244-0  0.20      306  1291  \n",
      "834324           46  2014461421-0  0.50      506  1096  \n",
      "825905           46  2014461449-0  0.65      123  1082  \n",
      "341307           24  2015241477-0  0.00     1035   430  \n",
      "1090979          20  2015201435-0  0.10      653  1433  \n",
      "1387326           3   201531475-0  0.12      978  1846  \n",
      "756836            9   201591466-0  0.03      875   994  \n",
      "141333            8   201581478-0  0.10     1047   163  \n",
      "1157101          45  2014451427-0  0.00       63  1518  \n",
      "772110           52  2014521482-0  0.46     1478  1009  \n",
      "472035           47  2014471459-0  0.25      789   583  \n",
      "210309           20  2015201514-0  0.00     1284   262  \n",
      "19541            51  2014511482-0  0.15     1520    22  \n",
      "709308           20  2015201487-0  0.15      210   931  \n",
      "649661           46  2014461440-0  0.25      712   843  \n",
      "...             ...           ...   ...      ...   ...  \n",
      "778788           42  2014421417-0  0.40      409  1018  \n",
      "953334            1   201511455-0  0.35      773  1257  \n",
      "925934           41  2014411413-0  0.01      382  1224  \n",
      "7315             43  2014431430-0  0.75      581     9  \n",
      "544822           41  2014411434-0  0.24      605   685  \n",
      "198762           48  2014481435-0  0.35      628   246  \n",
      "812396           35  2014351414-0  0.91       40  1064  \n",
      "1065138          41  2014411244-0  0.72      301  1398  \n",
      "229541           35  2014351409-0  0.90      340   286  \n",
      "1332693          15  2015151535-0  0.00     1535  1763  \n",
      "180350            7   201571466-0  0.01      873   220  \n",
      "1363191          49  2014491394-0  0.15       14  1806  \n",
      "1200440          15  2015151420-0  0.70      490  1576  \n",
      "917782           44  2014441420-0  0.55      466  1212  \n",
      "919239           46  2014461399-2  0.15      332  1214  \n",
      "514916           45  2014451455-0  0.60     1507   638  \n",
      "1213574          36  2014361409-0  0.60      341  1594  \n",
      "1375479           7   201571411-2  0.15       23  1825  \n",
      "640370           49  2014491467-2  0.40      893   830  \n",
      "1372018          48  2014481413-0  0.11      389  1819  \n",
      "306048            1   201411482-0  0.25     1479   380  \n",
      "790548           43  2014431434-0  0.35      607  1035  \n",
      "532347           52  2014521417-0  0.20      419   664  \n",
      "67242            23  2015231472-2  0.01      948    79  \n",
      "367437           13  2015131430-0  0.16       94   469  \n",
      "274554           47  2014471449-0  0.30      124   339  \n",
      "101000           34  2014341410-0  0.14     1449   115  \n",
      "1484697          16  2015161535-0  0.67     1361  2024  \n",
      "3886              4   201541483-0  0.75     1099     5  \n",
      "1282799           1   201511482-0  0.20     1074  1691  \n",
      "\n",
      "[100 rows x 13 columns]\n",
      "200164     0.33\n",
      "278478     0.00\n",
      "833967     0.65\n",
      "433576     0.57\n",
      "376032     0.33\n",
      "539241     0.40\n",
      "231171     0.10\n",
      "588708     0.25\n",
      "1443525    0.00\n",
      "537973     0.57\n",
      "1276099    0.75\n",
      "1295975    0.25\n",
      "967002     0.20\n",
      "76494      0.20\n",
      "296977     0.00\n",
      "978689     0.20\n",
      "834324     0.50\n",
      "825905     0.65\n",
      "341307     0.00\n",
      "1090979    0.10\n",
      "1387326    0.12\n",
      "756836     0.03\n",
      "141333     0.10\n",
      "1157101    0.00\n",
      "772110     0.46\n",
      "472035     0.25\n",
      "210309     0.00\n",
      "19541      0.15\n",
      "709308     0.15\n",
      "649661     0.25\n",
      "           ... \n",
      "778788     0.40\n",
      "953334     0.35\n",
      "925934     0.01\n",
      "7315       0.75\n",
      "544822     0.24\n",
      "198762     0.35\n",
      "812396     0.91\n",
      "1065138    0.72\n",
      "229541     0.90\n",
      "1332693    0.00\n",
      "180350     0.01\n",
      "1363191    0.15\n",
      "1200440    0.70\n",
      "917782     0.55\n",
      "919239     0.15\n",
      "514916     0.60\n",
      "1213574    0.60\n",
      "1375479    0.15\n",
      "640370     0.40\n",
      "1372018    0.11\n",
      "306048     0.25\n",
      "790548     0.35\n",
      "532347     0.20\n",
      "67242      0.01\n",
      "367437     0.16\n",
      "274554     0.30\n",
      "101000     0.14\n",
      "1484697    0.67\n",
      "3886       0.75\n",
      "1282799    0.20\n",
      "Name: bin, dtype: float64\n",
      "[ 0.33  0.    0.65  0.57  0.33  0.4   0.1   0.25  0.    0.57  0.75  0.25\n",
      "  0.2   0.2   0.    0.2   0.5   0.65  0.    0.1   0.12  0.03  0.1   0.\n",
      "  0.46  0.25  0.    0.15  0.15  0.25  0.22  1.    0.9   0.75  0.05  0.39\n",
      "  0.15  1.    0.15  0.75  0.24  0.15  0.    0.85  0.15  1.    0.    0.75\n",
      "  0.54  0.68  0.25  0.65  0.3   0.01  1.    0.15  1.    0.    0.2   1.\n",
      "  0.09  0.5   0.01  0.1   1.    0.62  0.8   0.    0.1   0.3   0.4   0.35\n",
      "  0.01  0.75  0.24  0.35  0.91  0.72  0.9   0.    0.01  0.15  0.7   0.55\n",
      "  0.15  0.6   0.6   0.15  0.4   0.11  0.25  0.35  0.2   0.01  0.16  0.3\n",
      "  0.14  0.67  0.75  0.2 ]\n",
      "('train:', 0.623172278066906, 0.4706275628381912)\n",
      "('val:', (0.4772414779591901, 0.3500622458349797))\n"
     ]
    }
   ],
   "source": [
    "train_df, validate_df, test_df = split(testframe)\n",
    "validate_df = validate_df[validate_df['uid'].isin(train_df['uid'].values)][validate_df['task_id'].isin(train_df['task_id'].values)]\n",
    "\n",
    "users = int(np.max(train_df.uid.unique()))\n",
    "tasks = int(np.max(train_df.task_id.unique()))\n",
    "print(users, tasks)\n",
    "print(int(np.max(validate_df.uid.unique())))\n",
    "print(int(np.max(validate_df.task_id.unique())))\n",
    "model = Model2(users+1, tasks+1, k=4)\n",
    "user_vec, user_bias = train(train_df, validate_df, test_df, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "user_vec, user_bias = model.user_lut.weight.data, model.user_bias.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      user_id  gender_yr4  citizen_yr4  workhours_yr4  tradeexp_yr4  \\\n",
      "0        -1.0         1.0         -1.0            5.0           3.0   \n",
      "1         2.0         1.0          1.0            4.0           2.0   \n",
      "2         4.0         1.0          1.0            6.0           1.0   \n",
      "3         5.0        -1.0          1.0            4.0           3.0   \n",
      "4         7.0         1.0          1.0            4.0           3.0   \n",
      "5         8.0         1.0          1.0            4.0           1.0   \n",
      "6         9.0         1.0          1.0            2.0           3.0   \n",
      "7        10.0         1.0          1.0            1.0           3.0   \n",
      "8        11.0         1.0          1.0            1.0           2.0   \n",
      "9        12.0         1.0         -1.0            6.0           2.0   \n",
      "10       13.0         1.0          1.0            5.0           1.0   \n",
      "11       14.0         1.0          1.0            5.0           3.0   \n",
      "12       15.0         1.0         -1.0            4.0           2.0   \n",
      "13       16.0         1.0          1.0            5.0           4.0   \n",
      "14       17.0         1.0          1.0            5.0           3.0   \n",
      "15       18.0         1.0          1.0            5.0           1.0   \n",
      "16       19.0         1.0         -1.0            1.0           1.0   \n",
      "17       20.0         1.0          1.0            5.0           4.0   \n",
      "18       21.0         1.0          1.0            5.0           4.0   \n",
      "19       22.0         1.0          1.0            5.0           2.0   \n",
      "20       23.0         1.0          1.0            5.0           1.0   \n",
      "21       25.0         1.0          1.0            1.0           2.0   \n",
      "22       26.0         1.0          1.0            5.0           2.0   \n",
      "23       27.0         1.0          1.0            6.0           3.0   \n",
      "24       28.0         1.0          1.0            5.0           2.0   \n",
      "25       29.0         1.0          1.0            5.0           1.0   \n",
      "26       30.0         1.0          1.0            5.0           3.0   \n",
      "27       31.0         1.0          1.0            3.0           3.0   \n",
      "28       32.0         1.0          1.0            6.0           1.0   \n",
      "29       34.0         1.0          1.0            1.0           4.0   \n",
      "...       ...         ...          ...            ...           ...   \n",
      "1682   2021.0         1.0         -1.0            4.0           3.0   \n",
      "1683   2022.0         1.0         -1.0           -1.0          -1.0   \n",
      "1684   2023.0         1.0         -1.0           -1.0          -1.0   \n",
      "1685   2024.0         1.0          1.0            1.0           1.0   \n",
      "1686   2025.0         1.0          1.0           -1.0          -1.0   \n",
      "1687   2026.0         1.0          1.0           -1.0          -1.0   \n",
      "1688   2027.0         1.0          1.0           -1.0          -1.0   \n",
      "1689   2028.0         1.0          1.0           -1.0          -1.0   \n",
      "1690   2029.0        -1.0          1.0           -1.0          -1.0   \n",
      "1691   2030.0         1.0         -1.0           -1.0          -1.0   \n",
      "1692   2031.0         1.0          1.0           -1.0          -1.0   \n",
      "1693   2032.0         1.0         -1.0           -1.0          -1.0   \n",
      "1694   2033.0        -1.0          1.0           -1.0          -1.0   \n",
      "1695   2034.0         1.0          1.0           -1.0          -1.0   \n",
      "1696   2035.0        -1.0          1.0           -1.0          -1.0   \n",
      "1697   2036.0         1.0          1.0           -1.0          -1.0   \n",
      "1698   2037.0        -1.0          1.0           -1.0          -1.0   \n",
      "1699   2038.0         1.0          1.0           -1.0          -1.0   \n",
      "1700   2039.0         1.0         -1.0           -1.0          -1.0   \n",
      "1701   2040.0         1.0          1.0           -1.0          -1.0   \n",
      "1702   2041.0         1.0          1.0           -1.0          -1.0   \n",
      "1703   2042.0        -1.0          1.0           -1.0          -1.0   \n",
      "1704   2044.0         1.0          1.0           -1.0          -1.0   \n",
      "1705   2045.0         1.0          1.0           -1.0          -1.0   \n",
      "1706   2046.0         1.0         -1.0            1.0           3.0   \n",
      "1707   2047.0         1.0          1.0           -1.0          -1.0   \n",
      "1708   2048.0         1.0          1.0           -1.0          -1.0   \n",
      "1709   2049.0         1.0         -1.0           -1.0          -1.0   \n",
      "1710   2050.0        -1.0         -1.0           -1.0          -1.0   \n",
      "1711   2051.0         1.0         -1.0           -1.0          -1.0   \n",
      "\n",
      "      tradeenjoy_yr4  newshrs_yr4  raven_score_yr4  crt_score_yr4  \\\n",
      "0                3.0          7.0              9.0           16.0   \n",
      "1                4.0          3.0              6.0            9.0   \n",
      "2                7.0         20.0             11.0           15.0   \n",
      "3                4.0          4.0             12.0           18.0   \n",
      "4                5.0          4.0             12.0           15.0   \n",
      "5                4.0          5.0              5.0           13.0   \n",
      "6                5.0         10.0              9.0           18.0   \n",
      "7                4.0          7.0             10.0           16.0   \n",
      "8                5.0         10.0              5.0           -1.0   \n",
      "9                5.0          6.0             10.0           18.0   \n",
      "10               7.0          6.0              6.0           18.0   \n",
      "11               5.0          2.5             11.0           13.0   \n",
      "12               3.0          2.0             12.0           18.0   \n",
      "13               5.0          0.0             10.0           17.0   \n",
      "14               4.0          3.0             10.0           17.0   \n",
      "15               7.0         10.0              5.0           10.0   \n",
      "16               7.0         10.0              3.0           12.0   \n",
      "17               4.0          2.0              6.0           16.0   \n",
      "18               5.0          2.0              9.0            7.0   \n",
      "19               6.0          6.0              5.0           17.0   \n",
      "20               7.0         10.0              4.0            9.0   \n",
      "21               4.0          4.0              8.0           18.0   \n",
      "22               4.0         15.0              7.0           18.0   \n",
      "23               4.0          6.0              1.0           10.0   \n",
      "24               4.0          1.0              9.0           16.0   \n",
      "25               7.0          4.0              9.0           18.0   \n",
      "26               5.0         10.0              8.0           17.0   \n",
      "27               4.0          4.0              7.0           15.0   \n",
      "28               7.0          5.0             10.0           17.0   \n",
      "29               5.0          1.0             11.0           18.0   \n",
      "...              ...          ...              ...            ...   \n",
      "1682             6.0          5.0             -1.0           -1.0   \n",
      "1683            -1.0          4.0             -1.0           -1.0   \n",
      "1684            -1.0         11.0             -1.0           -1.0   \n",
      "1685             6.0          4.0              9.0           14.0   \n",
      "1686            -1.0          5.0             -1.0           -1.0   \n",
      "1687            -1.0          6.0             -1.0           -1.0   \n",
      "1688            -1.0          7.0             -1.0           -1.0   \n",
      "1689            -1.0         15.0             -1.0           -1.0   \n",
      "1690            -1.0          8.0             -1.0           -1.0   \n",
      "1691            -1.0          3.0             -1.0           -1.0   \n",
      "1692            -1.0         40.0             -1.0           -1.0   \n",
      "1693            -1.0         10.0             -1.0           -1.0   \n",
      "1694            -1.0          8.0             -1.0           -1.0   \n",
      "1695            -1.0          2.0             -1.0           -1.0   \n",
      "1696            -1.0          5.0             -1.0           -1.0   \n",
      "1697            -1.0         20.0             -1.0           -1.0   \n",
      "1698            -1.0          1.0             -1.0           -1.0   \n",
      "1699            -1.0          6.0             -1.0           -1.0   \n",
      "1700            -1.0         30.0             -1.0           -1.0   \n",
      "1701            -1.0         20.0             -1.0           -1.0   \n",
      "1702            -1.0          3.0             -1.0           -1.0   \n",
      "1703            -1.0          4.0             -1.0           -1.0   \n",
      "1704            -1.0         20.0             -1.0           -1.0   \n",
      "1705            -1.0         15.0             -1.0           -1.0   \n",
      "1706             4.0         12.0             -1.0           -1.0   \n",
      "1707            -1.0         16.0             -1.0           -1.0   \n",
      "1708            -1.0         35.0             -1.0           -1.0   \n",
      "1709            -1.0         15.0             -1.0           -1.0   \n",
      "1710            -1.0          1.0             -1.0           -1.0   \n",
      "1711            -1.0          5.0             -1.0           -1.0   \n",
      "\n",
      "      crt_1_cor_yr4             ...              motivation_2  motivation_3  \\\n",
      "0               1.0             ...                       5.0           1.0   \n",
      "1               1.0             ...                       5.0           6.0   \n",
      "2               1.0             ...                       2.0           2.0   \n",
      "3               1.0             ...                       5.0           2.0   \n",
      "4               1.0             ...                       5.0           4.0   \n",
      "5               1.0             ...                       6.0           4.0   \n",
      "6               1.0             ...                       5.0           2.0   \n",
      "7               1.0             ...                       6.0           1.0   \n",
      "8              -1.0             ...                       5.0           5.0   \n",
      "9               1.0             ...                       2.0           2.0   \n",
      "10              1.0             ...                       4.0           1.0   \n",
      "11             -1.0             ...                       5.0           5.0   \n",
      "12              1.0             ...                       5.0           1.0   \n",
      "13              1.0             ...                       6.0           3.0   \n",
      "14              1.0             ...                       3.0           1.0   \n",
      "15              1.0             ...                       5.0           2.0   \n",
      "16              1.0             ...                       7.0           1.0   \n",
      "17             -1.0             ...                       4.0           2.0   \n",
      "18             -1.0             ...                       4.0           1.0   \n",
      "19              1.0             ...                       2.0           1.0   \n",
      "20             -1.0             ...                       5.0           4.0   \n",
      "21              1.0             ...                       2.0           1.0   \n",
      "22              1.0             ...                       7.0           2.0   \n",
      "23             -1.0             ...                       5.0           2.0   \n",
      "24              1.0             ...                       5.0           2.0   \n",
      "25              1.0             ...                       5.0           5.0   \n",
      "26              1.0             ...                       2.0           7.0   \n",
      "27              1.0             ...                       3.0           5.0   \n",
      "28              1.0             ...                       5.0           3.0   \n",
      "29              1.0             ...                       4.0           1.0   \n",
      "...             ...             ...                       ...           ...   \n",
      "1682           -1.0             ...                      -1.0          -1.0   \n",
      "1683           -1.0             ...                      -1.0          -1.0   \n",
      "1684           -1.0             ...                      -1.0          -1.0   \n",
      "1685           -1.0             ...                       4.0           2.0   \n",
      "1686           -1.0             ...                      -1.0          -1.0   \n",
      "1687           -1.0             ...                      -1.0          -1.0   \n",
      "1688           -1.0             ...                      -1.0          -1.0   \n",
      "1689           -1.0             ...                      -1.0          -1.0   \n",
      "1690           -1.0             ...                      -1.0          -1.0   \n",
      "1691           -1.0             ...                      -1.0          -1.0   \n",
      "1692           -1.0             ...                      -1.0          -1.0   \n",
      "1693           -1.0             ...                      -1.0          -1.0   \n",
      "1694           -1.0             ...                      -1.0          -1.0   \n",
      "1695           -1.0             ...                      -1.0          -1.0   \n",
      "1696           -1.0             ...                      -1.0          -1.0   \n",
      "1697           -1.0             ...                      -1.0          -1.0   \n",
      "1698           -1.0             ...                      -1.0          -1.0   \n",
      "1699           -1.0             ...                      -1.0          -1.0   \n",
      "1700           -1.0             ...                      -1.0          -1.0   \n",
      "1701           -1.0             ...                      -1.0          -1.0   \n",
      "1702           -1.0             ...                      -1.0          -1.0   \n",
      "1703           -1.0             ...                      -1.0          -1.0   \n",
      "1704           -1.0             ...                      -1.0          -1.0   \n",
      "1705           -1.0             ...                      -1.0          -1.0   \n",
      "1706           -1.0             ...                      -1.0          -1.0   \n",
      "1707           -1.0             ...                      -1.0          -1.0   \n",
      "1708           -1.0             ...                      -1.0          -1.0   \n",
      "1709           -1.0             ...                      -1.0          -1.0   \n",
      "1710           -1.0             ...                      -1.0          -1.0   \n",
      "1711           -1.0             ...                      -1.0          -1.0   \n",
      "\n",
      "      motivation_4  motivation_5  motivation_6  motivation_7  motivation_8  \\\n",
      "0              7.0           5.0           6.0           5.0           5.0   \n",
      "1              5.0           6.0           6.0           4.0           6.0   \n",
      "2              5.0           4.0           5.0           4.0           4.0   \n",
      "3              5.0           5.0           7.0           5.0           5.0   \n",
      "4              5.0           3.0           5.0           6.0           5.0   \n",
      "5              7.0           6.0           7.0           6.0           6.0   \n",
      "6              6.0           5.0           5.0           5.0           5.0   \n",
      "7              7.0           6.0           7.0           6.0           5.0   \n",
      "8              5.0           5.0           5.0           5.0           5.0   \n",
      "9              6.0           2.0           6.0           2.0           2.0   \n",
      "10             5.0           4.0           6.0           4.0           2.0   \n",
      "11             5.0           5.0           5.0           5.0           5.0   \n",
      "12             5.0           5.0           6.0           5.0           4.0   \n",
      "13             5.0           6.0           2.0           6.0           6.0   \n",
      "14             7.0           5.0           6.0           2.0           3.0   \n",
      "15             6.0           5.0           6.0           4.0           5.0   \n",
      "16             7.0           6.0           7.0           7.0           6.0   \n",
      "17             5.0           4.0           5.0           2.0           4.0   \n",
      "18             4.0           4.0           5.0           4.0           4.0   \n",
      "19             6.0           4.0           6.0           4.0           2.0   \n",
      "20             5.0           5.0           5.0           5.0           5.0   \n",
      "21             6.0           2.0           6.0           2.0           2.0   \n",
      "22             6.0           7.0           7.0           4.0           7.0   \n",
      "23             5.0           5.0           5.0           5.0           5.0   \n",
      "24             6.0           6.0           6.0           6.0           4.0   \n",
      "25             7.0           5.0           6.0           5.0           5.0   \n",
      "26             4.0           3.0           3.0           3.0           1.0   \n",
      "27             6.0           3.0           6.0           2.0           2.0   \n",
      "28             6.0           5.0           6.0           4.0           4.0   \n",
      "29             6.0           4.0           6.0           4.0           4.0   \n",
      "...            ...           ...           ...           ...           ...   \n",
      "1682          -1.0          -1.0          -1.0          -1.0          -1.0   \n",
      "1683          -1.0          -1.0          -1.0          -1.0          -1.0   \n",
      "1684          -1.0          -1.0          -1.0          -1.0          -1.0   \n",
      "1685           4.0           5.0           5.0           5.0           5.0   \n",
      "1686          -1.0          -1.0          -1.0          -1.0          -1.0   \n",
      "1687          -1.0          -1.0          -1.0          -1.0          -1.0   \n",
      "1688          -1.0          -1.0          -1.0          -1.0          -1.0   \n",
      "1689          -1.0          -1.0          -1.0          -1.0          -1.0   \n",
      "1690          -1.0          -1.0          -1.0          -1.0          -1.0   \n",
      "1691          -1.0          -1.0          -1.0          -1.0          -1.0   \n",
      "1692          -1.0          -1.0          -1.0          -1.0          -1.0   \n",
      "1693          -1.0          -1.0          -1.0          -1.0          -1.0   \n",
      "1694          -1.0          -1.0          -1.0          -1.0          -1.0   \n",
      "1695          -1.0          -1.0          -1.0          -1.0          -1.0   \n",
      "1696          -1.0          -1.0          -1.0          -1.0          -1.0   \n",
      "1697          -1.0          -1.0          -1.0          -1.0          -1.0   \n",
      "1698          -1.0          -1.0          -1.0          -1.0          -1.0   \n",
      "1699          -1.0          -1.0          -1.0          -1.0          -1.0   \n",
      "1700          -1.0          -1.0          -1.0          -1.0          -1.0   \n",
      "1701          -1.0          -1.0          -1.0          -1.0          -1.0   \n",
      "1702          -1.0          -1.0          -1.0          -1.0          -1.0   \n",
      "1703          -1.0          -1.0          -1.0          -1.0          -1.0   \n",
      "1704          -1.0          -1.0          -1.0          -1.0          -1.0   \n",
      "1705          -1.0          -1.0          -1.0          -1.0          -1.0   \n",
      "1706          -1.0          -1.0          -1.0          -1.0          -1.0   \n",
      "1707          -1.0          -1.0          -1.0          -1.0          -1.0   \n",
      "1708          -1.0          -1.0          -1.0          -1.0          -1.0   \n",
      "1709          -1.0          -1.0          -1.0          -1.0          -1.0   \n",
      "1710          -1.0          -1.0          -1.0          -1.0          -1.0   \n",
      "1711          -1.0          -1.0          -1.0          -1.0          -1.0   \n",
      "\n",
      "      motivation_intrinsic_score  motivation_prosocial_score  \\\n",
      "0                            6.0                         5.0   \n",
      "1                            5.0                         5.0   \n",
      "2                            5.0                         4.0   \n",
      "3                            5.0                         5.0   \n",
      "4                            5.0                         5.0   \n",
      "5                            7.0                         6.0   \n",
      "6                            6.0                         5.0   \n",
      "7                            7.0                         6.0   \n",
      "8                            5.0                         5.0   \n",
      "9                            6.0                         2.0   \n",
      "10                           6.0                         4.0   \n",
      "11                           5.0                         5.0   \n",
      "12                           6.0                         5.0   \n",
      "13                           3.0                         6.0   \n",
      "14                           6.0                         3.0   \n",
      "15                           6.0                         5.0   \n",
      "16                           7.0                         7.0   \n",
      "17                           5.0                         4.0   \n",
      "18                           5.0                         4.0   \n",
      "19                           6.0                         3.0   \n",
      "20                           5.0                         5.0   \n",
      "21                           6.0                         2.0   \n",
      "22                           7.0                         6.0   \n",
      "23                           5.0                         5.0   \n",
      "24                           6.0                         5.0   \n",
      "25                           6.0                         5.0   \n",
      "26                           3.0                         2.0   \n",
      "27                           6.0                         3.0   \n",
      "28                           6.0                         5.0   \n",
      "29                           6.0                         4.0   \n",
      "...                          ...                         ...   \n",
      "1682                        -1.0                        -1.0   \n",
      "1683                        -1.0                        -1.0   \n",
      "1684                        -1.0                        -1.0   \n",
      "1685                         4.0                         5.0   \n",
      "1686                        -1.0                        -1.0   \n",
      "1687                        -1.0                        -1.0   \n",
      "1688                        -1.0                        -1.0   \n",
      "1689                        -1.0                        -1.0   \n",
      "1690                        -1.0                        -1.0   \n",
      "1691                        -1.0                        -1.0   \n",
      "1692                        -1.0                        -1.0   \n",
      "1693                        -1.0                        -1.0   \n",
      "1694                        -1.0                        -1.0   \n",
      "1695                        -1.0                        -1.0   \n",
      "1696                        -1.0                        -1.0   \n",
      "1697                        -1.0                        -1.0   \n",
      "1698                        -1.0                        -1.0   \n",
      "1699                        -1.0                        -1.0   \n",
      "1700                        -1.0                        -1.0   \n",
      "1701                        -1.0                        -1.0   \n",
      "1702                        -1.0                        -1.0   \n",
      "1703                        -1.0                        -1.0   \n",
      "1704                        -1.0                        -1.0   \n",
      "1705                        -1.0                        -1.0   \n",
      "1706                        -1.0                        -1.0   \n",
      "1707                        -1.0                        -1.0   \n",
      "1708                        -1.0                        -1.0   \n",
      "1709                        -1.0                        -1.0   \n",
      "1710                        -1.0                        -1.0   \n",
      "1711                        -1.0                        -1.0   \n",
      "\n",
      "      motivation_extrinsic_score  \n",
      "0                            1.0  \n",
      "1                            6.0  \n",
      "2                            2.0  \n",
      "3                            2.0  \n",
      "4                            4.0  \n",
      "5                            4.0  \n",
      "6                            2.0  \n",
      "7                            1.0  \n",
      "8                            5.0  \n",
      "9                            2.0  \n",
      "10                           1.0  \n",
      "11                           5.0  \n",
      "12                           1.0  \n",
      "13                           3.0  \n",
      "14                           1.0  \n",
      "15                           2.0  \n",
      "16                           1.0  \n",
      "17                           2.0  \n",
      "18                           1.0  \n",
      "19                           1.0  \n",
      "20                           4.0  \n",
      "21                           1.0  \n",
      "22                           2.0  \n",
      "23                           2.0  \n",
      "24                           2.0  \n",
      "25                           5.0  \n",
      "26                           7.0  \n",
      "27                           5.0  \n",
      "28                           3.0  \n",
      "29                           1.0  \n",
      "...                          ...  \n",
      "1682                        -1.0  \n",
      "1683                        -1.0  \n",
      "1684                        -1.0  \n",
      "1685                         2.0  \n",
      "1686                        -1.0  \n",
      "1687                        -1.0  \n",
      "1688                        -1.0  \n",
      "1689                        -1.0  \n",
      "1690                        -1.0  \n",
      "1691                        -1.0  \n",
      "1692                        -1.0  \n",
      "1693                        -1.0  \n",
      "1694                        -1.0  \n",
      "1695                        -1.0  \n",
      "1696                        -1.0  \n",
      "1697                        -1.0  \n",
      "1698                        -1.0  \n",
      "1699                        -1.0  \n",
      "1700                        -1.0  \n",
      "1701                        -1.0  \n",
      "1702                        -1.0  \n",
      "1703                        -1.0  \n",
      "1704                        -1.0  \n",
      "1705                        -1.0  \n",
      "1706                        -1.0  \n",
      "1707                        -1.0  \n",
      "1708                        -1.0  \n",
      "1709                        -1.0  \n",
      "1710                        -1.0  \n",
      "1711                        -1.0  \n",
      "\n",
      "[1712 rows x 168 columns]\n",
      "Index([u'user_id', u'gender_yr4', u'citizen_yr4', u'workhours_yr4',\n",
      "       u'tradeexp_yr4', u'tradeenjoy_yr4', u'newshrs_yr4', u'raven_score_yr4',\n",
      "       u'crt_score_yr4', u'crt_1_cor_yr4',\n",
      "       ...\n",
      "       u'motivation_2', u'motivation_3', u'motivation_4', u'motivation_5',\n",
      "       u'motivation_6', u'motivation_7', u'motivation_8',\n",
      "       u'motivation_intrinsic_score', u'motivation_prosocial_score',\n",
      "       u'motivation_extrinsic_score'],\n",
      "      dtype='object', length=168)\n"
     ]
    }
   ],
   "source": [
    "first_import = pd.read_csv('./clean_frame.csv',delimiter='\\t', low_memory=False)\n",
    "first_import = first_import.drop(['Unnamed: 0'],axis=1)\n",
    "print first_import\n",
    "print first_import.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         user_id_x  gender_yr4  citizen_yr4  workhours_yr4  tradeexp_yr4  \\\n",
      "0              2.0         1.0          1.0            4.0           2.0   \n",
      "1              2.0         1.0          1.0            4.0           2.0   \n",
      "2              2.0         1.0          1.0            4.0           2.0   \n",
      "3              2.0         1.0          1.0            4.0           2.0   \n",
      "4              2.0         1.0          1.0            4.0           2.0   \n",
      "5              2.0         1.0          1.0            4.0           2.0   \n",
      "6              2.0         1.0          1.0            4.0           2.0   \n",
      "7              2.0         1.0          1.0            4.0           2.0   \n",
      "8              2.0         1.0          1.0            4.0           2.0   \n",
      "9              2.0         1.0          1.0            4.0           2.0   \n",
      "10             2.0         1.0          1.0            4.0           2.0   \n",
      "11             2.0         1.0          1.0            4.0           2.0   \n",
      "12             2.0         1.0          1.0            4.0           2.0   \n",
      "13             2.0         1.0          1.0            4.0           2.0   \n",
      "14             2.0         1.0          1.0            4.0           2.0   \n",
      "15             2.0         1.0          1.0            4.0           2.0   \n",
      "16             2.0         1.0          1.0            4.0           2.0   \n",
      "17             2.0         1.0          1.0            4.0           2.0   \n",
      "18             2.0         1.0          1.0            4.0           2.0   \n",
      "19             2.0         1.0          1.0            4.0           2.0   \n",
      "20             2.0         1.0          1.0            4.0           2.0   \n",
      "21             2.0         1.0          1.0            4.0           2.0   \n",
      "22             2.0         1.0          1.0            4.0           2.0   \n",
      "23             2.0         1.0          1.0            4.0           2.0   \n",
      "24             2.0         1.0          1.0            4.0           2.0   \n",
      "25             2.0         1.0          1.0            4.0           2.0   \n",
      "26             2.0         1.0          1.0            4.0           2.0   \n",
      "27             2.0         1.0          1.0            4.0           2.0   \n",
      "28             2.0         1.0          1.0            4.0           2.0   \n",
      "29             2.0         1.0          1.0            4.0           2.0   \n",
      "...            ...         ...          ...            ...           ...   \n",
      "1259122     2051.0         1.0         -1.0           -1.0          -1.0   \n",
      "1259123     2051.0         1.0         -1.0           -1.0          -1.0   \n",
      "1259124     2051.0         1.0         -1.0           -1.0          -1.0   \n",
      "1259125     2051.0         1.0         -1.0           -1.0          -1.0   \n",
      "1259126     2051.0         1.0         -1.0           -1.0          -1.0   \n",
      "1259127     2051.0         1.0         -1.0           -1.0          -1.0   \n",
      "1259128     2051.0         1.0         -1.0           -1.0          -1.0   \n",
      "1259129     2051.0         1.0         -1.0           -1.0          -1.0   \n",
      "1259130     2051.0         1.0         -1.0           -1.0          -1.0   \n",
      "1259131     2051.0         1.0         -1.0           -1.0          -1.0   \n",
      "1259132     2051.0         1.0         -1.0           -1.0          -1.0   \n",
      "1259133     2051.0         1.0         -1.0           -1.0          -1.0   \n",
      "1259134     2051.0         1.0         -1.0           -1.0          -1.0   \n",
      "1259135     2051.0         1.0         -1.0           -1.0          -1.0   \n",
      "1259136     2051.0         1.0         -1.0           -1.0          -1.0   \n",
      "1259137     2051.0         1.0         -1.0           -1.0          -1.0   \n",
      "1259138     2051.0         1.0         -1.0           -1.0          -1.0   \n",
      "1259139     2051.0         1.0         -1.0           -1.0          -1.0   \n",
      "1259140     2051.0         1.0         -1.0           -1.0          -1.0   \n",
      "1259141     2051.0         1.0         -1.0           -1.0          -1.0   \n",
      "1259142     2051.0         1.0         -1.0           -1.0          -1.0   \n",
      "1259143     2051.0         1.0         -1.0           -1.0          -1.0   \n",
      "1259144     2051.0         1.0         -1.0           -1.0          -1.0   \n",
      "1259145     2051.0         1.0         -1.0           -1.0          -1.0   \n",
      "1259146     2051.0         1.0         -1.0           -1.0          -1.0   \n",
      "1259147     2051.0         1.0         -1.0           -1.0          -1.0   \n",
      "1259148     2051.0         1.0         -1.0           -1.0          -1.0   \n",
      "1259149     2051.0         1.0         -1.0           -1.0          -1.0   \n",
      "1259150     2051.0         1.0         -1.0           -1.0          -1.0   \n",
      "1259151     2051.0         1.0         -1.0           -1.0          -1.0   \n",
      "\n",
      "         tradeenjoy_yr4  newshrs_yr4  raven_score_yr4  crt_score_yr4  \\\n",
      "0                   4.0          3.0              6.0            9.0   \n",
      "1                   4.0          3.0              6.0            9.0   \n",
      "2                   4.0          3.0              6.0            9.0   \n",
      "3                   4.0          3.0              6.0            9.0   \n",
      "4                   4.0          3.0              6.0            9.0   \n",
      "5                   4.0          3.0              6.0            9.0   \n",
      "6                   4.0          3.0              6.0            9.0   \n",
      "7                   4.0          3.0              6.0            9.0   \n",
      "8                   4.0          3.0              6.0            9.0   \n",
      "9                   4.0          3.0              6.0            9.0   \n",
      "10                  4.0          3.0              6.0            9.0   \n",
      "11                  4.0          3.0              6.0            9.0   \n",
      "12                  4.0          3.0              6.0            9.0   \n",
      "13                  4.0          3.0              6.0            9.0   \n",
      "14                  4.0          3.0              6.0            9.0   \n",
      "15                  4.0          3.0              6.0            9.0   \n",
      "16                  4.0          3.0              6.0            9.0   \n",
      "17                  4.0          3.0              6.0            9.0   \n",
      "18                  4.0          3.0              6.0            9.0   \n",
      "19                  4.0          3.0              6.0            9.0   \n",
      "20                  4.0          3.0              6.0            9.0   \n",
      "21                  4.0          3.0              6.0            9.0   \n",
      "22                  4.0          3.0              6.0            9.0   \n",
      "23                  4.0          3.0              6.0            9.0   \n",
      "24                  4.0          3.0              6.0            9.0   \n",
      "25                  4.0          3.0              6.0            9.0   \n",
      "26                  4.0          3.0              6.0            9.0   \n",
      "27                  4.0          3.0              6.0            9.0   \n",
      "28                  4.0          3.0              6.0            9.0   \n",
      "29                  4.0          3.0              6.0            9.0   \n",
      "...                 ...          ...              ...            ...   \n",
      "1259122            -1.0          5.0             -1.0           -1.0   \n",
      "1259123            -1.0          5.0             -1.0           -1.0   \n",
      "1259124            -1.0          5.0             -1.0           -1.0   \n",
      "1259125            -1.0          5.0             -1.0           -1.0   \n",
      "1259126            -1.0          5.0             -1.0           -1.0   \n",
      "1259127            -1.0          5.0             -1.0           -1.0   \n",
      "1259128            -1.0          5.0             -1.0           -1.0   \n",
      "1259129            -1.0          5.0             -1.0           -1.0   \n",
      "1259130            -1.0          5.0             -1.0           -1.0   \n",
      "1259131            -1.0          5.0             -1.0           -1.0   \n",
      "1259132            -1.0          5.0             -1.0           -1.0   \n",
      "1259133            -1.0          5.0             -1.0           -1.0   \n",
      "1259134            -1.0          5.0             -1.0           -1.0   \n",
      "1259135            -1.0          5.0             -1.0           -1.0   \n",
      "1259136            -1.0          5.0             -1.0           -1.0   \n",
      "1259137            -1.0          5.0             -1.0           -1.0   \n",
      "1259138            -1.0          5.0             -1.0           -1.0   \n",
      "1259139            -1.0          5.0             -1.0           -1.0   \n",
      "1259140            -1.0          5.0             -1.0           -1.0   \n",
      "1259141            -1.0          5.0             -1.0           -1.0   \n",
      "1259142            -1.0          5.0             -1.0           -1.0   \n",
      "1259143            -1.0          5.0             -1.0           -1.0   \n",
      "1259144            -1.0          5.0             -1.0           -1.0   \n",
      "1259145            -1.0          5.0             -1.0           -1.0   \n",
      "1259146            -1.0          5.0             -1.0           -1.0   \n",
      "1259147            -1.0          5.0             -1.0           -1.0   \n",
      "1259148            -1.0          5.0             -1.0           -1.0   \n",
      "1259149            -1.0          5.0             -1.0           -1.0   \n",
      "1259150            -1.0          5.0             -1.0           -1.0   \n",
      "1259151            -1.0          5.0             -1.0           -1.0   \n",
      "\n",
      "         crt_1_cor_yr4  ...   training  team  user_id_y  value  fcast_year  \\\n",
      "0                  1.0  ...          h  37.0         63  -0.96        2015   \n",
      "1                  1.0  ...          h  37.0         63  -0.96        2015   \n",
      "2                  1.0  ...          h  37.0         63  -1.00        2015   \n",
      "3                  1.0  ...          h  37.0         63  -1.00        2015   \n",
      "4                  1.0  ...          h  37.0         63  -1.00        2015   \n",
      "5                  1.0  ...          h  37.0         63  -1.00        2015   \n",
      "6                  1.0  ...          h  37.0         63  -1.00        2015   \n",
      "7                  1.0  ...          h  37.0         63  -1.00        2015   \n",
      "8                  1.0  ...          h  37.0         63  -0.50        2014   \n",
      "9                  1.0  ...          h  37.0         63  -0.50        2014   \n",
      "10                 1.0  ...          h  37.0         63  -0.50        2014   \n",
      "11                 1.0  ...          h  37.0         63  -0.50        2014   \n",
      "12                 1.0  ...          h  37.0         63  -0.50        2014   \n",
      "13                 1.0  ...          h  37.0         63  -0.84        2014   \n",
      "14                 1.0  ...          h  37.0         63  -0.84        2014   \n",
      "15                 1.0  ...          h  37.0         63  -0.84        2014   \n",
      "16                 1.0  ...          h  37.0         63  -0.84        2014   \n",
      "17                 1.0  ...          h  37.0         63  -0.84        2014   \n",
      "18                 1.0  ...          h  37.0         63  -0.84        2014   \n",
      "19                 1.0  ...          h  37.0         63  -0.84        2014   \n",
      "20                 1.0  ...          h  37.0         63  -0.84        2014   \n",
      "21                 1.0  ...          h  37.0         63  -0.96        2014   \n",
      "22                 1.0  ...          h  37.0         63  -0.96        2014   \n",
      "23                 1.0  ...          h  37.0         63  -0.96        2014   \n",
      "24                 1.0  ...          h  37.0         63  -0.96        2014   \n",
      "25                 1.0  ...          h  37.0         63  -0.96        2014   \n",
      "26                 1.0  ...          h  37.0         63   0.30        2014   \n",
      "27                 1.0  ...          h  37.0         63   0.30        2014   \n",
      "28                 1.0  ...          h  37.0         63   0.30        2014   \n",
      "29                 1.0  ...          h  37.0         63   0.30        2014   \n",
      "...                ...  ...        ...   ...        ...    ...         ...   \n",
      "1259122           -1.0  ...          q   NaN     145836   0.60        2015   \n",
      "1259123           -1.0  ...          q   NaN     145836   0.60        2015   \n",
      "1259124           -1.0  ...          q   NaN     145836   0.60        2015   \n",
      "1259125           -1.0  ...          q   NaN     145836   0.60        2015   \n",
      "1259126           -1.0  ...          q   NaN     145836   0.60        2015   \n",
      "1259127           -1.0  ...          q   NaN     145836   0.60        2015   \n",
      "1259128           -1.0  ...          q   NaN     145836   0.60        2015   \n",
      "1259129           -1.0  ...          q   NaN     145836   0.60        2015   \n",
      "1259130           -1.0  ...          q   NaN     145836   0.60        2015   \n",
      "1259131           -1.0  ...          q   NaN     145836   0.60        2015   \n",
      "1259132           -1.0  ...          q   NaN     145836   0.60        2015   \n",
      "1259133           -1.0  ...          q   NaN     145836   0.60        2015   \n",
      "1259134           -1.0  ...          q   NaN     145836   0.60        2015   \n",
      "1259135           -1.0  ...          q   NaN     145836   0.60        2015   \n",
      "1259136           -1.0  ...          q   NaN     145836  -0.40        2015   \n",
      "1259137           -1.0  ...          q   NaN     145836  -0.40        2015   \n",
      "1259138           -1.0  ...          q   NaN     145836  -0.40        2015   \n",
      "1259139           -1.0  ...          q   NaN     145836  -0.40        2015   \n",
      "1259140           -1.0  ...          q   NaN     145836  -0.40        2015   \n",
      "1259141           -1.0  ...          q   NaN     145836  -0.40        2015   \n",
      "1259142           -1.0  ...          q   NaN     145836  -0.40        2015   \n",
      "1259143           -1.0  ...          q   NaN     145836  -0.40        2015   \n",
      "1259144           -1.0  ...          q   NaN     145836  -0.40        2015   \n",
      "1259145           -1.0  ...          q   NaN     145836  -0.40        2015   \n",
      "1259146           -1.0  ...          q   NaN     145836  -0.40        2015   \n",
      "1259147           -1.0  ...          q   NaN     145836  -0.40        2015   \n",
      "1259148           -1.0  ...          q   NaN     145836  -0.40        2015   \n",
      "1259149           -1.0  ...          q   NaN     145836  -0.40        2015   \n",
      "1259150           -1.0  ...          q   NaN     145836  -0.40        2015   \n",
      "1259151           -1.0  ...          q   NaN     145836  -0.40        2015   \n",
      "\n",
      "         fcast_week      ifp_week   bin  task_id   uid  \n",
      "0                 3   201531244-0  0.02        0     2  \n",
      "1                 4   201541244-0  0.02        1     2  \n",
      "2                 4   201541244-0  0.00        1     2  \n",
      "3                 5   201551244-0  0.00        2     2  \n",
      "4                 6   201561244-0  0.00        3     2  \n",
      "5                 7   201571244-0  0.00        4     2  \n",
      "6                 8   201581244-0  0.00        5     2  \n",
      "7                 9   201591244-0  0.00        6     2  \n",
      "8                49  2014491394-0  0.25       14     2  \n",
      "9                50  2014501394-0  0.25       15     2  \n",
      "10               51  2014511394-0  0.25       16     2  \n",
      "11               52  2014521394-0  0.25       17     2  \n",
      "12                1   201411394-0  0.25       18     2  \n",
      "13               42  2014421399-2  0.08      328     2  \n",
      "14               43  2014431399-2  0.08      329     2  \n",
      "15               44  2014441399-2  0.08      330     2  \n",
      "16               45  2014451399-2  0.08      331     2  \n",
      "17               46  2014461399-2  0.08      332     2  \n",
      "18               47  2014471399-2  0.08      333     2  \n",
      "19               48  2014481399-2  0.08      334     2  \n",
      "20               49  2014491399-2  0.08      335     2  \n",
      "21               49  2014491399-2  0.02      335     2  \n",
      "22               50  2014501399-2  0.02      336     2  \n",
      "23               51  2014511399-2  0.02      337     2  \n",
      "24               52  2014521399-2  0.02      338     2  \n",
      "25                1   201411399-2  0.02      339     2  \n",
      "26               35  2014351409-0  0.65      340     2  \n",
      "27               36  2014361409-0  0.65      341     2  \n",
      "28               37  2014371409-0  0.65      342     2  \n",
      "29               38  2014381409-0  0.65      343     2  \n",
      "...             ...           ...   ...      ...   ...  \n",
      "1259122           9   201591515-0  0.80     1530  2051  \n",
      "1259123          10  2015101515-0  0.80     1286  2051  \n",
      "1259124          11  2015111515-0  0.80     1287  2051  \n",
      "1259125          12  2015121515-0  0.80     1288  2051  \n",
      "1259126          13  2015131515-0  0.80     1289  2051  \n",
      "1259127          14  2015141515-0  0.80     1290  2051  \n",
      "1259128          15  2015151515-0  0.80     1291  2051  \n",
      "1259129          16  2015161515-0  0.80     1292  2051  \n",
      "1259130          17  2015171515-0  0.80     1293  2051  \n",
      "1259131          18  2015181515-0  0.80     1294  2051  \n",
      "1259132          19  2015191515-0  0.80     1295  2051  \n",
      "1259133          20  2015201515-0  0.80      242  2051  \n",
      "1259134          21  2015211515-0  0.80      243  2051  \n",
      "1259135          22  2015221515-0  0.80      244  2051  \n",
      "1259136           9   201596413-0  0.30     1426  2051  \n",
      "1259137          10  2015106413-0  0.30     1427  2051  \n",
      "1259138          11  2015116413-0  0.30     1428  2051  \n",
      "1259139          12  2015126413-0  0.30     1429  2051  \n",
      "1259140          13  2015136413-0  0.30     1430  2051  \n",
      "1259141          14  2015146413-0  0.30     1431  2051  \n",
      "1259142          15  2015156413-0  0.30     1432  2051  \n",
      "1259143          16  2015166413-0  0.30     1433  2051  \n",
      "1259144          17  2015176413-0  0.30     1434  2051  \n",
      "1259145          18  2015186413-0  0.30     1435  2051  \n",
      "1259146          19  2015196413-0  0.30     1436  2051  \n",
      "1259147          20  2015206413-0  0.30     1437  2051  \n",
      "1259148          21  2015216413-0  0.30     1438  2051  \n",
      "1259149          22  2015226413-0  0.30     1439  2051  \n",
      "1259150          23  2015236413-0  0.30     1440  2051  \n",
      "1259151          24  2015246413-0  0.30     1441  2051  \n",
      "\n",
      "[1259152 rows x 181 columns]\n"
     ]
    }
   ],
   "source": [
    "joined_data = pd.merge(first_import,testframe,right_on='uid',left_on='user_id')\n",
    "print joined_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1259152, 181)\n",
      "(1499294, 13)\n",
      "(1712, 168)\n"
     ]
    }
   ],
   "source": [
    "print joined_data.shape\n",
    "print testframe.shape\n",
    "print first_import.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['uid', 'task_id', 'value', 'bin', 'user_id', 'gender_yr4', 'citizen_yr4', 'workhours_yr4', 'tradeexp_yr4', 'tradeenjoy_yr4', 'newshrs_yr4', 'raven_score_yr4', 'crt_score_yr4', 'crt_1_cor_yr4', 'crt_2_cor_yr4', 'crt_3_cor_yr4', 'crt_4_cor_yr4', 'crt_5_cor_yr4', 'crt_6_cor_yr4', 'crt_7_cor_yr4', 'crt_8_cor_yr4', 'crt_9_cor_yr4', 'crt_10_cor_yr4', 'crt_11_cor_yr4', 'crt_12_cor_yr4', 'crt_13_cor_yr4', 'crt_14_cor_yr4', 'crt_15_cor_yr4', 'crt_16_cor_yr4', 'crt_17_cor_yr4', 'crt_18_cor_yr4', 'numeracy_score_yr4', 'cpk_correct_1', 'cpk_correct_2', 'cpk_correct_3', 'cpk_correct_4', 'cpk_correct_5', 'cpk_correct_6', 'cpk_correct_7', 'cpk_correct_8', 'cpk_correct_9', 'cpk_correct_10', 'cpk_correct_11', 'cpk_correct_12', 'cpk_correct_13', 'cpk_correct_14', 'cpk_correct_15', 'cpk_correct_16', 'cpk_correct_17', 'cpk_correct_18', 'cpk_correct_19', 'cpk_correct_20', 'cpk_correct_21', 'cpk_correct_22', 'cpk_correct_23', 'cpk_correct_24', 'cpk_correct_25', 'cpk_correct_26', 'cpk_correct_27', 'cpk_correct_28', 'cpk_correct_29', 'cpk_correct_30', 'cpk_correct_31', 'cpk_correct_32', 'cpk_score_china', 'cpk_score_globecon', 'cpk_score_iran', 'cpk_score_russia', 'gpk_1', 'gpk_2', 'gpk_3', 'gpk_4', 'gpk_5', 'gpk_6', 'gpk_7', 'gpk_8', 'gpk_9', 'gpk_10', 'gpk_11', 'gpk_12', 'gpk_13', 'gpk_correct_1', 'gpk_correct_2', 'gpk_correct_3', 'gpk_correct_4', 'gpk_correct_5', 'gpk_correct_6', 'gpk_correct_7', 'gpk_correct_8', 'gpk_correct_9', 'gpk_correct_10', 'gpk_correct_11', 'gpk_correct_12', 'gpk_correct_13', 'gpk_score', 'aomt_1_yr4', 'aomt_2_yr4', 'aomt_3_yr4', 'aomt_4_yr4', 'aomt_5_yr4', 'aomt_6_yr4', 'aomt_7_yr4', 'aomt_8_yr4', 'aomt_9_yr4', 'aomt_score_yr4', 'cultwv_ind_score_yr4', 'cultwv_hier_score_yr4', 'cultwv_ind_1_yr4', 'cultwv_ind_2_yr4', 'cultwv_ind_3_yr4', 'cultwv_ind_4_yr4', 'cultwv_ind_5_yr4', 'cultwv_ind_6_yr4', 'cultwv_hier_7_yr4', 'cultwv_hier_8_yr4', 'cultwv_hier_9_yr4', 'cultwv_hier_10_yr4', 'cultwv_hier_11_yr4', 'cultwv_hier_12_yr4', 'nfcog_1_yr4', 'nfcog_2_yr4', 'nfcog_3_yr4', 'nfcog_4_yr4', 'nfcog_5_yr4', 'nfcog_6_yr4', 'nfcog_7_yr4', 'nfcog_8_yr4', 'nfcog_9_yr4', 'nfcog_10_yr4', 'nfcog_11_yr4', 'nfcog_12_yr4', 'nfcog_13_yr4', 'nfcog_14_yr4', 'nfcog_15_yr4', 'nfcog_16_yr4', 'nfcog_17_yr4', 'nfcog_18_yr4', 'nfcog_score_yr4', 'memory_math_correct_1', 'memory_math_correct_2', 'memory_math_correct_3', 'memory_math_correct_4', 'memory_math_correct_5', 'memory_math_correct_6', 'memory_word_correct_1', 'memory_word_correct_2', 'memory_word_correct_3', 'memory_word_correct_4', 'memory_word_correct_5', 'memory_word_correct_6', 'memory_math_score', 'memory_word_score', 'helping_1', 'helping_2', 'helping_3', 'helping_score', 'orgcommit_1', 'orgcommit_2', 'orgcommit_3', 'orgcommit_4', 'orgcommit_5', 'motivation_1', 'motivation_2', 'motivation_3', 'motivation_4', 'motivation_5', 'motivation_6', 'motivation_7', 'motivation_8', 'motivation_intrinsic_score', 'motivation_prosocial_score', 'motivation_extrinsic_score']\n",
      "172\n"
     ]
    }
   ],
   "source": [
    "tot_feats = ['uid','task_id','value','bin']\n",
    "tot_feats = tot_feats + list(first_import.columns.values)\n",
    "print tot_feats\n",
    "print len(tot_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1259152, 171)\n"
     ]
    }
   ],
   "source": [
    "mask = joined_data.columns.isin(tot_feats)\n",
    "cleanframe = joined_data[joined_data.columns[mask]]\n",
    "print(cleanframe.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'gender_yr4', u'citizen_yr4', u'workhours_yr4', u'tradeexp_yr4',\n",
      "       u'tradeenjoy_yr4', u'newshrs_yr4', u'raven_score_yr4', u'crt_score_yr4',\n",
      "       u'crt_1_cor_yr4', u'crt_2_cor_yr4',\n",
      "       ...\n",
      "       u'motivation_6', u'motivation_7', u'motivation_8',\n",
      "       u'motivation_intrinsic_score', u'motivation_prosocial_score',\n",
      "       u'motivation_extrinsic_score', u'value', u'bin', u'task_id', u'uid'],\n",
      "      dtype='object', length=171)\n"
     ]
    }
   ],
   "source": [
    "print cleanframe.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model_LightFM_Vanilla_Psych_No_Bias(nn.Module):\n",
    "    def __init__(self, users, tasks, num_user_features, k=4):\n",
    "        super(Model_LightFM_Vanilla_Psych_No_Bias, self).__init__()\n",
    "        self.user_feature_lv = nn.Embedding(num_user_features, k)\n",
    "        self.user_lv = nn.Embedding(users, k)\n",
    "        self.task_lv = nn.Embedding(tasks, k)\n",
    "\n",
    "        self.num_user_features_bias = nn.Embedding(num_user_features, 1)\n",
    "        self.num_user_features_add_bias = nn.Embedding(num_user_features, 1)\n",
    "        self.users_bias = nn.Embedding(users, 1)\n",
    "        self.users_add_bias = nn.Embedding(users, 1)\n",
    "        self.task_bias = nn.Embedding(tasks, 1)\n",
    "        self.global_bias = nn.Parameter(torch.FloatTensor(1))\n",
    "        \n",
    "    def forward(self, users, users_features,jokes):\n",
    "        user_vectors = self.user_lv(users)\n",
    "        user_feature_vectors = torch.mm(users_features,self.user_feature_lv.weight)\n",
    "        user_finals = torch.add(user_vectors,user_feature_vectors)\n",
    "        task_vectors = self.task_lv(jokes)\n",
    "        user_bias = self.users_bias(users)\n",
    "        task_bias = self.task_bias(jokes)\n",
    "        user_add_bias = self.users_add_bias(users)\n",
    "\n",
    "        preds = torch.bmm(user_finals.unsqueeze(1), task_vectors.unsqueeze(2)).squeeze() \\\n",
    "                          + task_bias.squeeze() + self.global_bias.expand_as(user_bias.squeeze())\n",
    "        m = nn.Tanh()\n",
    "#         return m((user_bias.squeeze() * (preds - .5)) + .5)\n",
    "#         return m(user_bias.squeeze() * preds )\n",
    "        return m(user_bias.squeeze() * preds + user_add_bias.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Model_LightFM_Vanilla_Psych_With_Bias(nn.Module):\n",
    "    def __init__(self, users, tasks, num_user_features, k=4):\n",
    "        super(Model_LightFM_Vanilla_Psych_With_Bias, self).__init__()\n",
    "        self.user_feature_lv = nn.Embedding(num_user_features, k)\n",
    "        self.user_lv = nn.Embedding(users, k)\n",
    "        self.task_lv = nn.Embedding(tasks, k)\n",
    "\n",
    "        self.num_user_features_bias = nn.Embedding(num_user_features, 1)\n",
    "        self.num_user_features_add_bias = nn.Embedding(num_user_features, 1)\n",
    "        self.users_bias = nn.Embedding(users, 1)\n",
    "        self.users_add_bias = nn.Embedding(users, 1)\n",
    "        self.task_bias = nn.Embedding(tasks, 1)\n",
    "        self.global_bias = nn.Parameter(torch.FloatTensor(1))\n",
    "        \n",
    "    def forward(self, users, users_features,jokes):\n",
    "        user_vectors = self.user_lv(users)\n",
    "        user_feature_vectors = torch.mm(users_features,self.user_feature_lv.weight)\n",
    "        user_finals = torch.add(user_vectors,user_feature_vectors)\n",
    "        task_vectors = self.task_lv(jokes)\n",
    "        user_bias = self.users_bias(users)\n",
    "        user_features_bias = torch.mm(users_features, self.num_user_features_bias.weight)\n",
    "        task_bias = self.task_bias(jokes)\n",
    "        user_add_bias = self.users_add_bias(users)\n",
    "        user_features_add_bias = torch.mm(users_features,self.num_user_features_add_bias.weight)\n",
    "        \n",
    "        preds = torch.bmm(user_vectors.unsqueeze(1), task_vectors.unsqueeze(2)).squeeze() \\\n",
    "                          + task_bias.squeeze() + self.global_bias.expand_as(user_bias.squeeze())\n",
    "        m = nn.Tanh()\n",
    "#         return m((user_bias.squeeze() * (preds - .5)) + .5)\n",
    "#         return m(user_bias.squeeze() * preds )\n",
    "        return m((user_bias * user_features_bias).squeeze() * preds + (user_add_bias.squeeze() + user_features_add_bias.squeeze()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Model_LightFM_Psych_First(nn.Module):\n",
    "    def __init__(self, tasks, num_user_features, k=4):\n",
    "        super(Model_LightFM_Psych_First, self).__init__()\n",
    "        self.user_feature_lv = nn.Embedding(num_user_features, k)\n",
    "        self.task_lv = nn.Embedding(tasks, k)\n",
    "\n",
    "        self.num_user_features_add_bias = nn.Embedding(num_user_features, 1)\n",
    "        self.task_bias = nn.Embedding(tasks, 1)\n",
    "        self.global_bias = nn.Parameter(torch.FloatTensor(1))\n",
    "        \n",
    "    def forward(self, users, users_features,jokes):\n",
    "        user_feature_vectors = torch.mm(users_features,self.user_feature_lv.weight)\n",
    "        task_vectors = self.task_lv(jokes)\n",
    "        user_features_add_bias = torch.mm(users_features, self.num_user_features_add_bias.weight)\n",
    "        task_bias = self.task_bias(jokes)\n",
    "        \n",
    "        preds = torch.bmm(user_feature_vectors.unsqueeze(1), task_vectors.unsqueeze(2)).squeeze() \\\n",
    "                          + task_bias.squeeze() + self.global_bias.expand_as(task_bias.squeeze())\n",
    "        m = nn.Tanh()\n",
    "#         return m((user_bias.squeeze() * (preds - .5)) + .5)\n",
    "#         return m(user_bias.squeeze() * preds )\n",
    "        return m(preds + user_features_add_bias.squeeze())\n",
    "\n",
    "class Model_LightFM_Users_Psych_Given(nn.Module):\n",
    "    def __init__(self, users, tasks, feature_lv, feature_add_bias, task1_lv, task_bias1, global_bias1, k=4):\n",
    "        super(Model_LightFM_Users_Psych_Given, self).__init__()\n",
    "        self.user_lv = nn.Embedding(users, k)\n",
    "        self.task2_lv = nn.Embedding(tasks, k)\n",
    "        self.user_feature_lv = Variable(feature_lv.weight.data, requires_grad=False)\n",
    "        self.task1_lv = nn.Embedding(tasks, k)\n",
    "        self.task1_lv.weight = task1_lv.weight\n",
    "        self.task1_lv.weight.requires_grad=False\n",
    "        self.task_bias1 = nn.Embedding(tasks, 1)\n",
    "        self.task_bias1.weight = task_bias1.weight\n",
    "        self.task_bias1.weight.requires_grad=False\n",
    "        self.global_bias1 = Variable(global_bias1.data, requires_grad=False)\n",
    "\n",
    "        self.num_user_features_add_bias = Variable(feature_add_bias.weight.data, requires_grad=False)\n",
    "        self.users_bias = nn.Embedding(users, 1)\n",
    "        self.users_add_bias = nn.Embedding(users, 1)\n",
    "        self.task_bias2 = nn.Embedding(tasks, 1)\n",
    "        self.global_bias2 = nn.Parameter(torch.FloatTensor(1))\n",
    "        \n",
    "    def forward(self, users, users_features,jokes):\n",
    "        user_vectors = self.user_lv(users)\n",
    "        user_feature_vectors = torch.mm(users_features,self.user_feature_lv)\n",
    "        user_finals = torch.add(user_vectors,user_feature_vectors)\n",
    "        task_vectors1 = self.task1_lv(jokes)\n",
    "        task_vectors2 = self.task2_lv(jokes)\n",
    "        task_vectors = task_vectors1 + task_vectors2\n",
    "        \n",
    "        user_bias = self.users_bias(users)\n",
    "        task_bias1 = self.task_bias1(jokes)\n",
    "        task_bias2 = self.task_bias2(jokes)\n",
    "        task_bias = task_bias1 + task_bias2\n",
    "        user_add_bias = self.users_add_bias(users)\n",
    "        global_bias = self.global_bias1 + self.global_bias2\n",
    "\n",
    "        preds = torch.bmm(user_finals.unsqueeze(1), task_vectors.unsqueeze(2)).squeeze() \\\n",
    "                          + task_bias.squeeze() + global_bias.expand_as(user_bias.squeeze())\n",
    "        m = nn.Tanh()\n",
    "#         return m((user_bias.squeeze() * (preds - .5)) + .5)\n",
    "#         return m(user_bias.squeeze() * preds )\n",
    "        return m(user_bias.squeeze() * preds + user_add_bias.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batcher(seq, size):\n",
    "    return (seq[pos:pos + size] for pos in range(0, len(seq), size))\n",
    "\n",
    "def split(df):\n",
    "    train_df, validate_df, test_df = np.split(df.sample(frac=1), [int(.6*len(df)), int(.8*len(df))])\n",
    "    return train_df, validate_df, test_df\n",
    "\n",
    "def val(df, model):\n",
    "    crit = nn.MSELoss(size_average=False)\n",
    "    crit2 = nn.L1Loss(size_average=False)\n",
    "    total_loss = 0.\n",
    "    total_l1 = 0.\n",
    "    total_num = 0\n",
    "    for batch in batcher(df, 100):\n",
    "        true_rating = Variable(torch.Tensor(batch.bin.values.astype(float)))\n",
    "        total_num = total_num + true_rating.size(0)\n",
    "        users = Variable(torch.LongTensor(batch.uid.values))\n",
    "        tasks = Variable(torch.LongTensor(batch.task_id.values))\n",
    "        users_features = Variable(torch.FloatTensor(batch.values[:,:167]))\n",
    "#         print(users, tasks)\n",
    "        scores = model.forward(users, users_features,tasks)\n",
    "        total_loss += crit(scores, true_rating).data[0]\n",
    "        total_l1 += crit2(scores,true_rating).data[0]\n",
    "    return math.sqrt(total_loss/total_num), total_l1/total_num\n",
    "\n",
    "def train(train_iter, val_iter, test_iter, model):\n",
    "    train_params = itertools.ifilter(lambda p: p.requires_grad, model.parameters())\n",
    "    train_param_list = [param for param in train_params]\n",
    "    opt = optim.SGD(train_param_list, lr=.3)\n",
    "    crit = nn.MSELoss()\n",
    "    crit2 = nn.L1Loss()\n",
    "\n",
    "    print(\"val:\", val(validate_df, model))\n",
    "    for epochs in range(150):\n",
    "        avg_loss = 0\n",
    "        avg_l1 = 0\n",
    "        total = 0\n",
    "        for i,batch in enumerate(batcher(train_df, 100)):\n",
    "            opt.zero_grad()\n",
    "            rating = Variable(torch.Tensor(batch.bin.values.astype(float)))\n",
    "#             print(rating)\n",
    "            users = Variable(torch.LongTensor(batch.uid.values))\n",
    "            users_features = Variable(torch.FloatTensor(batch.values[:,:167]))\n",
    "            tasks = Variable(torch.LongTensor(batch.task_id.values))\n",
    "#             print(users, tasks)\n",
    "            scores = model.forward(users, users_features, tasks) \n",
    "#             + torch.sum(model.user_lut.weight.data.pow(2)) + \\\n",
    "#             torch.sum(model.task_lut.weight.data.pow(2)) + torch.sum(model.user_bias.weight.data.pow(2)) + \\\n",
    "#             torch.sum(model.task_bias.weight.data.pow(2))\n",
    "            loss = crit(scores, rating)\n",
    "            #if i % 1000==0:\n",
    "            #    print (loss.data[0])\n",
    "            loss.backward()\n",
    "            avg_loss += loss.data[0]\n",
    "            avg_l1 += crit2(scores,rating).data[0]\n",
    "            total += 1\n",
    "            opt.step()\n",
    "        if (epochs == 149 or epochs % 10 == 0):\n",
    "            print(\"train:\", math.sqrt(avg_loss / float(total)), avg_l1/ float(total))\n",
    "            print(\"val:\", val(validate_df, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df, validate_df, test_df = split(cleanframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2051, 1547)\n",
      "2051\n",
      "1547\n",
      "Psych No Bias\n",
      "('val:', (1.0708891941634497, 0.9741797762849089))\n",
      "('train:', 1.0536613828868724, 0.9517919979373167)\n",
      "('val:', (1.0405037372050918, 0.935224200660249))\n",
      "('train:', 0.9046787080883812, 0.7590235442297889)\n",
      "('val:', (0.9057850684136225, 0.759620731768661))\n",
      "('train:', 0.8684427044387533, 0.7131518847022444)\n",
      "('val:', (0.8730955341142729, 0.7189242722019326))\n",
      "('train:', 0.8501069355975551, 0.6891454628017231)\n",
      "('val:', (0.8672531253742205, 0.7119431811083785))\n",
      "('train:', 0.8438784290802985, 0.6817723257544812)\n",
      "('val:', (0.8505200417430913, 0.689745353282466))\n",
      "('train:', 0.8399330535919071, 0.6759741127372656)\n",
      "('val:', (0.8386203737921714, 0.6742538348536757))\n",
      "('train:', 0.8342891620888174, 0.6686090140489456)\n",
      "('val:', (0.8362331197395606, 0.6701849898016539))\n",
      "('train:', 0.8323651627404135, 0.6660284563336918)\n",
      "('val:', (0.8353831579425794, 0.6692333151830917))\n",
      "('train:', 0.8289250578397462, 0.662760100350484)\n",
      "('val:', (0.832575767227746, 0.6661581743654879))\n",
      "('train:', 0.8277235737845724, 0.6610561749423916)\n",
      "('val:', (0.8318775729421286, 0.6651274128003377))\n",
      "('train:', 0.8277826975196261, 0.6611774420556763)\n",
      "('val:', (0.8318770905367213, 0.6651794810776619))\n",
      "('train:', 0.8269013585565219, 0.6599330914367674)\n",
      "('val:', (0.8313216141591546, 0.6642601173524183))\n",
      "('train:', 0.8264667719258365, 0.6592988957126436)\n",
      "('val:', (0.8315000014676917, 0.6646176986488325))\n",
      "('train:', 0.8264934755757448, 0.6593184321783458)\n",
      "('val:', (0.8311170844328072, 0.6640035310939209))\n",
      "('train:', 0.826371459126744, 0.6591067159712354)\n",
      "('val:', (0.831052090334098, 0.66391152427833))\n",
      "('train:', 0.8263293690379906, 0.6590151361287784)\n",
      "('val:', (0.83098016862748, 0.6637858497865934))\n",
      "Psych Features Only\n",
      "('val:', (1.106689368384514, 1.0150980148797133))\n",
      "('train:', 0.7480153312188826, 0.6835026217248564)\n",
      "('val:', (0.7448993440170653, 0.6813630888158446))\n",
      "('train:', 0.7435775541745324, 0.6802199776956848)\n",
      "('val:', (0.7441349256274575, 0.6808102497488162))\n",
      "('train:', 0.7435796672201392, 0.6802298901256231)\n",
      "('val:', (0.7441525101206966, 0.680826363364532))\n",
      "('train:', 0.7435715444001926, 0.6802183270612195)\n",
      "('val:', (0.7441394079909194, 0.6807973575323165))\n",
      "('train:', 0.743545347410187, 0.68018063908772)\n",
      "('val:', (0.7441059932397974, 0.6807674245421949))\n",
      "('train:', 0.7435547874326758, 0.680197414170031)\n",
      "('val:', (0.7441219840064162, 0.6807866167426654))\n",
      "('train:', 0.7435252138209183, 0.6801611762264562)\n",
      "('val:', (0.7440864893924196, 0.6807402087077815))\n",
      "('train:', 0.743547167969948, 0.6801916462392068)\n",
      "('val:', (0.7441364253224678, 0.6807836055357983))\n",
      "('train:', 0.7435561050496402, 0.6801982797983069)\n",
      "('val:', (0.7441067515243465, 0.680768450100901))\n",
      "('train:', 0.74354812718519, 0.6801940130840304)\n",
      "('val:', (0.7441259052255398, 0.6807795730419346))\n",
      "('train:', 0.7435297205643498, 0.6801644878936556)\n",
      "('val:', (0.7441190239659923, 0.680770781075884))\n",
      "('train:', 0.743562822006729, 0.6802079951897273)\n",
      "('val:', (0.7441513193902282, 0.6808091049154952))\n",
      "('train:', 0.7435346084729741, 0.6801734290242906)\n",
      "('val:', (0.7441066463083146, 0.6807576744710652))\n",
      "('train:', 0.7435532034833041, 0.6801943428385423)\n",
      "('val:', (0.7441380651167838, 0.6808119953632525))\n",
      "('train:', 0.7435507167675783, 0.6801954714451934)\n",
      "('val:', (0.7441481907251628, 0.6808051690652742))\n",
      "('train:', 0.7435490746480619, 0.6801905040706253)\n",
      "('val:', (0.7441176283104205, 0.6807907913693346))\n",
      "Psych Full\n",
      "('val:', (1.0531087255267526, 0.9517681007383739))\n",
      "('train:', 1.0235799634625888, 0.9133477992841063)\n",
      "('val:', (1.0022357699433235, 0.8870080248875837))\n",
      "('train:', 0.7639219678579023, 0.6213481693887931)\n",
      "('val:', (0.7586513104615403, 0.6167985798135505))\n",
      "('train:', 0.5213384197058987, 0.3929353558685825)\n",
      "('val:', (0.512582399875668, 0.38551053617612463))\n",
      "('train:', 0.37114865508853323, 0.2821466597656317)\n",
      "('val:', (0.3698737075350992, 0.28143434644583293))\n",
      "('train:', 0.3015605827248658, 0.23049937148586477)\n",
      "('val:', (0.2989281109187323, 0.22858415754435105))\n",
      "('train:', 0.26549054522677284, 0.20085681126774108)\n",
      "('val:', (0.26774672546139416, 0.2024458119977445))\n",
      "('train:', 0.25312487694950203, 0.19140972987379481)\n",
      "('val:', (0.2571781358431325, 0.19429695816882261))\n",
      "('train:', 0.2479626952796913, 0.18779723284991667)\n",
      "('val:', (0.252044633313087, 0.19065715117568707))\n",
      "('train:', 0.24301320268806328, 0.18471652418473633)\n",
      "('val:', (0.24724528332810694, 0.18743112362846978))\n",
      "('train:', 0.24051187195756032, 0.18301992187982832)\n",
      "('val:', (0.24419003948199644, 0.1853867412328256))\n",
      "('train:', 0.23899890948356772, 0.18173845978977662)\n",
      "('val:', (0.24214282879813087, 0.18387149511922823))\n",
      "('train:', 0.23763961123587946, 0.18065870900488626)\n",
      "('val:', (0.24059652157826106, 0.18269733213400371))\n",
      "('train:', 0.23668937062575574, 0.17985224509026182)\n",
      "('val:', (0.23945991959155508, 0.18173828426443336))\n",
      "('train:', 0.23546388978764282, 0.17878524514024893)\n",
      "('val:', (0.23835103114652684, 0.18077318360010908))\n",
      "('train:', 0.2346265172899567, 0.17803517256368476)\n",
      "('val:', (0.23786381552911207, 0.18019600687283355))\n",
      "('train:', 0.23379148497798583, 0.17730735497491237)\n",
      "('val:', (0.23703581973024035, 0.17948523845285577))\n"
     ]
    }
   ],
   "source": [
    "users = int(np.max(train_df.uid.unique()))\n",
    "tasks = int(np.max(train_df.task_id.unique()))\n",
    "print(users, tasks)\n",
    "print(int(np.max(validate_df.uid.unique())))\n",
    "print(int(np.max(validate_df.task_id.unique())))\n",
    "model_full = Model_LightFM_Vanilla_Psych_No_Bias(users+1, tasks+1, 167, k=4)\n",
    "print \"Psych No Bias\"\n",
    "train(train_df, validate_df, test_df, model_full)\n",
    "print \"Psych Features Only\"\n",
    "model_feat = Model_LightFM_Psych_First(tasks+1, 167, k=4)\n",
    "train(train_df, validate_df, test_df, model_feat)\n",
    "print \"Psych Full\"\n",
    "model_full_divided = Model_LightFM_Users_Psych_Given(users+1, tasks+1, model_feat.user_feature_lv, \\\n",
    "                                                     model_feat.num_user_features_add_bias, \\\n",
    "                                        model_feat.task_lv, model_feat.task_bias, model_feat.global_bias, k=4)\n",
    "train(train_df, validate_df, test_df, model_full_divided)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2051, 1547)\n",
      "2051\n",
      "1547\n",
      "('val:', (0.8915230299671335, 0.7652044651407314))\n",
      "('train:', 0.8596424511125262, 0.7280517475325725)\n",
      "('val:', (0.8311609915393258, 0.6942214340590477))\n",
      "('train:', 0.8034867818624493, 0.6616149925144204)\n",
      "('val:', (0.7824126691879957, 0.6371802382390976))\n",
      "('train:', 0.7594661415894768, 0.6114982932076243)\n",
      "('val:', (0.7432957906568286, 0.5939230789331807))\n",
      "('train:', 0.7223979527080907, 0.5712242516478823)\n",
      "('val:', (0.7091872421105431, 0.5572729701561862))\n",
      "('train:', 0.6892195422505012, 0.5363840171077408)\n",
      "('val:', (0.6777707314182552, 0.5250955226901072))\n",
      "('train:', 0.6593369928801041, 0.5061076087363109)\n",
      "('val:', (0.6507080472864755, 0.49775794869982515))\n",
      "('train:', 0.6335709860510509, 0.48087749676070096)\n",
      "('val:', (0.6264218493330572, 0.4745095451163208))\n",
      "('train:', 0.6105462969595223, 0.4591482143659295)\n",
      "('val:', (0.6055073905844184, 0.45469510269255925))\n",
      "('train:', 0.5912885909448529, 0.4412479698066282)\n",
      "('val:', (0.5877251640160408, 0.4382980722082678))\n",
      "('train:', 0.574081615802913, 0.42570177181361923)\n",
      "('val:', (0.5717717611185525, 0.4238138231723496))\n",
      "('train:', 0.5590994968703442, 0.4125523377143335)\n",
      "('val:', (0.5577084509149608, 0.41165759250168404))\n",
      "('train:', 0.5448523270188044, 0.40062099753981945)\n",
      "('val:', (0.5434720734436579, 0.39957556181306547))\n",
      "('train:', 0.5316247470637239, 0.38963442579316115)\n",
      "('val:', (0.531301231343653, 0.3894444817106505))\n",
      "('train:', 0.520095250405781, 0.38029008405560455)\n",
      "('val:', (0.5203106052529314, 0.38073837785607595))\n",
      "('train:', 0.5093576951321881, 0.37193684284849565)\n",
      "('val:', (0.5097056749992105, 0.3726009996777204))\n",
      "('train:', 0.49874379730376955, 0.363892989273816)\n",
      "('val:', (0.4990569339838116, 0.3645392274797869))\n",
      "('train:', 0.4883007158217781, 0.35606340997705077)\n",
      "('val:', (0.4890182330248016, 0.356966444223105))\n",
      "('train:', 0.4788501180935971, 0.34899572735981466)\n",
      "('val:', (0.47988345167912794, 0.35005588472038607))\n",
      "('train:', 0.4705790880084695, 0.34275665101746233)\n",
      "('val:', (0.47221597552145117, 0.34423435341302155))\n",
      "('train:', 0.4631970913030489, 0.33734727938351927)\n",
      "('val:', (0.46484143859176574, 0.3388141788103514))\n",
      "('train:', 0.4558914157266843, 0.3320472569673918)\n",
      "('val:', (0.45765657918879227, 0.3336286990829908))\n",
      "('train:', 0.44873392293010894, 0.3270129315767124)\n",
      "('val:', (0.45069299836285076, 0.3286346848668152))\n",
      "('train:', 0.4420246252876301, 0.3223655385461412)\n",
      "('val:', (0.44405731151125716, 0.32410372965189005))\n",
      "('train:', 0.4348827228311686, 0.31769418994138604)\n",
      "('val:', (0.43605216143891085, 0.3189602102614356))\n",
      "('train:', 0.4268803176142461, 0.31248796124832273)\n",
      "('val:', (0.4287928177693053, 0.3140157728290244))\n",
      "('train:', 0.42105541286844234, 0.308416682990634)\n",
      "('val:', (0.4236191955915746, 0.3103886543035801))\n",
      "('train:', 0.41606319890443944, 0.3050773032310386)\n",
      "('val:', (0.41873893068750395, 0.30713490820648665))\n",
      "('train:', 0.41128325030304924, 0.3019585592493484)\n",
      "('val:', (0.4140681748435755, 0.30403526752485405))\n",
      "('train:', 0.40626918525985073, 0.2987956049918024)\n",
      "('val:', (0.40851523276727836, 0.3004866283653661))\n",
      "('train:', 0.4012304965317504, 0.2956133313876442)\n",
      "('val:', (0.4041978674292384, 0.2977016598749534))\n",
      "('train:', 0.39672492647311325, 0.2927830545713695)\n",
      "('val:', (0.39946149927318825, 0.29466814355381016))\n",
      "('train:', 0.3919418855460511, 0.28974235293133066)\n",
      "('val:', (0.3947336395770372, 0.2916293752206232))\n",
      "('train:', 0.38733535844159317, 0.2867781557423795)\n",
      "('val:', (0.3904654837553731, 0.28880019799523965))\n",
      "('train:', 0.3832285401156277, 0.28413939427690427)\n",
      "('val:', (0.3862916121747481, 0.28607654331817584))\n",
      "('train:', 0.37916949214940093, 0.2815232341787343)\n",
      "('val:', (0.382416041316094, 0.2835141350693478))\n",
      "('train:', 0.3752182060665611, 0.27903221928663086)\n",
      "('val:', (0.3786137847233453, 0.2811137238956826))\n",
      "('train:', 0.37174467428865515, 0.27680708801091547)\n",
      "('val:', (0.37556981318452964, 0.279073086227231))\n",
      "('train:', 0.36896306212833546, 0.27500227499607777)\n",
      "('val:', (0.3730211426459454, 0.27740842249804065))\n",
      "('train:', 0.36649418018301777, 0.2734426579891176)\n",
      "('val:', (0.37060707115250546, 0.2758850469613091))\n",
      "('train:', 0.3640748821280223, 0.2719696637299738)\n",
      "('val:', (0.36816193432952393, 0.2743964163819559))\n",
      "('train:', 0.36164826342449896, 0.2705137410734285)\n",
      "('val:', (0.36577960614220234, 0.2729265837977443))\n",
      "('train:', 0.3592935078360167, 0.26910192989143294)\n",
      "('val:', (0.36341195814823596, 0.2715036206328168))\n",
      "('train:', 0.35690469024576615, 0.2677011458389338)\n",
      "('val:', (0.360929189011115, 0.27006005931070043))\n",
      "('train:', 0.35424733680634884, 0.26620820870192774)\n",
      "('val:', (0.35797811913327193, 0.26838618659549))\n",
      "('train:', 0.3509754729650726, 0.2643603554531725)\n",
      "('val:', (0.35429667473520154, 0.26626668098698736))\n",
      "('train:', 0.34735302793566764, 0.2622602851263819)\n",
      "('val:', (0.3513738759922333, 0.26450045051706494))\n",
      "('train:', 0.34512462832610735, 0.26091949323366054)\n",
      "('val:', (0.3492911892811915, 0.2632378340136686))\n",
      "('train:', 0.34319942837760103, 0.2597781932941736)\n",
      "('val:', (0.3473548550343931, 0.2621249547997077))\n",
      "('train:', 0.3413034054558815, 0.25870724115483734)\n",
      "('val:', (0.34548128619873814, 0.26106826025740115))\n",
      "('train:', 0.33957997135685913, 0.2577280657347427)\n",
      "('val:', (0.343802337106867, 0.2601468502526078))\n",
      "('train:', 0.3379847868198758, 0.25684802531209866)\n",
      "('val:', (0.3421644549751954, 0.259264444874102))\n",
      "('train:', 0.3363986445288449, 0.2559876237520708)\n",
      "('val:', (0.340506806385077, 0.25838230021))\n",
      "('train:', 0.33473343489829177, 0.25510882450838424)\n",
      "('val:', (0.3387133651846874, 0.257444386362381))\n",
      "('train:', 0.3328265252028564, 0.25412538944885954)\n",
      "('val:', (0.3366554788634319, 0.25635264252042844))\n",
      "('train:', 0.33093847440803864, 0.2531187952255114)\n",
      "('val:', (0.3349569760278933, 0.2554278217316128))\n",
      "('train:', 0.32934805924696475, 0.2522637224607638)\n",
      "('val:', (0.3333435839668915, 0.25455217874206826))\n",
      "('train:', 0.3277374101113729, 0.25139225604201215)\n",
      "('val:', (0.33168890418286423, 0.25364290354520075))\n",
      "('train:', 0.32607101604863115, 0.25049544763557174)\n",
      "('val:', (0.32988130162078755, 0.2526915820925021))\n",
      "('train:', 0.3244593642069888, 0.24958875546149273)\n",
      "('val:', (0.3283414105473188, 0.25180974996568517))\n",
      "('train:', 0.32308264324799507, 0.24885131858146245)\n",
      "('val:', (0.3269257181189211, 0.2510902489272604))\n",
      "('train:', 0.3217812399132785, 0.2481735413394804)\n",
      "('val:', (0.3256152618818225, 0.2503902362403435))\n",
      "('train:', 0.32062803676308593, 0.24755982031970525)\n",
      "('val:', (0.32445813362905174, 0.24976716055576756))\n",
      "('train:', 0.31956512216690547, 0.2470091132199346)\n",
      "('val:', (0.32335266247585237, 0.24918409769289762))\n",
      "('train:', 0.3185270406587819, 0.2464845968921953)\n",
      "('val:', (0.3222733561974464, 0.24863185730521248))\n",
      "('train:', 0.31749225971948514, 0.24597166794211087)\n",
      "('val:', (0.32119437668689493, 0.2480959187440865))\n",
      "('train:', 0.31645773202482025, 0.24545705640931384)\n",
      "('val:', (0.3201297963980864, 0.24755104559747768))\n",
      "('train:', 0.31546403632628794, 0.24494261163145717)\n",
      "('val:', (0.31911867622036866, 0.24701887350836385))\n",
      "('train:', 0.3145333217891134, 0.24444773022225957)\n",
      "('val:', (0.31816043159578816, 0.24651157732437437))\n",
      "('train:', 0.31366790102213293, 0.24398305173309964)\n",
      "('val:', (0.31725890071243223, 0.2460366773183099))\n",
      "('train:', 0.3128558255996327, 0.2435519642028329)\n",
      "('val:', (0.316393513550032, 0.24557773989954748))\n",
      "('train:', 0.31206548604022527, 0.24313281453618302)\n",
      "('val:', (0.3155376891296447, 0.24512802051578575))\n",
      "('train:', 0.31127420599554406, 0.24271446333903496)\n",
      "('val:', (0.31467078893978445, 0.244683278171697))\n",
      "('train:', 0.31046244798016914, 0.24229200041349325)\n",
      "('val:', (0.31376841412354894, 0.24423012972228347))\n",
      "('train:', 0.3096073149521856, 0.24185495598279902)\n",
      "('val:', (0.3128130971665695, 0.2437528927900024))\n",
      "('train:', 0.30874999280125587, 0.24141632243744354)\n",
      "('val:', (0.31194268818641907, 0.24329732610041083))\n"
     ]
    }
   ],
   "source": [
    "users = int(np.max(train_df.uid.unique()))\n",
    "tasks = int(np.max(train_df.task_id.unique()))\n",
    "print(users, tasks)\n",
    "print(int(np.max(validate_df.uid.unique())))\n",
    "print(int(np.max(validate_df.task_id.unique())))\n",
    "model = Model_LightFM_Vanilla_Psych_No_Bias(users+1, tasks+1, 167, k=4)\n",
    "train(train_df, validate_df, test_df, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   1.    1.    4.    2.    4.    4.   -1.   -1.   -1.   -1.   -1.   -1.\n",
      "   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.\n",
      "   -1.   -1.   -1.    1.   -1.    1.   -1.   -1.   -1.    1.    1.   -1.\n",
      "    1.   -1.   -1.   -1.    1.    1.    1.    1.    1.   -1.    1.   -1.\n",
      "    1.   -1.   -1.   -1.    1.    1.   -1.    1.    1.    1.   -1.    4.\n",
      "    4.    4.    5.    1.    2.    1.    1.    3.    3.    4.    1.    1.\n",
      "    3.    2.    2.    3.   -1.    1.   -1.   -1.   -1.    1.    1.    1.\n",
      "   -1.    1.   -1.    1.   -1.    8.   -1.   -1.   -1.   -1.   -1.   -1.\n",
      "   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.\n",
      "   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.\n",
      "   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.\n",
      "   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.\n",
      "   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.\n",
      "   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.    1.\n",
      "    1.  464.  995.]\n"
     ]
    }
   ],
   "source": [
    "print train_df.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  1.  4.  2.  4.  4. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1.  1. -1.  1. -1. -1. -1.  1.  1. -1.\n",
      "  1. -1. -1. -1.  1.  1.  1.  1.  1. -1.  1. -1.  1. -1. -1. -1.  1.  1.\n",
      " -1.  1.  1.  1. -1.  4.  4.  4.  5.  1.  2.  1.  1.  3.  3.  4.  1.  1.\n",
      "  3.  2.  2.  3. -1.  1. -1. -1. -1.  1.  1.  1. -1.  1. -1.  1. -1.  8.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1.]\n"
     ]
    }
   ],
   "source": [
    "print train_df.values[0,:167]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         gender_yr4  citizen_yr4  workhours_yr4  tradeexp_yr4  tradeenjoy_yr4  \\\n",
      "698541          1.0          1.0            4.0           2.0             4.0   \n",
      "296335          1.0          1.0            5.0           4.0             6.0   \n",
      "1188589        -1.0          1.0            6.0           1.0             6.0   \n",
      "866959         -1.0          1.0            4.0           3.0             2.0   \n",
      "693909          1.0          1.0            4.0           5.0             5.0   \n",
      "\n",
      "         newshrs_yr4  raven_score_yr4  crt_score_yr4  crt_1_cor_yr4  \\\n",
      "698541           4.0             -1.0           -1.0           -1.0   \n",
      "296335           2.0              9.0           18.0            1.0   \n",
      "1188589          3.0             -1.0           -1.0           -1.0   \n",
      "866959           5.0             -1.0           -1.0           -1.0   \n",
      "693909           3.0             -1.0           -1.0           -1.0   \n",
      "\n",
      "         crt_2_cor_yr4  ...   motivation_6  motivation_7  motivation_8  \\\n",
      "698541            -1.0  ...           -1.0          -1.0          -1.0   \n",
      "296335             1.0  ...            6.0           3.0           3.0   \n",
      "1188589           -1.0  ...           -1.0          -1.0          -1.0   \n",
      "866959            -1.0  ...           -1.0          -1.0          -1.0   \n",
      "693909            -1.0  ...           -1.0          -1.0          -1.0   \n",
      "\n",
      "         motivation_intrinsic_score  motivation_prosocial_score  \\\n",
      "698541                         -1.0                        -1.0   \n",
      "296335                          6.0                         3.0   \n",
      "1188589                        -1.0                        -1.0   \n",
      "866959                         -1.0                        -1.0   \n",
      "693909                         -1.0                        -1.0   \n",
      "\n",
      "         motivation_extrinsic_score  value   bin  task_id   uid  \n",
      "698541                         -1.0   1.00  1.00      464   995  \n",
      "296335                          2.0   0.76  0.88      429   380  \n",
      "1188589                        -1.0  -1.00  0.00        5  1930  \n",
      "866959                         -1.0  -0.80  0.10      199  1254  \n",
      "693909                         -1.0  -0.98  0.01      309   990  \n",
      "\n",
      "[5 rows x 171 columns]\n"
     ]
    }
   ],
   "source": [
    "print train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
