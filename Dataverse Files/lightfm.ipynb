{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.cm as cm\n",
    "from itertools import product\n",
    "from itertools import combinations_with_replacement\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import time\n",
    "import math\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, SpectralClustering, AffinityPropagation\n",
    "from sklearn.metrics.pairwise import pairwise_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from utils import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "gj = pd.read_csv('./gj_df.csv')\n",
    "# gj = gj[['ifp_id', 'ctt', 'cond', 'training', 'team', 'user_id', 'value', 'fcast_date']]\n",
    "# gj['fcast_year'] = pd.to_datetime(gj['fcast_date']).dt.year\n",
    "# gj['fcast_week'] = pd.to_datetime(gj['fcast_date']).dt.week\n",
    "# gj['ifp_week'] = gj['fcast_year'].map(str) + gj['fcast_week'].map(str) + gj['ifp_id']\n",
    "# gj = gj.drop('fcast_date', axis=1)\n",
    "# gj = gj.drop_duplicates()\n",
    "# gj.to_csv('./gj_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['G', 'P', 'R', 'X'], dtype=object)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# adult = pd.read_csv('../labels.txt', delimiter='\\t', header=0, names=['user_id','website','rating'])\n",
    "# trec = pd.read_csv('../trec-rf10-crowd/trec-rf10-data.txt', delimiter='\\t')\n",
    "# gj = pd.read_csv('./filled_active_df.csv')\n",
    "\n",
    "# best_users = trec.groupby('workerID').count().sort_values('docID', ascending=False)[:150].index\n",
    "# trec = trec[trec['workerID'].isin(best_users)]\n",
    "\n",
    "# r = pd.Series([2,3,2,3], index=[1,2,0,-2])\n",
    "# trec['label_bin'] = trec['label'].map(r)\n",
    "# adult.rating.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'2014311399-2', u'2014311244-0', u'2014311394-0', u'2015241543-0',\n",
       "       u'2015231543-0', u'2015221520-0', u'2015211520-0', u'2015221543-0',\n",
       "       u'2015201520-0', u'2015101520-0',\n",
       "       ...\n",
       "       u'2015141459-0', u'2015181417-0', u'2015151459-0', u'2015161459-0',\n",
       "       u'2015171459-0', u'2015181459-0', u'2015191459-0', u'2015201459-0',\n",
       "       u'2015211459-0', u'2015221459-0'],\n",
       "      dtype='object', name=u'ifp_week', length=1548)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ifp_var = gj.groupby('ifp_week')['value'].var().sort_values()\n",
    "ifps_less_var = (ifp_var<.1).index\n",
    "ifps_less_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         ifp_id ctt  cond training  team  user_id  value  fcast_year  \\\n",
      "0        1244-0  1a     1        a   NaN       51   0.20        2015   \n",
      "1        1244-0  1a     1        a   NaN       51   0.20        2015   \n",
      "2        1244-0  1a     1        a   NaN       51   0.20        2015   \n",
      "3        1244-0  1a     1        a   NaN       51   0.20        2015   \n",
      "4        1244-0  1a     1        a   NaN       51   0.20        2015   \n",
      "5        1244-0  1a     1        a   NaN       51   0.20        2015   \n",
      "6        1244-0  1a     1        a   NaN       51   0.09        2015   \n",
      "7        1244-0  1a     1        a   NaN       51   0.09        2015   \n",
      "8        1394-0  1a     1        a   NaN       51   0.75        2014   \n",
      "9        1394-0  1a     1        a   NaN       51   0.75        2014   \n",
      "10       1394-0  1a     1        a   NaN       51   0.75        2014   \n",
      "11       1394-0  1a     1        a   NaN       51   0.75        2014   \n",
      "12       1394-0  1a     1        a   NaN       51   0.75        2014   \n",
      "13       1394-0  1a     1        a   NaN       51   0.75        2014   \n",
      "14       1394-0  1a     1        a   NaN       51   0.75        2014   \n",
      "15       1394-0  1a     1        a   NaN       51   0.75        2014   \n",
      "16       1394-0  1a     1        a   NaN       51   0.75        2014   \n",
      "17       1394-0  1a     1        a   NaN       51   0.75        2014   \n",
      "18       1394-0  1a     1        a   NaN       51   0.75        2014   \n",
      "19       1394-0  1a     1        a   NaN       51   0.75        2014   \n",
      "20       1411-2  1a     1        a   NaN       51   0.30        2015   \n",
      "21       1411-2  1a     1        a   NaN       51   0.30        2015   \n",
      "22       1411-2  1a     1        a   NaN       51   0.30        2015   \n",
      "23       1411-2  1a     1        a   NaN       51   0.30        2015   \n",
      "24       1411-2  1a     1        a   NaN       51   0.30        2015   \n",
      "25       1411-2  1a     1        a   NaN       51   0.30        2015   \n",
      "26       1411-2  1a     1        a   NaN       51   0.30        2015   \n",
      "27       1411-2  1a     1        a   NaN       51   0.30        2015   \n",
      "28       1411-2  1a     1        a   NaN       51   0.30        2015   \n",
      "29       1411-2  1a     1        a   NaN       51   0.30        2015   \n",
      "...         ...  ..   ...      ...   ...      ...    ...         ...   \n",
      "1499264  1515-0  1q     1        q   NaN   145836   0.80        2015   \n",
      "1499265  1515-0  1q     1        q   NaN   145836   0.80        2015   \n",
      "1499266  1515-0  1q     1        q   NaN   145836   0.80        2015   \n",
      "1499267  1515-0  1q     1        q   NaN   145836   0.80        2015   \n",
      "1499268  1515-0  1q     1        q   NaN   145836   0.80        2015   \n",
      "1499269  1515-0  1q     1        q   NaN   145836   0.80        2015   \n",
      "1499270  1515-0  1q     1        q   NaN   145836   0.80        2015   \n",
      "1499271  1515-0  1q     1        q   NaN   145836   0.80        2015   \n",
      "1499272  1515-0  1q     1        q   NaN   145836   0.80        2015   \n",
      "1499273  1515-0  1q     1        q   NaN   145836   0.80        2015   \n",
      "1499274  1515-0  1q     1        q   NaN   145836   0.80        2015   \n",
      "1499275  1515-0  1q     1        q   NaN   145836   0.80        2015   \n",
      "1499276  1515-0  1q     1        q   NaN   145836   0.80        2015   \n",
      "1499277  1515-0  1q     1        q   NaN   145836   0.80        2015   \n",
      "1499278  6413-0  1q     1        q   NaN   145836   0.30        2015   \n",
      "1499279  6413-0  1q     1        q   NaN   145836   0.30        2015   \n",
      "1499280  6413-0  1q     1        q   NaN   145836   0.30        2015   \n",
      "1499281  6413-0  1q     1        q   NaN   145836   0.30        2015   \n",
      "1499282  6413-0  1q     1        q   NaN   145836   0.30        2015   \n",
      "1499283  6413-0  1q     1        q   NaN   145836   0.30        2015   \n",
      "1499284  6413-0  1q     1        q   NaN   145836   0.30        2015   \n",
      "1499285  6413-0  1q     1        q   NaN   145836   0.30        2015   \n",
      "1499286  6413-0  1q     1        q   NaN   145836   0.30        2015   \n",
      "1499287  6413-0  1q     1        q   NaN   145836   0.30        2015   \n",
      "1499288  6413-0  1q     1        q   NaN   145836   0.30        2015   \n",
      "1499289  6413-0  1q     1        q   NaN   145836   0.30        2015   \n",
      "1499290  6413-0  1q     1        q   NaN   145836   0.30        2015   \n",
      "1499291  6413-0  1q     1        q   NaN   145836   0.30        2015   \n",
      "1499292  6413-0  1q     1        q   NaN   145836   0.30        2015   \n",
      "1499293  6413-0  1q     1        q   NaN   145836   0.30        2015   \n",
      "\n",
      "         fcast_week      ifp_week   bin  task_id   uid  \n",
      "0                 3   201531244-0  0.20        0     0  \n",
      "1                 4   201541244-0  0.20        1     0  \n",
      "2                 5   201551244-0  0.20        2     0  \n",
      "3                 6   201561244-0  0.20        3     0  \n",
      "4                 7   201571244-0  0.20        4     0  \n",
      "5                 8   201581244-0  0.20        5     0  \n",
      "6                 8   201581244-0  0.09        5     0  \n",
      "7                 9   201591244-0  0.09        6     0  \n",
      "8                42  2014421394-0  0.75        7     0  \n",
      "9                43  2014431394-0  0.75        8     0  \n",
      "10               44  2014441394-0  0.75        9     0  \n",
      "11               45  2014451394-0  0.75       10     0  \n",
      "12               46  2014461394-0  0.75       11     0  \n",
      "13               47  2014471394-0  0.75       12     0  \n",
      "14               48  2014481394-0  0.75       13     0  \n",
      "15               49  2014491394-0  0.75       14     0  \n",
      "16               50  2014501394-0  0.75       15     0  \n",
      "17               51  2014511394-0  0.75       16     0  \n",
      "18               52  2014521394-0  0.75       17     0  \n",
      "19                1   201411394-0  0.75       18     0  \n",
      "20                3   201531411-2  0.30       19     0  \n",
      "21                4   201541411-2  0.30       20     0  \n",
      "22                5   201551411-2  0.30       21     0  \n",
      "23                6   201561411-2  0.30       22     0  \n",
      "24                7   201571411-2  0.30       23     0  \n",
      "25                8   201581411-2  0.30       24     0  \n",
      "26                9   201591411-2  0.30       25     0  \n",
      "27               10  2015101411-2  0.30       26     0  \n",
      "28               11  2015111411-2  0.30       27     0  \n",
      "29               12  2015121411-2  0.30       28     0  \n",
      "...             ...           ...   ...      ...   ...  \n",
      "1499264           9   201591515-0  0.80     1530  2051  \n",
      "1499265          10  2015101515-0  0.80     1286  2051  \n",
      "1499266          11  2015111515-0  0.80     1287  2051  \n",
      "1499267          12  2015121515-0  0.80     1288  2051  \n",
      "1499268          13  2015131515-0  0.80     1289  2051  \n",
      "1499269          14  2015141515-0  0.80     1290  2051  \n",
      "1499270          15  2015151515-0  0.80     1291  2051  \n",
      "1499271          16  2015161515-0  0.80     1292  2051  \n",
      "1499272          17  2015171515-0  0.80     1293  2051  \n",
      "1499273          18  2015181515-0  0.80     1294  2051  \n",
      "1499274          19  2015191515-0  0.80     1295  2051  \n",
      "1499275          20  2015201515-0  0.80      242  2051  \n",
      "1499276          21  2015211515-0  0.80      243  2051  \n",
      "1499277          22  2015221515-0  0.80      244  2051  \n",
      "1499278           9   201596413-0  0.30     1426  2051  \n",
      "1499279          10  2015106413-0  0.30     1427  2051  \n",
      "1499280          11  2015116413-0  0.30     1428  2051  \n",
      "1499281          12  2015126413-0  0.30     1429  2051  \n",
      "1499282          13  2015136413-0  0.30     1430  2051  \n",
      "1499283          14  2015146413-0  0.30     1431  2051  \n",
      "1499284          15  2015156413-0  0.30     1432  2051  \n",
      "1499285          16  2015166413-0  0.30     1433  2051  \n",
      "1499286          17  2015176413-0  0.30     1434  2051  \n",
      "1499287          18  2015186413-0  0.30     1435  2051  \n",
      "1499288          19  2015196413-0  0.30     1436  2051  \n",
      "1499289          20  2015206413-0  0.30     1437  2051  \n",
      "1499290          21  2015216413-0  0.30     1438  2051  \n",
      "1499291          22  2015226413-0  0.30     1439  2051  \n",
      "1499292          23  2015236413-0  0.30     1440  2051  \n",
      "1499293          24  2015246413-0  0.30     1441  2051  \n",
      "\n",
      "[1499294 rows x 13 columns]\n"
     ]
    }
   ],
   "source": [
    "print gj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "389"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ratingcts = adult.groupby('user_id')['rating'].nunique()\n",
    "# ratingcts = ratingcts[ratingcts==4]\n",
    "# len(ratingcts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ifp_id</th>\n",
       "      <th>ctt</th>\n",
       "      <th>cond</th>\n",
       "      <th>training</th>\n",
       "      <th>team</th>\n",
       "      <th>user_id</th>\n",
       "      <th>value</th>\n",
       "      <th>fcast_year</th>\n",
       "      <th>fcast_week</th>\n",
       "      <th>ifp_week</th>\n",
       "      <th>bin</th>\n",
       "      <th>task_id</th>\n",
       "      <th>uid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1244-0</td>\n",
       "      <td>1a</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2015</td>\n",
       "      <td>3</td>\n",
       "      <td>201531244-0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1244-0</td>\n",
       "      <td>1a</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2015</td>\n",
       "      <td>4</td>\n",
       "      <td>201541244-0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1244-0</td>\n",
       "      <td>1a</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2015</td>\n",
       "      <td>5</td>\n",
       "      <td>201551244-0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1244-0</td>\n",
       "      <td>1a</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2015</td>\n",
       "      <td>6</td>\n",
       "      <td>201561244-0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1244-0</td>\n",
       "      <td>1a</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2015</td>\n",
       "      <td>7</td>\n",
       "      <td>201571244-0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ifp_id ctt  cond training  team  user_id  value  fcast_year  fcast_week  \\\n",
       "0  1244-0  1a     1        a   NaN       51    0.2        2015           3   \n",
       "1  1244-0  1a     1        a   NaN       51    0.2        2015           4   \n",
       "2  1244-0  1a     1        a   NaN       51    0.2        2015           5   \n",
       "3  1244-0  1a     1        a   NaN       51    0.2        2015           6   \n",
       "4  1244-0  1a     1        a   NaN       51    0.2        2015           7   \n",
       "\n",
       "      ifp_week  bin  task_id  uid  \n",
       "0  201531244-0  0.2        0    0  \n",
       "1  201541244-0  0.2        1    0  \n",
       "2  201551244-0  0.2        2    0  \n",
       "3  201561244-0  0.2        3    0  \n",
       "4  201571244-0  0.2        4    0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(len(adult['user_id'].unique()))\n",
    "# ratingcts = adult.groupby('user_id')['rating'].nunique()\n",
    "# ratingcts = ratingcts[ratingcts==4]\n",
    "# uids = ratingcts.index\n",
    "# print(len(uids))\n",
    "# adult = adult[adult['user_id'].isin(uids)]\n",
    "# testframe = create_user_task_ids(adult, 'user_id', 'website', 'rating', prime=True)\n",
    "# print(np.max(testframe['uid'].values))\n",
    "testframe = create_user_task_ids(gj, 'user_id', 'ifp_week', 'value', False, True)\n",
    "testframe = testframe[testframe['ifp_week'].isin(ifps_less_var)]\n",
    "testframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1499294, 13)\n",
      "Index([u'ifp_id', u'ctt', u'cond', u'training', u'team', u'user_id', u'value',\n",
      "       u'fcast_year', u'fcast_week', u'ifp_week', u'bin', u'task_id', u'uid'],\n",
      "      dtype='object')\n",
      "2052\n",
      "1548\n"
     ]
    }
   ],
   "source": [
    "print(testframe.shape)\n",
    "print testframe.columns\n",
    "print(len(testframe['uid'].unique()))\n",
    "print(len(testframe['task_id'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "testframe['value'] = (testframe['value'] - .5)*2\n",
    "# testframe['value'] = testframe['value']\n",
    "# testframe['value'] = (testframe['value']*.5) + .5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ifp_id</th>\n",
       "      <th>ctt</th>\n",
       "      <th>cond</th>\n",
       "      <th>training</th>\n",
       "      <th>team</th>\n",
       "      <th>user_id</th>\n",
       "      <th>value</th>\n",
       "      <th>fcast_year</th>\n",
       "      <th>fcast_week</th>\n",
       "      <th>ifp_week</th>\n",
       "      <th>bin</th>\n",
       "      <th>task_id</th>\n",
       "      <th>uid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1244-0</td>\n",
       "      <td>1a</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>2015</td>\n",
       "      <td>3</td>\n",
       "      <td>201531244-0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1244-0</td>\n",
       "      <td>1a</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>2015</td>\n",
       "      <td>4</td>\n",
       "      <td>201541244-0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1244-0</td>\n",
       "      <td>1a</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>2015</td>\n",
       "      <td>5</td>\n",
       "      <td>201551244-0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1244-0</td>\n",
       "      <td>1a</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>2015</td>\n",
       "      <td>6</td>\n",
       "      <td>201561244-0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1244-0</td>\n",
       "      <td>1a</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>2015</td>\n",
       "      <td>7</td>\n",
       "      <td>201571244-0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ifp_id ctt  cond training  team  user_id  value  fcast_year  fcast_week  \\\n",
       "0  1244-0  1a     1        a   NaN       51   -0.6        2015           3   \n",
       "1  1244-0  1a     1        a   NaN       51   -0.6        2015           4   \n",
       "2  1244-0  1a     1        a   NaN       51   -0.6        2015           5   \n",
       "3  1244-0  1a     1        a   NaN       51   -0.6        2015           6   \n",
       "4  1244-0  1a     1        a   NaN       51   -0.6        2015           7   \n",
       "\n",
       "      ifp_week  bin  task_id  uid  \n",
       "0  201531244-0  0.2        0    0  \n",
       "1  201541244-0  0.2        1    0  \n",
       "2  201551244-0  0.2        2    0  \n",
       "3  201561244-0  0.2        3    0  \n",
       "4  201571244-0  0.2        4    0  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "c = pd.cut(\n",
    "    testframe['value'],\n",
    "    [-np.inf, .2, .4, .6, .8, np.inf],\n",
    "    labels=[2,3,5,7,11]\n",
    ")\n",
    "testframe['bin_levels'] = c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# testframe = testframe[testframe['uid']<1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def batcher(seq, size):\n",
    "    return (seq[pos:pos + size] for pos in range(0, len(seq), size))\n",
    "\n",
    "def split(df):\n",
    "    train_df, validate_df, test_df = np.split(df.sample(frac=1), [int(.6*len(df)), int(.8*len(df))])\n",
    "    return train_df, validate_df, test_df\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, users, tasks, k=2):\n",
    "        super(Model, self).__init__()\n",
    "        self.user_lut = nn.Embedding(users, k)\n",
    "        self.task_lut = nn.Embedding(tasks, k)\n",
    "\n",
    "        self.user_bias = nn.Embedding(users, 1)\n",
    "        self.task_bias = nn.Embedding(tasks, 1)\n",
    "        self.global_bias = nn.Parameter(torch.FloatTensor(1))\n",
    "        \n",
    "    def forward(self, users, jokes):\n",
    "        user_vectors = self.user_lut(users)\n",
    "        task_vectors = self.task_lut(jokes)\n",
    "        user_bias = self.user_bias(users)\n",
    "        task_bias = self.task_bias(jokes)\n",
    "        preds = torch.bmm(user_vectors.unsqueeze(1), task_vectors.unsqueeze(2)).squeeze() \\\n",
    "                         + user_bias.squeeze() + task_bias.squeeze() + self.global_bias.expand_as(user_bias.squeeze())\n",
    "#         m = nn.Sigmoid()\n",
    "        return preds\n",
    "        \n",
    "class Model2(nn.Module):\n",
    "    def __init__(self, users, tasks, k=2):\n",
    "        super(Model2, self).__init__()\n",
    "        self.user_lut = nn.Embedding(users, k)\n",
    "        self.task_lut = nn.Embedding(tasks, k)\n",
    "\n",
    "        self.user_bias = nn.Embedding(users, 1)\n",
    "        self.user_add_bias = nn.Embedding(users, 1)\n",
    "        self.task_bias = nn.Embedding(tasks, 1)\n",
    "        self.global_bias = nn.Parameter(torch.FloatTensor(1))\n",
    "        \n",
    "    def forward(self, users, jokes):\n",
    "        user_vectors = self.user_lut(users)\n",
    "        task_vectors = self.task_lut(jokes)\n",
    "        user_bias = self.user_bias(users)\n",
    "        task_bias = self.task_bias(jokes)\n",
    "        user_add_bias = self.user_add_bias(users)\n",
    "\n",
    "        preds = torch.bmm(user_vectors.unsqueeze(1), task_vectors.unsqueeze(2)).squeeze() \\\n",
    "                          + task_bias.squeeze() + self.global_bias.expand_as(user_bias.squeeze())\n",
    "        m = nn.Tanh()\n",
    "#         return m((user_bias.squeeze() * (preds - .5)) + .5)\n",
    "#         return m(user_bias.squeeze() * preds )\n",
    "        return m(user_bias.squeeze() * preds + user_add_bias.squeeze() )\n",
    "            \n",
    "\n",
    "def val(df, model):\n",
    "    crit = nn.MSELoss(size_average=False)\n",
    "    crit2 = nn.L1Loss(size_average=False)\n",
    "    total_loss = 0.\n",
    "    total_l1 = 0.\n",
    "    total_num = 0\n",
    "    for batch in batcher(df, 100):\n",
    "        true_rating = Variable(torch.Tensor(batch.bin.values.astype(float)))\n",
    "        total_num = total_num + true_rating.size(0)\n",
    "        users = Variable(torch.LongTensor(batch.uid.values))\n",
    "        tasks = Variable(torch.LongTensor(batch.task_id.values))\n",
    "#         print(users, tasks)\n",
    "        scores = model.forward(users, tasks)\n",
    "        total_loss += crit(scores, true_rating).data[0]\n",
    "        total_l1 += crit2(scores,true_rating).data[0]\n",
    "    return math.sqrt(total_loss/total_num), total_l1/total_num\n",
    "\n",
    "\n",
    "def train(train_iter, val_iter, test_iter, model):\n",
    "    opt = optim.SGD(model.parameters(), lr=.9)\n",
    "    crit = nn.MSELoss()\n",
    "    crit2 = nn.L1Loss()\n",
    "\n",
    "    print(\"val:\", val(validate_df, model))\n",
    "    for epochs in range(1):\n",
    "        avg_loss = 0\n",
    "        avg_l1 = 0\n",
    "        total = 0\n",
    "        for i,batch in enumerate(batcher(train_df, 100)):\n",
    "            opt.zero_grad()\n",
    "            if total == 0:\n",
    "                print batch.bin.values\n",
    "            rating = Variable(torch.Tensor(batch.bin.values.astype(float)))\n",
    "#             print(rating)\n",
    "            users = Variable(torch.LongTensor(batch.uid.values))\n",
    "            tasks = Variable(torch.LongTensor(batch.task_id.values))\n",
    "#             print(users, tasks)\n",
    "            scores = model.forward(users, tasks) \n",
    "#             + torch.sum(model.user_lut.weight.data.pow(2)) + \\\n",
    "#             torch.sum(model.task_lut.weight.data.pow(2)) + torch.sum(model.user_bias.weight.data.pow(2)) + \\\n",
    "#             torch.sum(model.task_bias.weight.data.pow(2))\n",
    "            loss = crit(scores, rating)\n",
    "            #if i % 1000==0:\n",
    "            #    print (loss.data[0])\n",
    "            loss.backward()\n",
    "            avg_loss += loss.data[0]\n",
    "            avg_l1 += crit2(scores,rating).data[0]\n",
    "            total += 1\n",
    "            opt.step()\n",
    "        print(\"train:\", math.sqrt(avg_loss / float(total)), avg_l1/ float(total))\n",
    "        print(\"val:\", val(validate_df, model))\n",
    "#         print(model.user_bias.weight.data)\n",
    "    return model.user_lut.weight.data, model.user_bias.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2051, 1547)\n",
      "2051\n",
      "1547\n",
      "('val:', (0.8875000975499351, 0.758736459664383))\n",
      "         ifp_id   ctt  cond training  team  user_id  value  fcast_year  \\\n",
      "200164   1428-0    1p     1        p   NaN     8142  -0.34        2014   \n",
      "278478   1467-2    1b     1        b   NaN     9735  -1.00        2015   \n",
      "833967   1459-0    1z     1        z   NaN   125407   0.30        2015   \n",
      "433576   1459-0    1a     1        a   NaN    22231   0.14        2014   \n",
      "376032   1428-0    1a     1        a   NaN    21150  -0.34        2014   \n",
      "539241   1419-2    1h     1        h   NaN    22599  -0.20        2014   \n",
      "231171   1244-0   4b7     4        b   7.0     8811  -0.80        2014   \n",
      "588708   1414-0  4b22     4        b  22.0    22871  -0.50        2014   \n",
      "1443525  1529-0  4w74     4        w  74.0   145238  -1.00        2015   \n",
      "537973   1480-0    1h     1        h   NaN    22580   0.14        2014   \n",
      "1276099  1438-0    1z     1        z   NaN   130640   0.50        2014   \n",
      "1295975  1417-0    1z     1        z   NaN   130782  -0.50        2015   \n",
      "967002   1394-0   1r1     1        r   1.0   127310  -0.60        2014   \n",
      "76494    1502-0   5s3     5        s   3.0     3001  -0.60        2015   \n",
      "296977   1440-0  4b11     4        b  11.0    10212  -1.00        2015   \n",
      "978689   1244-0   1r1     1        r   1.0   127448  -0.60        2014   \n",
      "834324   1421-0    1z     1        z   NaN   125461   0.00        2014   \n",
      "825905   1449-0   1r1     1        r   1.0   125044   0.30        2014   \n",
      "341307   1477-0   5s2     5        s   2.0    15063  -1.00        2015   \n",
      "1090979  1435-0    1n     1        n   NaN   128570  -0.80        2015   \n",
      "1387326  1475-0    1z     1        z   NaN   132770  -0.76        2015   \n",
      "756836   1466-0   1r1     1        r   1.0   124396  -0.94        2015   \n",
      "141333   1478-0  4h46     4        h  46.0     4938  -0.80        2015   \n",
      "1157101  1427-0    1z     1        z   NaN   129689  -1.00        2014   \n",
      "772110   1482-0   1r1     1        r   1.0   124544  -0.08        2014   \n",
      "472035   1459-0  4h48     4        h  48.0    22299  -0.50        2014   \n",
      "210309   1514-0  4b19     4        b  19.0     8224  -1.00        2015   \n",
      "19541    1482-0   5s2     5        s   2.0      803  -0.70        2014   \n",
      "709308   1487-0    1z     1        z   NaN   123651  -0.70        2015   \n",
      "649661   1440-0  4h31     4        h  31.0    23278  -0.50        2014   \n",
      "...         ...   ...   ...      ...   ...      ...    ...         ...   \n",
      "778788   1417-0   1r1     1        r   1.0   124619  -0.20        2014   \n",
      "953334   1455-0   1r1     1        r   1.0   127165  -0.30        2015   \n",
      "925934   1413-0   1r1     1        r   1.0   126972  -0.98        2014   \n",
      "7315     1430-0    1a     1        a   NaN      362   0.50        2014   \n",
      "544822   1434-0    1h     1        h   NaN    22641  -0.52        2014   \n",
      "198762   1435-0  4b17     4        b  17.0     8124  -0.30        2014   \n",
      "812396   1414-0   1r1     1        r   1.0   124891   0.82        2014   \n",
      "1065138  1244-0   1r1     1        r   1.0   128308   0.44        2014   \n",
      "229541   1409-0  4h47     4        h  47.0     8809   0.80        2014   \n",
      "1332693  1535-0    1z     1        z   NaN   131131  -1.00        2015   \n",
      "180350   1466-0   5s5     5        s   5.0     6372  -0.98        2015   \n",
      "1363191  1394-0    1z     1        z   NaN   131434  -0.70        2014   \n",
      "1200440  1420-0    1z     1        z   NaN   129994   0.40        2015   \n",
      "917782   1420-0   1r1     1        r   1.0   126876   0.10        2014   \n",
      "919239   1399-2   1r1     1        r   1.0   126899  -0.70        2014   \n",
      "514916   1455-0    1b     1        b   NaN    22432   0.20        2014   \n",
      "1213574  1409-0    1z     1        z   NaN   130072   0.20        2014   \n",
      "1375479  1411-2    1z     1        z   NaN   132023  -0.70        2015   \n",
      "640370   1467-2  4h48     4        h  48.0    23211  -0.20        2014   \n",
      "1372018  1413-0    1z     1        z   NaN   131496  -0.78        2014   \n",
      "306048   1482-0   5s2     5        s   2.0    13190  -0.50        2014   \n",
      "790548   1434-0   1r1     1        r   1.0   124756  -0.30        2014   \n",
      "532347   1417-0    1p     1        p   NaN    22540  -0.60        2014   \n",
      "67242    1472-2   5s8     5        s   8.0     2880  -0.98        2015   \n",
      "367437   1430-0    1a     1        a   NaN    21046  -0.68        2015   \n",
      "274554   1449-0    1p     1        p   NaN     9657  -0.40        2014   \n",
      "101000   1410-0  4h52     4        h  52.0     3626  -0.72        2014   \n",
      "1484697  1535-0  4u26     4        u  26.0   145576   0.34        2015   \n",
      "3886     1483-0   5s1     5        s   1.0      149   0.50        2015   \n",
      "1282799  1482-0    1z     1        z   NaN   130690  -0.60        2015   \n",
      "\n",
      "         fcast_week      ifp_week   bin  task_id   uid  \n",
      "200164           46  2014461428-0  0.33       76   249  \n",
      "278478           23  2015231467-2  0.00      170   342  \n",
      "833967            8   201581459-0  0.65      803  1095  \n",
      "433576           50  2014501459-0  0.57      792   547  \n",
      "376032           42  2014421428-0  0.33       72   478  \n",
      "539241           40  2014401419-2  0.40      448   675  \n",
      "231171           42  2014421244-0  0.10      302   287  \n",
      "588708           37  2014371414-0  0.25       42   750  \n",
      "1443525          21  2015211529-0  0.00     1345  1957  \n",
      "537973            1   201411480-0  0.57     1052   672  \n",
      "1276099          49  2014491438-0  0.75      699  1682  \n",
      "1295975           7   201571417-0  0.25      427  1711  \n",
      "967002           49  2014491394-0  0.20       14  1274  \n",
      "76494             7   201571502-0  0.20     1180    86  \n",
      "296977            8   201581440-0  0.00      727   367  \n",
      "978689           46  2014461244-0  0.20      306  1291  \n",
      "834324           46  2014461421-0  0.50      506  1096  \n",
      "825905           46  2014461449-0  0.65      123  1082  \n",
      "341307           24  2015241477-0  0.00     1035   430  \n",
      "1090979          20  2015201435-0  0.10      653  1433  \n",
      "1387326           3   201531475-0  0.12      978  1846  \n",
      "756836            9   201591466-0  0.03      875   994  \n",
      "141333            8   201581478-0  0.10     1047   163  \n",
      "1157101          45  2014451427-0  0.00       63  1518  \n",
      "772110           52  2014521482-0  0.46     1478  1009  \n",
      "472035           47  2014471459-0  0.25      789   583  \n",
      "210309           20  2015201514-0  0.00     1284   262  \n",
      "19541            51  2014511482-0  0.15     1520    22  \n",
      "709308           20  2015201487-0  0.15      210   931  \n",
      "649661           46  2014461440-0  0.25      712   843  \n",
      "...             ...           ...   ...      ...   ...  \n",
      "778788           42  2014421417-0  0.40      409  1018  \n",
      "953334            1   201511455-0  0.35      773  1257  \n",
      "925934           41  2014411413-0  0.01      382  1224  \n",
      "7315             43  2014431430-0  0.75      581     9  \n",
      "544822           41  2014411434-0  0.24      605   685  \n",
      "198762           48  2014481435-0  0.35      628   246  \n",
      "812396           35  2014351414-0  0.91       40  1064  \n",
      "1065138          41  2014411244-0  0.72      301  1398  \n",
      "229541           35  2014351409-0  0.90      340   286  \n",
      "1332693          15  2015151535-0  0.00     1535  1763  \n",
      "180350            7   201571466-0  0.01      873   220  \n",
      "1363191          49  2014491394-0  0.15       14  1806  \n",
      "1200440          15  2015151420-0  0.70      490  1576  \n",
      "917782           44  2014441420-0  0.55      466  1212  \n",
      "919239           46  2014461399-2  0.15      332  1214  \n",
      "514916           45  2014451455-0  0.60     1507   638  \n",
      "1213574          36  2014361409-0  0.60      341  1594  \n",
      "1375479           7   201571411-2  0.15       23  1825  \n",
      "640370           49  2014491467-2  0.40      893   830  \n",
      "1372018          48  2014481413-0  0.11      389  1819  \n",
      "306048            1   201411482-0  0.25     1479   380  \n",
      "790548           43  2014431434-0  0.35      607  1035  \n",
      "532347           52  2014521417-0  0.20      419   664  \n",
      "67242            23  2015231472-2  0.01      948    79  \n",
      "367437           13  2015131430-0  0.16       94   469  \n",
      "274554           47  2014471449-0  0.30      124   339  \n",
      "101000           34  2014341410-0  0.14     1449   115  \n",
      "1484697          16  2015161535-0  0.67     1361  2024  \n",
      "3886              4   201541483-0  0.75     1099     5  \n",
      "1282799           1   201511482-0  0.20     1074  1691  \n",
      "\n",
      "[100 rows x 13 columns]\n",
      "200164     0.33\n",
      "278478     0.00\n",
      "833967     0.65\n",
      "433576     0.57\n",
      "376032     0.33\n",
      "539241     0.40\n",
      "231171     0.10\n",
      "588708     0.25\n",
      "1443525    0.00\n",
      "537973     0.57\n",
      "1276099    0.75\n",
      "1295975    0.25\n",
      "967002     0.20\n",
      "76494      0.20\n",
      "296977     0.00\n",
      "978689     0.20\n",
      "834324     0.50\n",
      "825905     0.65\n",
      "341307     0.00\n",
      "1090979    0.10\n",
      "1387326    0.12\n",
      "756836     0.03\n",
      "141333     0.10\n",
      "1157101    0.00\n",
      "772110     0.46\n",
      "472035     0.25\n",
      "210309     0.00\n",
      "19541      0.15\n",
      "709308     0.15\n",
      "649661     0.25\n",
      "           ... \n",
      "778788     0.40\n",
      "953334     0.35\n",
      "925934     0.01\n",
      "7315       0.75\n",
      "544822     0.24\n",
      "198762     0.35\n",
      "812396     0.91\n",
      "1065138    0.72\n",
      "229541     0.90\n",
      "1332693    0.00\n",
      "180350     0.01\n",
      "1363191    0.15\n",
      "1200440    0.70\n",
      "917782     0.55\n",
      "919239     0.15\n",
      "514916     0.60\n",
      "1213574    0.60\n",
      "1375479    0.15\n",
      "640370     0.40\n",
      "1372018    0.11\n",
      "306048     0.25\n",
      "790548     0.35\n",
      "532347     0.20\n",
      "67242      0.01\n",
      "367437     0.16\n",
      "274554     0.30\n",
      "101000     0.14\n",
      "1484697    0.67\n",
      "3886       0.75\n",
      "1282799    0.20\n",
      "Name: bin, dtype: float64\n",
      "[ 0.33  0.    0.65  0.57  0.33  0.4   0.1   0.25  0.    0.57  0.75  0.25\n",
      "  0.2   0.2   0.    0.2   0.5   0.65  0.    0.1   0.12  0.03  0.1   0.\n",
      "  0.46  0.25  0.    0.15  0.15  0.25  0.22  1.    0.9   0.75  0.05  0.39\n",
      "  0.15  1.    0.15  0.75  0.24  0.15  0.    0.85  0.15  1.    0.    0.75\n",
      "  0.54  0.68  0.25  0.65  0.3   0.01  1.    0.15  1.    0.    0.2   1.\n",
      "  0.09  0.5   0.01  0.1   1.    0.62  0.8   0.    0.1   0.3   0.4   0.35\n",
      "  0.01  0.75  0.24  0.35  0.91  0.72  0.9   0.    0.01  0.15  0.7   0.55\n",
      "  0.15  0.6   0.6   0.15  0.4   0.11  0.25  0.35  0.2   0.01  0.16  0.3\n",
      "  0.14  0.67  0.75  0.2 ]\n",
      "('train:', 0.623172278066906, 0.4706275628381912)\n",
      "('val:', (0.4772414779591901, 0.3500622458349797))\n"
     ]
    }
   ],
   "source": [
    "train_df, validate_df, test_df = split(testframe)\n",
    "validate_df = validate_df[validate_df['uid'].isin(train_df['uid'].values)][validate_df['task_id'].isin(train_df['task_id'].values)]\n",
    "\n",
    "users = int(np.max(train_df.uid.unique()))\n",
    "tasks = int(np.max(train_df.task_id.unique()))\n",
    "print(users, tasks)\n",
    "print(int(np.max(validate_df.uid.unique())))\n",
    "print(int(np.max(validate_df.task_id.unique())))\n",
    "model = Model2(users+1, tasks+1, k=4)\n",
    "user_vec, user_bias = train(train_df, validate_df, test_df, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "user_vec, user_bias = model.user_lut.weight.data, model.user_bias.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      user_id  gender_yr4  citizen_yr4  workhours_yr4  tradeexp_yr4  \\\n",
      "0        -1.0         1.0         -1.0            5.0           3.0   \n",
      "1         2.0         1.0          1.0            4.0           2.0   \n",
      "2         4.0         1.0          1.0            6.0           1.0   \n",
      "3         5.0        -1.0          1.0            4.0           3.0   \n",
      "4         7.0         1.0          1.0            4.0           3.0   \n",
      "5         8.0         1.0          1.0            4.0           1.0   \n",
      "6         9.0         1.0          1.0            2.0           3.0   \n",
      "7        10.0         1.0          1.0            1.0           3.0   \n",
      "8        11.0         1.0          1.0            1.0           2.0   \n",
      "9        12.0         1.0         -1.0            6.0           2.0   \n",
      "10       13.0         1.0          1.0            5.0           1.0   \n",
      "11       14.0         1.0          1.0            5.0           3.0   \n",
      "12       15.0         1.0         -1.0            4.0           2.0   \n",
      "13       16.0         1.0          1.0            5.0           4.0   \n",
      "14       17.0         1.0          1.0            5.0           3.0   \n",
      "15       18.0         1.0          1.0            5.0           1.0   \n",
      "16       19.0         1.0         -1.0            1.0           1.0   \n",
      "17       20.0         1.0          1.0            5.0           4.0   \n",
      "18       21.0         1.0          1.0            5.0           4.0   \n",
      "19       22.0         1.0          1.0            5.0           2.0   \n",
      "20       23.0         1.0          1.0            5.0           1.0   \n",
      "21       25.0         1.0          1.0            1.0           2.0   \n",
      "22       26.0         1.0          1.0            5.0           2.0   \n",
      "23       27.0         1.0          1.0            6.0           3.0   \n",
      "24       28.0         1.0          1.0            5.0           2.0   \n",
      "25       29.0         1.0          1.0            5.0           1.0   \n",
      "26       30.0         1.0          1.0            5.0           3.0   \n",
      "27       31.0         1.0          1.0            3.0           3.0   \n",
      "28       32.0         1.0          1.0            6.0           1.0   \n",
      "29       34.0         1.0          1.0            1.0           4.0   \n",
      "...       ...         ...          ...            ...           ...   \n",
      "1682   2021.0         1.0         -1.0            4.0           3.0   \n",
      "1683   2022.0         1.0         -1.0           -1.0          -1.0   \n",
      "1684   2023.0         1.0         -1.0           -1.0          -1.0   \n",
      "1685   2024.0         1.0          1.0            1.0           1.0   \n",
      "1686   2025.0         1.0          1.0           -1.0          -1.0   \n",
      "1687   2026.0         1.0          1.0           -1.0          -1.0   \n",
      "1688   2027.0         1.0          1.0           -1.0          -1.0   \n",
      "1689   2028.0         1.0          1.0           -1.0          -1.0   \n",
      "1690   2029.0        -1.0          1.0           -1.0          -1.0   \n",
      "1691   2030.0         1.0         -1.0           -1.0          -1.0   \n",
      "1692   2031.0         1.0          1.0           -1.0          -1.0   \n",
      "1693   2032.0         1.0         -1.0           -1.0          -1.0   \n",
      "1694   2033.0        -1.0          1.0           -1.0          -1.0   \n",
      "1695   2034.0         1.0          1.0           -1.0          -1.0   \n",
      "1696   2035.0        -1.0          1.0           -1.0          -1.0   \n",
      "1697   2036.0         1.0          1.0           -1.0          -1.0   \n",
      "1698   2037.0        -1.0          1.0           -1.0          -1.0   \n",
      "1699   2038.0         1.0          1.0           -1.0          -1.0   \n",
      "1700   2039.0         1.0         -1.0           -1.0          -1.0   \n",
      "1701   2040.0         1.0          1.0           -1.0          -1.0   \n",
      "1702   2041.0         1.0          1.0           -1.0          -1.0   \n",
      "1703   2042.0        -1.0          1.0           -1.0          -1.0   \n",
      "1704   2044.0         1.0          1.0           -1.0          -1.0   \n",
      "1705   2045.0         1.0          1.0           -1.0          -1.0   \n",
      "1706   2046.0         1.0         -1.0            1.0           3.0   \n",
      "1707   2047.0         1.0          1.0           -1.0          -1.0   \n",
      "1708   2048.0         1.0          1.0           -1.0          -1.0   \n",
      "1709   2049.0         1.0         -1.0           -1.0          -1.0   \n",
      "1710   2050.0        -1.0         -1.0           -1.0          -1.0   \n",
      "1711   2051.0         1.0         -1.0           -1.0          -1.0   \n",
      "\n",
      "      tradeenjoy_yr4  newshrs_yr4  raven_score_yr4  crt_score_yr4  \\\n",
      "0                3.0          7.0              9.0           16.0   \n",
      "1                4.0          3.0              6.0            9.0   \n",
      "2                7.0         20.0             11.0           15.0   \n",
      "3                4.0          4.0             12.0           18.0   \n",
      "4                5.0          4.0             12.0           15.0   \n",
      "5                4.0          5.0              5.0           13.0   \n",
      "6                5.0         10.0              9.0           18.0   \n",
      "7                4.0          7.0             10.0           16.0   \n",
      "8                5.0         10.0              5.0           -1.0   \n",
      "9                5.0          6.0             10.0           18.0   \n",
      "10               7.0          6.0              6.0           18.0   \n",
      "11               5.0          2.5             11.0           13.0   \n",
      "12               3.0          2.0             12.0           18.0   \n",
      "13               5.0          0.0             10.0           17.0   \n",
      "14               4.0          3.0             10.0           17.0   \n",
      "15               7.0         10.0              5.0           10.0   \n",
      "16               7.0         10.0              3.0           12.0   \n",
      "17               4.0          2.0              6.0           16.0   \n",
      "18               5.0          2.0              9.0            7.0   \n",
      "19               6.0          6.0              5.0           17.0   \n",
      "20               7.0         10.0              4.0            9.0   \n",
      "21               4.0          4.0              8.0           18.0   \n",
      "22               4.0         15.0              7.0           18.0   \n",
      "23               4.0          6.0              1.0           10.0   \n",
      "24               4.0          1.0              9.0           16.0   \n",
      "25               7.0          4.0              9.0           18.0   \n",
      "26               5.0         10.0              8.0           17.0   \n",
      "27               4.0          4.0              7.0           15.0   \n",
      "28               7.0          5.0             10.0           17.0   \n",
      "29               5.0          1.0             11.0           18.0   \n",
      "...              ...          ...              ...            ...   \n",
      "1682             6.0          5.0             -1.0           -1.0   \n",
      "1683            -1.0          4.0             -1.0           -1.0   \n",
      "1684            -1.0         11.0             -1.0           -1.0   \n",
      "1685             6.0          4.0              9.0           14.0   \n",
      "1686            -1.0          5.0             -1.0           -1.0   \n",
      "1687            -1.0          6.0             -1.0           -1.0   \n",
      "1688            -1.0          7.0             -1.0           -1.0   \n",
      "1689            -1.0         15.0             -1.0           -1.0   \n",
      "1690            -1.0          8.0             -1.0           -1.0   \n",
      "1691            -1.0          3.0             -1.0           -1.0   \n",
      "1692            -1.0         40.0             -1.0           -1.0   \n",
      "1693            -1.0         10.0             -1.0           -1.0   \n",
      "1694            -1.0          8.0             -1.0           -1.0   \n",
      "1695            -1.0          2.0             -1.0           -1.0   \n",
      "1696            -1.0          5.0             -1.0           -1.0   \n",
      "1697            -1.0         20.0             -1.0           -1.0   \n",
      "1698            -1.0          1.0             -1.0           -1.0   \n",
      "1699            -1.0          6.0             -1.0           -1.0   \n",
      "1700            -1.0         30.0             -1.0           -1.0   \n",
      "1701            -1.0         20.0             -1.0           -1.0   \n",
      "1702            -1.0          3.0             -1.0           -1.0   \n",
      "1703            -1.0          4.0             -1.0           -1.0   \n",
      "1704            -1.0         20.0             -1.0           -1.0   \n",
      "1705            -1.0         15.0             -1.0           -1.0   \n",
      "1706             4.0         12.0             -1.0           -1.0   \n",
      "1707            -1.0         16.0             -1.0           -1.0   \n",
      "1708            -1.0         35.0             -1.0           -1.0   \n",
      "1709            -1.0         15.0             -1.0           -1.0   \n",
      "1710            -1.0          1.0             -1.0           -1.0   \n",
      "1711            -1.0          5.0             -1.0           -1.0   \n",
      "\n",
      "      crt_1_cor_yr4             ...              motivation_2  motivation_3  \\\n",
      "0               1.0             ...                       5.0           1.0   \n",
      "1               1.0             ...                       5.0           6.0   \n",
      "2               1.0             ...                       2.0           2.0   \n",
      "3               1.0             ...                       5.0           2.0   \n",
      "4               1.0             ...                       5.0           4.0   \n",
      "5               1.0             ...                       6.0           4.0   \n",
      "6               1.0             ...                       5.0           2.0   \n",
      "7               1.0             ...                       6.0           1.0   \n",
      "8              -1.0             ...                       5.0           5.0   \n",
      "9               1.0             ...                       2.0           2.0   \n",
      "10              1.0             ...                       4.0           1.0   \n",
      "11             -1.0             ...                       5.0           5.0   \n",
      "12              1.0             ...                       5.0           1.0   \n",
      "13              1.0             ...                       6.0           3.0   \n",
      "14              1.0             ...                       3.0           1.0   \n",
      "15              1.0             ...                       5.0           2.0   \n",
      "16              1.0             ...                       7.0           1.0   \n",
      "17             -1.0             ...                       4.0           2.0   \n",
      "18             -1.0             ...                       4.0           1.0   \n",
      "19              1.0             ...                       2.0           1.0   \n",
      "20             -1.0             ...                       5.0           4.0   \n",
      "21              1.0             ...                       2.0           1.0   \n",
      "22              1.0             ...                       7.0           2.0   \n",
      "23             -1.0             ...                       5.0           2.0   \n",
      "24              1.0             ...                       5.0           2.0   \n",
      "25              1.0             ...                       5.0           5.0   \n",
      "26              1.0             ...                       2.0           7.0   \n",
      "27              1.0             ...                       3.0           5.0   \n",
      "28              1.0             ...                       5.0           3.0   \n",
      "29              1.0             ...                       4.0           1.0   \n",
      "...             ...             ...                       ...           ...   \n",
      "1682           -1.0             ...                      -1.0          -1.0   \n",
      "1683           -1.0             ...                      -1.0          -1.0   \n",
      "1684           -1.0             ...                      -1.0          -1.0   \n",
      "1685           -1.0             ...                       4.0           2.0   \n",
      "1686           -1.0             ...                      -1.0          -1.0   \n",
      "1687           -1.0             ...                      -1.0          -1.0   \n",
      "1688           -1.0             ...                      -1.0          -1.0   \n",
      "1689           -1.0             ...                      -1.0          -1.0   \n",
      "1690           -1.0             ...                      -1.0          -1.0   \n",
      "1691           -1.0             ...                      -1.0          -1.0   \n",
      "1692           -1.0             ...                      -1.0          -1.0   \n",
      "1693           -1.0             ...                      -1.0          -1.0   \n",
      "1694           -1.0             ...                      -1.0          -1.0   \n",
      "1695           -1.0             ...                      -1.0          -1.0   \n",
      "1696           -1.0             ...                      -1.0          -1.0   \n",
      "1697           -1.0             ...                      -1.0          -1.0   \n",
      "1698           -1.0             ...                      -1.0          -1.0   \n",
      "1699           -1.0             ...                      -1.0          -1.0   \n",
      "1700           -1.0             ...                      -1.0          -1.0   \n",
      "1701           -1.0             ...                      -1.0          -1.0   \n",
      "1702           -1.0             ...                      -1.0          -1.0   \n",
      "1703           -1.0             ...                      -1.0          -1.0   \n",
      "1704           -1.0             ...                      -1.0          -1.0   \n",
      "1705           -1.0             ...                      -1.0          -1.0   \n",
      "1706           -1.0             ...                      -1.0          -1.0   \n",
      "1707           -1.0             ...                      -1.0          -1.0   \n",
      "1708           -1.0             ...                      -1.0          -1.0   \n",
      "1709           -1.0             ...                      -1.0          -1.0   \n",
      "1710           -1.0             ...                      -1.0          -1.0   \n",
      "1711           -1.0             ...                      -1.0          -1.0   \n",
      "\n",
      "      motivation_4  motivation_5  motivation_6  motivation_7  motivation_8  \\\n",
      "0              7.0           5.0           6.0           5.0           5.0   \n",
      "1              5.0           6.0           6.0           4.0           6.0   \n",
      "2              5.0           4.0           5.0           4.0           4.0   \n",
      "3              5.0           5.0           7.0           5.0           5.0   \n",
      "4              5.0           3.0           5.0           6.0           5.0   \n",
      "5              7.0           6.0           7.0           6.0           6.0   \n",
      "6              6.0           5.0           5.0           5.0           5.0   \n",
      "7              7.0           6.0           7.0           6.0           5.0   \n",
      "8              5.0           5.0           5.0           5.0           5.0   \n",
      "9              6.0           2.0           6.0           2.0           2.0   \n",
      "10             5.0           4.0           6.0           4.0           2.0   \n",
      "11             5.0           5.0           5.0           5.0           5.0   \n",
      "12             5.0           5.0           6.0           5.0           4.0   \n",
      "13             5.0           6.0           2.0           6.0           6.0   \n",
      "14             7.0           5.0           6.0           2.0           3.0   \n",
      "15             6.0           5.0           6.0           4.0           5.0   \n",
      "16             7.0           6.0           7.0           7.0           6.0   \n",
      "17             5.0           4.0           5.0           2.0           4.0   \n",
      "18             4.0           4.0           5.0           4.0           4.0   \n",
      "19             6.0           4.0           6.0           4.0           2.0   \n",
      "20             5.0           5.0           5.0           5.0           5.0   \n",
      "21             6.0           2.0           6.0           2.0           2.0   \n",
      "22             6.0           7.0           7.0           4.0           7.0   \n",
      "23             5.0           5.0           5.0           5.0           5.0   \n",
      "24             6.0           6.0           6.0           6.0           4.0   \n",
      "25             7.0           5.0           6.0           5.0           5.0   \n",
      "26             4.0           3.0           3.0           3.0           1.0   \n",
      "27             6.0           3.0           6.0           2.0           2.0   \n",
      "28             6.0           5.0           6.0           4.0           4.0   \n",
      "29             6.0           4.0           6.0           4.0           4.0   \n",
      "...            ...           ...           ...           ...           ...   \n",
      "1682          -1.0          -1.0          -1.0          -1.0          -1.0   \n",
      "1683          -1.0          -1.0          -1.0          -1.0          -1.0   \n",
      "1684          -1.0          -1.0          -1.0          -1.0          -1.0   \n",
      "1685           4.0           5.0           5.0           5.0           5.0   \n",
      "1686          -1.0          -1.0          -1.0          -1.0          -1.0   \n",
      "1687          -1.0          -1.0          -1.0          -1.0          -1.0   \n",
      "1688          -1.0          -1.0          -1.0          -1.0          -1.0   \n",
      "1689          -1.0          -1.0          -1.0          -1.0          -1.0   \n",
      "1690          -1.0          -1.0          -1.0          -1.0          -1.0   \n",
      "1691          -1.0          -1.0          -1.0          -1.0          -1.0   \n",
      "1692          -1.0          -1.0          -1.0          -1.0          -1.0   \n",
      "1693          -1.0          -1.0          -1.0          -1.0          -1.0   \n",
      "1694          -1.0          -1.0          -1.0          -1.0          -1.0   \n",
      "1695          -1.0          -1.0          -1.0          -1.0          -1.0   \n",
      "1696          -1.0          -1.0          -1.0          -1.0          -1.0   \n",
      "1697          -1.0          -1.0          -1.0          -1.0          -1.0   \n",
      "1698          -1.0          -1.0          -1.0          -1.0          -1.0   \n",
      "1699          -1.0          -1.0          -1.0          -1.0          -1.0   \n",
      "1700          -1.0          -1.0          -1.0          -1.0          -1.0   \n",
      "1701          -1.0          -1.0          -1.0          -1.0          -1.0   \n",
      "1702          -1.0          -1.0          -1.0          -1.0          -1.0   \n",
      "1703          -1.0          -1.0          -1.0          -1.0          -1.0   \n",
      "1704          -1.0          -1.0          -1.0          -1.0          -1.0   \n",
      "1705          -1.0          -1.0          -1.0          -1.0          -1.0   \n",
      "1706          -1.0          -1.0          -1.0          -1.0          -1.0   \n",
      "1707          -1.0          -1.0          -1.0          -1.0          -1.0   \n",
      "1708          -1.0          -1.0          -1.0          -1.0          -1.0   \n",
      "1709          -1.0          -1.0          -1.0          -1.0          -1.0   \n",
      "1710          -1.0          -1.0          -1.0          -1.0          -1.0   \n",
      "1711          -1.0          -1.0          -1.0          -1.0          -1.0   \n",
      "\n",
      "      motivation_intrinsic_score  motivation_prosocial_score  \\\n",
      "0                            6.0                         5.0   \n",
      "1                            5.0                         5.0   \n",
      "2                            5.0                         4.0   \n",
      "3                            5.0                         5.0   \n",
      "4                            5.0                         5.0   \n",
      "5                            7.0                         6.0   \n",
      "6                            6.0                         5.0   \n",
      "7                            7.0                         6.0   \n",
      "8                            5.0                         5.0   \n",
      "9                            6.0                         2.0   \n",
      "10                           6.0                         4.0   \n",
      "11                           5.0                         5.0   \n",
      "12                           6.0                         5.0   \n",
      "13                           3.0                         6.0   \n",
      "14                           6.0                         3.0   \n",
      "15                           6.0                         5.0   \n",
      "16                           7.0                         7.0   \n",
      "17                           5.0                         4.0   \n",
      "18                           5.0                         4.0   \n",
      "19                           6.0                         3.0   \n",
      "20                           5.0                         5.0   \n",
      "21                           6.0                         2.0   \n",
      "22                           7.0                         6.0   \n",
      "23                           5.0                         5.0   \n",
      "24                           6.0                         5.0   \n",
      "25                           6.0                         5.0   \n",
      "26                           3.0                         2.0   \n",
      "27                           6.0                         3.0   \n",
      "28                           6.0                         5.0   \n",
      "29                           6.0                         4.0   \n",
      "...                          ...                         ...   \n",
      "1682                        -1.0                        -1.0   \n",
      "1683                        -1.0                        -1.0   \n",
      "1684                        -1.0                        -1.0   \n",
      "1685                         4.0                         5.0   \n",
      "1686                        -1.0                        -1.0   \n",
      "1687                        -1.0                        -1.0   \n",
      "1688                        -1.0                        -1.0   \n",
      "1689                        -1.0                        -1.0   \n",
      "1690                        -1.0                        -1.0   \n",
      "1691                        -1.0                        -1.0   \n",
      "1692                        -1.0                        -1.0   \n",
      "1693                        -1.0                        -1.0   \n",
      "1694                        -1.0                        -1.0   \n",
      "1695                        -1.0                        -1.0   \n",
      "1696                        -1.0                        -1.0   \n",
      "1697                        -1.0                        -1.0   \n",
      "1698                        -1.0                        -1.0   \n",
      "1699                        -1.0                        -1.0   \n",
      "1700                        -1.0                        -1.0   \n",
      "1701                        -1.0                        -1.0   \n",
      "1702                        -1.0                        -1.0   \n",
      "1703                        -1.0                        -1.0   \n",
      "1704                        -1.0                        -1.0   \n",
      "1705                        -1.0                        -1.0   \n",
      "1706                        -1.0                        -1.0   \n",
      "1707                        -1.0                        -1.0   \n",
      "1708                        -1.0                        -1.0   \n",
      "1709                        -1.0                        -1.0   \n",
      "1710                        -1.0                        -1.0   \n",
      "1711                        -1.0                        -1.0   \n",
      "\n",
      "      motivation_extrinsic_score  \n",
      "0                            1.0  \n",
      "1                            6.0  \n",
      "2                            2.0  \n",
      "3                            2.0  \n",
      "4                            4.0  \n",
      "5                            4.0  \n",
      "6                            2.0  \n",
      "7                            1.0  \n",
      "8                            5.0  \n",
      "9                            2.0  \n",
      "10                           1.0  \n",
      "11                           5.0  \n",
      "12                           1.0  \n",
      "13                           3.0  \n",
      "14                           1.0  \n",
      "15                           2.0  \n",
      "16                           1.0  \n",
      "17                           2.0  \n",
      "18                           1.0  \n",
      "19                           1.0  \n",
      "20                           4.0  \n",
      "21                           1.0  \n",
      "22                           2.0  \n",
      "23                           2.0  \n",
      "24                           2.0  \n",
      "25                           5.0  \n",
      "26                           7.0  \n",
      "27                           5.0  \n",
      "28                           3.0  \n",
      "29                           1.0  \n",
      "...                          ...  \n",
      "1682                        -1.0  \n",
      "1683                        -1.0  \n",
      "1684                        -1.0  \n",
      "1685                         2.0  \n",
      "1686                        -1.0  \n",
      "1687                        -1.0  \n",
      "1688                        -1.0  \n",
      "1689                        -1.0  \n",
      "1690                        -1.0  \n",
      "1691                        -1.0  \n",
      "1692                        -1.0  \n",
      "1693                        -1.0  \n",
      "1694                        -1.0  \n",
      "1695                        -1.0  \n",
      "1696                        -1.0  \n",
      "1697                        -1.0  \n",
      "1698                        -1.0  \n",
      "1699                        -1.0  \n",
      "1700                        -1.0  \n",
      "1701                        -1.0  \n",
      "1702                        -1.0  \n",
      "1703                        -1.0  \n",
      "1704                        -1.0  \n",
      "1705                        -1.0  \n",
      "1706                        -1.0  \n",
      "1707                        -1.0  \n",
      "1708                        -1.0  \n",
      "1709                        -1.0  \n",
      "1710                        -1.0  \n",
      "1711                        -1.0  \n",
      "\n",
      "[1712 rows x 168 columns]\n",
      "Index([u'user_id', u'gender_yr4', u'citizen_yr4', u'workhours_yr4',\n",
      "       u'tradeexp_yr4', u'tradeenjoy_yr4', u'newshrs_yr4', u'raven_score_yr4',\n",
      "       u'crt_score_yr4', u'crt_1_cor_yr4',\n",
      "       ...\n",
      "       u'motivation_2', u'motivation_3', u'motivation_4', u'motivation_5',\n",
      "       u'motivation_6', u'motivation_7', u'motivation_8',\n",
      "       u'motivation_intrinsic_score', u'motivation_prosocial_score',\n",
      "       u'motivation_extrinsic_score'],\n",
      "      dtype='object', length=168)\n"
     ]
    }
   ],
   "source": [
    "first_import = pd.read_csv('./clean_frame.csv',delimiter='\\t', low_memory=False)\n",
    "first_import = first_import.drop(['Unnamed: 0'],axis=1)\n",
    "print first_import\n",
    "print first_import.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         user_id_x  gender_yr4  citizen_yr4  workhours_yr4  tradeexp_yr4  \\\n",
      "0              2.0         1.0          1.0            4.0           2.0   \n",
      "1              2.0         1.0          1.0            4.0           2.0   \n",
      "2              2.0         1.0          1.0            4.0           2.0   \n",
      "3              2.0         1.0          1.0            4.0           2.0   \n",
      "4              2.0         1.0          1.0            4.0           2.0   \n",
      "5              2.0         1.0          1.0            4.0           2.0   \n",
      "6              2.0         1.0          1.0            4.0           2.0   \n",
      "7              2.0         1.0          1.0            4.0           2.0   \n",
      "8              2.0         1.0          1.0            4.0           2.0   \n",
      "9              2.0         1.0          1.0            4.0           2.0   \n",
      "10             2.0         1.0          1.0            4.0           2.0   \n",
      "11             2.0         1.0          1.0            4.0           2.0   \n",
      "12             2.0         1.0          1.0            4.0           2.0   \n",
      "13             2.0         1.0          1.0            4.0           2.0   \n",
      "14             2.0         1.0          1.0            4.0           2.0   \n",
      "15             2.0         1.0          1.0            4.0           2.0   \n",
      "16             2.0         1.0          1.0            4.0           2.0   \n",
      "17             2.0         1.0          1.0            4.0           2.0   \n",
      "18             2.0         1.0          1.0            4.0           2.0   \n",
      "19             2.0         1.0          1.0            4.0           2.0   \n",
      "20             2.0         1.0          1.0            4.0           2.0   \n",
      "21             2.0         1.0          1.0            4.0           2.0   \n",
      "22             2.0         1.0          1.0            4.0           2.0   \n",
      "23             2.0         1.0          1.0            4.0           2.0   \n",
      "24             2.0         1.0          1.0            4.0           2.0   \n",
      "25             2.0         1.0          1.0            4.0           2.0   \n",
      "26             2.0         1.0          1.0            4.0           2.0   \n",
      "27             2.0         1.0          1.0            4.0           2.0   \n",
      "28             2.0         1.0          1.0            4.0           2.0   \n",
      "29             2.0         1.0          1.0            4.0           2.0   \n",
      "...            ...         ...          ...            ...           ...   \n",
      "1259122     2051.0         1.0         -1.0           -1.0          -1.0   \n",
      "1259123     2051.0         1.0         -1.0           -1.0          -1.0   \n",
      "1259124     2051.0         1.0         -1.0           -1.0          -1.0   \n",
      "1259125     2051.0         1.0         -1.0           -1.0          -1.0   \n",
      "1259126     2051.0         1.0         -1.0           -1.0          -1.0   \n",
      "1259127     2051.0         1.0         -1.0           -1.0          -1.0   \n",
      "1259128     2051.0         1.0         -1.0           -1.0          -1.0   \n",
      "1259129     2051.0         1.0         -1.0           -1.0          -1.0   \n",
      "1259130     2051.0         1.0         -1.0           -1.0          -1.0   \n",
      "1259131     2051.0         1.0         -1.0           -1.0          -1.0   \n",
      "1259132     2051.0         1.0         -1.0           -1.0          -1.0   \n",
      "1259133     2051.0         1.0         -1.0           -1.0          -1.0   \n",
      "1259134     2051.0         1.0         -1.0           -1.0          -1.0   \n",
      "1259135     2051.0         1.0         -1.0           -1.0          -1.0   \n",
      "1259136     2051.0         1.0         -1.0           -1.0          -1.0   \n",
      "1259137     2051.0         1.0         -1.0           -1.0          -1.0   \n",
      "1259138     2051.0         1.0         -1.0           -1.0          -1.0   \n",
      "1259139     2051.0         1.0         -1.0           -1.0          -1.0   \n",
      "1259140     2051.0         1.0         -1.0           -1.0          -1.0   \n",
      "1259141     2051.0         1.0         -1.0           -1.0          -1.0   \n",
      "1259142     2051.0         1.0         -1.0           -1.0          -1.0   \n",
      "1259143     2051.0         1.0         -1.0           -1.0          -1.0   \n",
      "1259144     2051.0         1.0         -1.0           -1.0          -1.0   \n",
      "1259145     2051.0         1.0         -1.0           -1.0          -1.0   \n",
      "1259146     2051.0         1.0         -1.0           -1.0          -1.0   \n",
      "1259147     2051.0         1.0         -1.0           -1.0          -1.0   \n",
      "1259148     2051.0         1.0         -1.0           -1.0          -1.0   \n",
      "1259149     2051.0         1.0         -1.0           -1.0          -1.0   \n",
      "1259150     2051.0         1.0         -1.0           -1.0          -1.0   \n",
      "1259151     2051.0         1.0         -1.0           -1.0          -1.0   \n",
      "\n",
      "         tradeenjoy_yr4  newshrs_yr4  raven_score_yr4  crt_score_yr4  \\\n",
      "0                   4.0          3.0              6.0            9.0   \n",
      "1                   4.0          3.0              6.0            9.0   \n",
      "2                   4.0          3.0              6.0            9.0   \n",
      "3                   4.0          3.0              6.0            9.0   \n",
      "4                   4.0          3.0              6.0            9.0   \n",
      "5                   4.0          3.0              6.0            9.0   \n",
      "6                   4.0          3.0              6.0            9.0   \n",
      "7                   4.0          3.0              6.0            9.0   \n",
      "8                   4.0          3.0              6.0            9.0   \n",
      "9                   4.0          3.0              6.0            9.0   \n",
      "10                  4.0          3.0              6.0            9.0   \n",
      "11                  4.0          3.0              6.0            9.0   \n",
      "12                  4.0          3.0              6.0            9.0   \n",
      "13                  4.0          3.0              6.0            9.0   \n",
      "14                  4.0          3.0              6.0            9.0   \n",
      "15                  4.0          3.0              6.0            9.0   \n",
      "16                  4.0          3.0              6.0            9.0   \n",
      "17                  4.0          3.0              6.0            9.0   \n",
      "18                  4.0          3.0              6.0            9.0   \n",
      "19                  4.0          3.0              6.0            9.0   \n",
      "20                  4.0          3.0              6.0            9.0   \n",
      "21                  4.0          3.0              6.0            9.0   \n",
      "22                  4.0          3.0              6.0            9.0   \n",
      "23                  4.0          3.0              6.0            9.0   \n",
      "24                  4.0          3.0              6.0            9.0   \n",
      "25                  4.0          3.0              6.0            9.0   \n",
      "26                  4.0          3.0              6.0            9.0   \n",
      "27                  4.0          3.0              6.0            9.0   \n",
      "28                  4.0          3.0              6.0            9.0   \n",
      "29                  4.0          3.0              6.0            9.0   \n",
      "...                 ...          ...              ...            ...   \n",
      "1259122            -1.0          5.0             -1.0           -1.0   \n",
      "1259123            -1.0          5.0             -1.0           -1.0   \n",
      "1259124            -1.0          5.0             -1.0           -1.0   \n",
      "1259125            -1.0          5.0             -1.0           -1.0   \n",
      "1259126            -1.0          5.0             -1.0           -1.0   \n",
      "1259127            -1.0          5.0             -1.0           -1.0   \n",
      "1259128            -1.0          5.0             -1.0           -1.0   \n",
      "1259129            -1.0          5.0             -1.0           -1.0   \n",
      "1259130            -1.0          5.0             -1.0           -1.0   \n",
      "1259131            -1.0          5.0             -1.0           -1.0   \n",
      "1259132            -1.0          5.0             -1.0           -1.0   \n",
      "1259133            -1.0          5.0             -1.0           -1.0   \n",
      "1259134            -1.0          5.0             -1.0           -1.0   \n",
      "1259135            -1.0          5.0             -1.0           -1.0   \n",
      "1259136            -1.0          5.0             -1.0           -1.0   \n",
      "1259137            -1.0          5.0             -1.0           -1.0   \n",
      "1259138            -1.0          5.0             -1.0           -1.0   \n",
      "1259139            -1.0          5.0             -1.0           -1.0   \n",
      "1259140            -1.0          5.0             -1.0           -1.0   \n",
      "1259141            -1.0          5.0             -1.0           -1.0   \n",
      "1259142            -1.0          5.0             -1.0           -1.0   \n",
      "1259143            -1.0          5.0             -1.0           -1.0   \n",
      "1259144            -1.0          5.0             -1.0           -1.0   \n",
      "1259145            -1.0          5.0             -1.0           -1.0   \n",
      "1259146            -1.0          5.0             -1.0           -1.0   \n",
      "1259147            -1.0          5.0             -1.0           -1.0   \n",
      "1259148            -1.0          5.0             -1.0           -1.0   \n",
      "1259149            -1.0          5.0             -1.0           -1.0   \n",
      "1259150            -1.0          5.0             -1.0           -1.0   \n",
      "1259151            -1.0          5.0             -1.0           -1.0   \n",
      "\n",
      "         crt_1_cor_yr4  ...   training  team  user_id_y  value  fcast_year  \\\n",
      "0                  1.0  ...          h  37.0         63  -0.96        2015   \n",
      "1                  1.0  ...          h  37.0         63  -0.96        2015   \n",
      "2                  1.0  ...          h  37.0         63  -1.00        2015   \n",
      "3                  1.0  ...          h  37.0         63  -1.00        2015   \n",
      "4                  1.0  ...          h  37.0         63  -1.00        2015   \n",
      "5                  1.0  ...          h  37.0         63  -1.00        2015   \n",
      "6                  1.0  ...          h  37.0         63  -1.00        2015   \n",
      "7                  1.0  ...          h  37.0         63  -1.00        2015   \n",
      "8                  1.0  ...          h  37.0         63  -0.50        2014   \n",
      "9                  1.0  ...          h  37.0         63  -0.50        2014   \n",
      "10                 1.0  ...          h  37.0         63  -0.50        2014   \n",
      "11                 1.0  ...          h  37.0         63  -0.50        2014   \n",
      "12                 1.0  ...          h  37.0         63  -0.50        2014   \n",
      "13                 1.0  ...          h  37.0         63  -0.84        2014   \n",
      "14                 1.0  ...          h  37.0         63  -0.84        2014   \n",
      "15                 1.0  ...          h  37.0         63  -0.84        2014   \n",
      "16                 1.0  ...          h  37.0         63  -0.84        2014   \n",
      "17                 1.0  ...          h  37.0         63  -0.84        2014   \n",
      "18                 1.0  ...          h  37.0         63  -0.84        2014   \n",
      "19                 1.0  ...          h  37.0         63  -0.84        2014   \n",
      "20                 1.0  ...          h  37.0         63  -0.84        2014   \n",
      "21                 1.0  ...          h  37.0         63  -0.96        2014   \n",
      "22                 1.0  ...          h  37.0         63  -0.96        2014   \n",
      "23                 1.0  ...          h  37.0         63  -0.96        2014   \n",
      "24                 1.0  ...          h  37.0         63  -0.96        2014   \n",
      "25                 1.0  ...          h  37.0         63  -0.96        2014   \n",
      "26                 1.0  ...          h  37.0         63   0.30        2014   \n",
      "27                 1.0  ...          h  37.0         63   0.30        2014   \n",
      "28                 1.0  ...          h  37.0         63   0.30        2014   \n",
      "29                 1.0  ...          h  37.0         63   0.30        2014   \n",
      "...                ...  ...        ...   ...        ...    ...         ...   \n",
      "1259122           -1.0  ...          q   NaN     145836   0.60        2015   \n",
      "1259123           -1.0  ...          q   NaN     145836   0.60        2015   \n",
      "1259124           -1.0  ...          q   NaN     145836   0.60        2015   \n",
      "1259125           -1.0  ...          q   NaN     145836   0.60        2015   \n",
      "1259126           -1.0  ...          q   NaN     145836   0.60        2015   \n",
      "1259127           -1.0  ...          q   NaN     145836   0.60        2015   \n",
      "1259128           -1.0  ...          q   NaN     145836   0.60        2015   \n",
      "1259129           -1.0  ...          q   NaN     145836   0.60        2015   \n",
      "1259130           -1.0  ...          q   NaN     145836   0.60        2015   \n",
      "1259131           -1.0  ...          q   NaN     145836   0.60        2015   \n",
      "1259132           -1.0  ...          q   NaN     145836   0.60        2015   \n",
      "1259133           -1.0  ...          q   NaN     145836   0.60        2015   \n",
      "1259134           -1.0  ...          q   NaN     145836   0.60        2015   \n",
      "1259135           -1.0  ...          q   NaN     145836   0.60        2015   \n",
      "1259136           -1.0  ...          q   NaN     145836  -0.40        2015   \n",
      "1259137           -1.0  ...          q   NaN     145836  -0.40        2015   \n",
      "1259138           -1.0  ...          q   NaN     145836  -0.40        2015   \n",
      "1259139           -1.0  ...          q   NaN     145836  -0.40        2015   \n",
      "1259140           -1.0  ...          q   NaN     145836  -0.40        2015   \n",
      "1259141           -1.0  ...          q   NaN     145836  -0.40        2015   \n",
      "1259142           -1.0  ...          q   NaN     145836  -0.40        2015   \n",
      "1259143           -1.0  ...          q   NaN     145836  -0.40        2015   \n",
      "1259144           -1.0  ...          q   NaN     145836  -0.40        2015   \n",
      "1259145           -1.0  ...          q   NaN     145836  -0.40        2015   \n",
      "1259146           -1.0  ...          q   NaN     145836  -0.40        2015   \n",
      "1259147           -1.0  ...          q   NaN     145836  -0.40        2015   \n",
      "1259148           -1.0  ...          q   NaN     145836  -0.40        2015   \n",
      "1259149           -1.0  ...          q   NaN     145836  -0.40        2015   \n",
      "1259150           -1.0  ...          q   NaN     145836  -0.40        2015   \n",
      "1259151           -1.0  ...          q   NaN     145836  -0.40        2015   \n",
      "\n",
      "         fcast_week      ifp_week   bin  task_id   uid  \n",
      "0                 3   201531244-0  0.02        0     2  \n",
      "1                 4   201541244-0  0.02        1     2  \n",
      "2                 4   201541244-0  0.00        1     2  \n",
      "3                 5   201551244-0  0.00        2     2  \n",
      "4                 6   201561244-0  0.00        3     2  \n",
      "5                 7   201571244-0  0.00        4     2  \n",
      "6                 8   201581244-0  0.00        5     2  \n",
      "7                 9   201591244-0  0.00        6     2  \n",
      "8                49  2014491394-0  0.25       14     2  \n",
      "9                50  2014501394-0  0.25       15     2  \n",
      "10               51  2014511394-0  0.25       16     2  \n",
      "11               52  2014521394-0  0.25       17     2  \n",
      "12                1   201411394-0  0.25       18     2  \n",
      "13               42  2014421399-2  0.08      328     2  \n",
      "14               43  2014431399-2  0.08      329     2  \n",
      "15               44  2014441399-2  0.08      330     2  \n",
      "16               45  2014451399-2  0.08      331     2  \n",
      "17               46  2014461399-2  0.08      332     2  \n",
      "18               47  2014471399-2  0.08      333     2  \n",
      "19               48  2014481399-2  0.08      334     2  \n",
      "20               49  2014491399-2  0.08      335     2  \n",
      "21               49  2014491399-2  0.02      335     2  \n",
      "22               50  2014501399-2  0.02      336     2  \n",
      "23               51  2014511399-2  0.02      337     2  \n",
      "24               52  2014521399-2  0.02      338     2  \n",
      "25                1   201411399-2  0.02      339     2  \n",
      "26               35  2014351409-0  0.65      340     2  \n",
      "27               36  2014361409-0  0.65      341     2  \n",
      "28               37  2014371409-0  0.65      342     2  \n",
      "29               38  2014381409-0  0.65      343     2  \n",
      "...             ...           ...   ...      ...   ...  \n",
      "1259122           9   201591515-0  0.80     1530  2051  \n",
      "1259123          10  2015101515-0  0.80     1286  2051  \n",
      "1259124          11  2015111515-0  0.80     1287  2051  \n",
      "1259125          12  2015121515-0  0.80     1288  2051  \n",
      "1259126          13  2015131515-0  0.80     1289  2051  \n",
      "1259127          14  2015141515-0  0.80     1290  2051  \n",
      "1259128          15  2015151515-0  0.80     1291  2051  \n",
      "1259129          16  2015161515-0  0.80     1292  2051  \n",
      "1259130          17  2015171515-0  0.80     1293  2051  \n",
      "1259131          18  2015181515-0  0.80     1294  2051  \n",
      "1259132          19  2015191515-0  0.80     1295  2051  \n",
      "1259133          20  2015201515-0  0.80      242  2051  \n",
      "1259134          21  2015211515-0  0.80      243  2051  \n",
      "1259135          22  2015221515-0  0.80      244  2051  \n",
      "1259136           9   201596413-0  0.30     1426  2051  \n",
      "1259137          10  2015106413-0  0.30     1427  2051  \n",
      "1259138          11  2015116413-0  0.30     1428  2051  \n",
      "1259139          12  2015126413-0  0.30     1429  2051  \n",
      "1259140          13  2015136413-0  0.30     1430  2051  \n",
      "1259141          14  2015146413-0  0.30     1431  2051  \n",
      "1259142          15  2015156413-0  0.30     1432  2051  \n",
      "1259143          16  2015166413-0  0.30     1433  2051  \n",
      "1259144          17  2015176413-0  0.30     1434  2051  \n",
      "1259145          18  2015186413-0  0.30     1435  2051  \n",
      "1259146          19  2015196413-0  0.30     1436  2051  \n",
      "1259147          20  2015206413-0  0.30     1437  2051  \n",
      "1259148          21  2015216413-0  0.30     1438  2051  \n",
      "1259149          22  2015226413-0  0.30     1439  2051  \n",
      "1259150          23  2015236413-0  0.30     1440  2051  \n",
      "1259151          24  2015246413-0  0.30     1441  2051  \n",
      "\n",
      "[1259152 rows x 181 columns]\n"
     ]
    }
   ],
   "source": [
    "joined_data = pd.merge(first_import,testframe,right_on='uid',left_on='user_id')\n",
    "print joined_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1259152, 181)\n",
      "(1499294, 13)\n",
      "(1712, 168)\n"
     ]
    }
   ],
   "source": [
    "print joined_data.shape\n",
    "print testframe.shape\n",
    "print first_import.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['uid', 'task_id', 'value', 'bin', 'user_id', 'gender_yr4', 'citizen_yr4', 'workhours_yr4', 'tradeexp_yr4', 'tradeenjoy_yr4', 'newshrs_yr4', 'raven_score_yr4', 'crt_score_yr4', 'crt_1_cor_yr4', 'crt_2_cor_yr4', 'crt_3_cor_yr4', 'crt_4_cor_yr4', 'crt_5_cor_yr4', 'crt_6_cor_yr4', 'crt_7_cor_yr4', 'crt_8_cor_yr4', 'crt_9_cor_yr4', 'crt_10_cor_yr4', 'crt_11_cor_yr4', 'crt_12_cor_yr4', 'crt_13_cor_yr4', 'crt_14_cor_yr4', 'crt_15_cor_yr4', 'crt_16_cor_yr4', 'crt_17_cor_yr4', 'crt_18_cor_yr4', 'numeracy_score_yr4', 'cpk_correct_1', 'cpk_correct_2', 'cpk_correct_3', 'cpk_correct_4', 'cpk_correct_5', 'cpk_correct_6', 'cpk_correct_7', 'cpk_correct_8', 'cpk_correct_9', 'cpk_correct_10', 'cpk_correct_11', 'cpk_correct_12', 'cpk_correct_13', 'cpk_correct_14', 'cpk_correct_15', 'cpk_correct_16', 'cpk_correct_17', 'cpk_correct_18', 'cpk_correct_19', 'cpk_correct_20', 'cpk_correct_21', 'cpk_correct_22', 'cpk_correct_23', 'cpk_correct_24', 'cpk_correct_25', 'cpk_correct_26', 'cpk_correct_27', 'cpk_correct_28', 'cpk_correct_29', 'cpk_correct_30', 'cpk_correct_31', 'cpk_correct_32', 'cpk_score_china', 'cpk_score_globecon', 'cpk_score_iran', 'cpk_score_russia', 'gpk_1', 'gpk_2', 'gpk_3', 'gpk_4', 'gpk_5', 'gpk_6', 'gpk_7', 'gpk_8', 'gpk_9', 'gpk_10', 'gpk_11', 'gpk_12', 'gpk_13', 'gpk_correct_1', 'gpk_correct_2', 'gpk_correct_3', 'gpk_correct_4', 'gpk_correct_5', 'gpk_correct_6', 'gpk_correct_7', 'gpk_correct_8', 'gpk_correct_9', 'gpk_correct_10', 'gpk_correct_11', 'gpk_correct_12', 'gpk_correct_13', 'gpk_score', 'aomt_1_yr4', 'aomt_2_yr4', 'aomt_3_yr4', 'aomt_4_yr4', 'aomt_5_yr4', 'aomt_6_yr4', 'aomt_7_yr4', 'aomt_8_yr4', 'aomt_9_yr4', 'aomt_score_yr4', 'cultwv_ind_score_yr4', 'cultwv_hier_score_yr4', 'cultwv_ind_1_yr4', 'cultwv_ind_2_yr4', 'cultwv_ind_3_yr4', 'cultwv_ind_4_yr4', 'cultwv_ind_5_yr4', 'cultwv_ind_6_yr4', 'cultwv_hier_7_yr4', 'cultwv_hier_8_yr4', 'cultwv_hier_9_yr4', 'cultwv_hier_10_yr4', 'cultwv_hier_11_yr4', 'cultwv_hier_12_yr4', 'nfcog_1_yr4', 'nfcog_2_yr4', 'nfcog_3_yr4', 'nfcog_4_yr4', 'nfcog_5_yr4', 'nfcog_6_yr4', 'nfcog_7_yr4', 'nfcog_8_yr4', 'nfcog_9_yr4', 'nfcog_10_yr4', 'nfcog_11_yr4', 'nfcog_12_yr4', 'nfcog_13_yr4', 'nfcog_14_yr4', 'nfcog_15_yr4', 'nfcog_16_yr4', 'nfcog_17_yr4', 'nfcog_18_yr4', 'nfcog_score_yr4', 'memory_math_correct_1', 'memory_math_correct_2', 'memory_math_correct_3', 'memory_math_correct_4', 'memory_math_correct_5', 'memory_math_correct_6', 'memory_word_correct_1', 'memory_word_correct_2', 'memory_word_correct_3', 'memory_word_correct_4', 'memory_word_correct_5', 'memory_word_correct_6', 'memory_math_score', 'memory_word_score', 'helping_1', 'helping_2', 'helping_3', 'helping_score', 'orgcommit_1', 'orgcommit_2', 'orgcommit_3', 'orgcommit_4', 'orgcommit_5', 'motivation_1', 'motivation_2', 'motivation_3', 'motivation_4', 'motivation_5', 'motivation_6', 'motivation_7', 'motivation_8', 'motivation_intrinsic_score', 'motivation_prosocial_score', 'motivation_extrinsic_score']\n",
      "172\n"
     ]
    }
   ],
   "source": [
    "tot_feats = ['uid','task_id','value','bin']\n",
    "tot_feats = tot_feats + list(first_import.columns.values)\n",
    "print tot_feats\n",
    "print len(tot_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1259152, 171)\n"
     ]
    }
   ],
   "source": [
    "mask = joined_data.columns.isin(tot_feats)\n",
    "cleanframe = joined_data[joined_data.columns[mask]]\n",
    "print(cleanframe.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'gender_yr4', u'citizen_yr4', u'workhours_yr4', u'tradeexp_yr4',\n",
      "       u'tradeenjoy_yr4', u'newshrs_yr4', u'raven_score_yr4', u'crt_score_yr4',\n",
      "       u'crt_1_cor_yr4', u'crt_2_cor_yr4',\n",
      "       ...\n",
      "       u'motivation_3', u'motivation_4', u'motivation_5', u'motivation_6',\n",
      "       u'motivation_7', u'motivation_8', u'motivation_intrinsic_score',\n",
      "       u'motivation_prosocial_score', u'motivation_extrinsic_score', u'uid'],\n",
      "      dtype='object', length=168)\n"
     ]
    }
   ],
   "source": [
    "no_task = ['uid'] + list(first_import.columns.values)\n",
    "mask_no_task = cleanframe.columns.isin(no_task)\n",
    "small_frame = cleanframe[cleanframe.columns[mask_no_task]]\n",
    "print small_frame.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1711, 168)\n"
     ]
    }
   ],
   "source": [
    "small_frame_unique = small_frame.drop_duplicates()\n",
    "print small_frame_unique.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Vanilla_MF(nn.Module):\n",
    "    def __init__(self, users, tasks, num_user_features, k=4):\n",
    "        super(Vanilla_MF, self).__init__()\n",
    "        self.user_lv = nn.Embedding(users, k)\n",
    "        self.task_lv = nn.Embedding(tasks, k)\n",
    "\n",
    "        self.users_bias = nn.Embedding(users, 1)\n",
    "        self.users_add_bias = nn.Embedding(users, 1)\n",
    "        self.task_bias = nn.Embedding(tasks, 1)\n",
    "        self.global_bias = nn.Parameter(torch.FloatTensor(1))\n",
    "        \n",
    "    def forward(self, users, users_features,jokes):\n",
    "        user_vectors = self.user_lv(users)\n",
    "        task_vectors = self.task_lv(jokes)\n",
    "        user_bias = self.users_bias(users)\n",
    "        task_bias = self.task_bias(jokes)\n",
    "        user_add_bias = self.users_add_bias(users)\n",
    "        \n",
    "        preds = torch.bmm(user_vectors.unsqueeze(1), task_vectors.unsqueeze(2)).squeeze() \\\n",
    "                          + task_bias.squeeze() + self.global_bias.expand_as(user_bias.squeeze())\n",
    "        m = nn.Tanh()\n",
    "#         return m((user_bias.squeeze() * (preds - .5)) + .5)\n",
    "#         return m(user_bias.squeeze() * preds )\n",
    "        return m(user_bias.squeeze() * preds + user_add_bias.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Model_LightFM_Vanilla_Psych_With_Bias(nn.Module):\n",
    "    def __init__(self, users, tasks, num_user_features, k=4):\n",
    "        super(Model_LightFM_Vanilla_Psych_With_Bias, self).__init__()\n",
    "        self.user_feature_lv = nn.Embedding(num_user_features, k)\n",
    "        self.user_lv = nn.Embedding(users, k)\n",
    "        self.task_lv = nn.Embedding(tasks, k)\n",
    "\n",
    "        self.num_user_features_bias = nn.Embedding(num_user_features, 1)\n",
    "        self.num_user_features_add_bias = nn.Embedding(num_user_features, 1)\n",
    "        self.users_bias = nn.Embedding(users, 1)\n",
    "        self.users_add_bias = nn.Embedding(users, 1)\n",
    "        self.task_bias = nn.Embedding(tasks, 1)\n",
    "        self.global_bias = nn.Parameter(torch.FloatTensor(1))\n",
    "        \n",
    "    def forward(self, users, users_features,jokes):\n",
    "        user_vectors = self.user_lv(users)\n",
    "        user_feature_vectors = torch.mm(users_features,self.user_feature_lv.weight)\n",
    "        user_finals = torch.add(user_vectors,user_feature_vectors)\n",
    "        task_vectors = self.task_lv(jokes)\n",
    "        user_bias = self.users_bias(users)\n",
    "        user_features_bias = torch.mm(users_features, self.num_user_features_bias.weight)\n",
    "        task_bias = self.task_bias(jokes)\n",
    "        user_add_bias = self.users_add_bias(users)\n",
    "        user_features_add_bias = torch.mm(users_features,self.num_user_features_add_bias.weight)\n",
    "        \n",
    "        preds = torch.bmm(user_vectors.unsqueeze(1), task_vectors.unsqueeze(2)).squeeze() \\\n",
    "                          + task_bias.squeeze() + self.global_bias.expand_as(user_bias.squeeze())\n",
    "        m = nn.Tanh()\n",
    "#         return m((user_bias.squeeze() * (preds - .5)) + .5)\n",
    "#         return m(user_bias.squeeze() * preds )\n",
    "        return m((user_bias * user_features_bias).squeeze() * preds + (user_add_bias.squeeze() + user_features_add_bias.squeeze()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Model_LightFM_Psych_First(nn.Module):\n",
    "    def __init__(self, tasks, num_user_features, k=4):\n",
    "        super(Model_LightFM_Psych_First, self).__init__()\n",
    "        self.user_feature_lv = nn.Embedding(num_user_features, k)\n",
    "        self.task_lv = nn.Embedding(tasks, k)\n",
    "        self.mul_bias = nn.Parameter(torch.FloatTensor(1))\n",
    "        self.num_user_features_add_bias = nn.Embedding(num_user_features, 1)\n",
    "        self.task_bias = nn.Embedding(tasks, 1)\n",
    "        self.global_bias = nn.Parameter(torch.FloatTensor(1))\n",
    "        \n",
    "    def forward(self, users, users_features,jokes):\n",
    "        user_feature_vectors = torch.mm(users_features,self.user_feature_lv.weight)\n",
    "        task_vectors = self.task_lv(jokes)\n",
    "        user_features_add_bias = torch.mm(users_features, self.num_user_features_add_bias.weight)\n",
    "        task_bias = self.task_bias(jokes)\n",
    "        \n",
    "        preds = torch.bmm(user_feature_vectors.unsqueeze(1), task_vectors.unsqueeze(2)).squeeze() \\\n",
    "                          + task_bias.squeeze() + self.global_bias.expand_as(task_bias.squeeze())\n",
    "        m = nn.Tanh()\n",
    "#         return m((user_bias.squeeze() * (preds - .5)) + .5)\n",
    "#         return m(user_bias.squeeze() * preds )\n",
    "        return m(self.mul_bias * preds + user_features_add_bias.squeeze())\n",
    "\n",
    "class Model_LightFM_Users_Psych_Given(nn.Module):\n",
    "    def __init__(self, users, tasks, feature_lv, feature_add_bias, task1_lv, task_bias1, global_bias1, k=4):\n",
    "        super(Model_LightFM_Users_Psych_Given, self).__init__()\n",
    "        self.user_lv = nn.Embedding(users, k)\n",
    "        self.task2_lv = nn.Embedding(tasks, k)\n",
    "        self.user_feature_lv = Variable(feature_lv.weight.data, requires_grad=False)\n",
    "        self.task1_lv = nn.Embedding(tasks, k)\n",
    "        self.task1_lv.weight = task1_lv.weight\n",
    "        self.task1_lv.weight.requires_grad=False\n",
    "        self.task_bias1 = nn.Embedding(tasks, 1)\n",
    "        self.task_bias1.weight = task_bias1.weight\n",
    "        self.task_bias1.weight.requires_grad=False\n",
    "        self.global_bias1 = Variable(global_bias1.data, requires_grad=False)\n",
    "\n",
    "        self.num_user_features_add_bias = Variable(feature_add_bias.weight.data, requires_grad=False)\n",
    "        self.users_bias = nn.Embedding(users, 1)\n",
    "        self.users_add_bias = nn.Embedding(users, 1)\n",
    "        self.task_bias2 = nn.Embedding(tasks, 1)\n",
    "        self.global_bias2 = nn.Parameter(torch.FloatTensor(1))\n",
    "        \n",
    "    def forward(self, users, users_features,jokes):\n",
    "        user_vectors = self.user_lv(users)\n",
    "        user_feature_vectors = torch.mm(users_features,self.user_feature_lv)\n",
    "        user_finals = torch.add(user_vectors,user_feature_vectors)\n",
    "        task_vectors1 = self.task1_lv(jokes)\n",
    "        task_vectors2 = self.task2_lv(jokes)\n",
    "        task_vectors = task_vectors1 + task_vectors2\n",
    "        \n",
    "        user_bias = self.users_bias(users)\n",
    "        task_bias1 = self.task_bias1(jokes)\n",
    "        task_bias2 = self.task_bias2(jokes)\n",
    "        task_bias = task_bias1 + task_bias2\n",
    "        user_add_bias = self.users_add_bias(users)\n",
    "        global_bias = self.global_bias1 + self.global_bias2\n",
    "\n",
    "        preds = torch.bmm(user_finals.unsqueeze(1), task_vectors.unsqueeze(2)).squeeze() \\\n",
    "                          + task_bias.squeeze() + global_bias.expand_as(user_bias.squeeze())\n",
    "        m = nn.Tanh()\n",
    "#         return m((user_bias.squeeze() * (preds - .5)) + .5)\n",
    "#         return m(user_bias.squeeze() * preds )\n",
    "        return m(user_bias.squeeze() * preds + user_add_bias.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batcher(seq, size):\n",
    "    return (seq[pos:pos + size] for pos in range(0, len(seq), size))\n",
    "\n",
    "def split(df):\n",
    "    train_df, validate_df, test_df = np.split(df.sample(frac=1), [int(.6*len(df)), int(.8*len(df))])\n",
    "    return train_df, validate_df, test_df\n",
    "\n",
    "def val(df, model):\n",
    "    crit = nn.MSELoss(size_average=False)\n",
    "    crit2 = nn.L1Loss(size_average=False)\n",
    "    total_loss = 0.\n",
    "    total_l1 = 0.\n",
    "    total_num = 0\n",
    "    for batch in batcher(df, 100):\n",
    "        true_rating = Variable(torch.Tensor(batch.bin.values.astype(float)))\n",
    "        total_num = total_num + true_rating.size(0)\n",
    "        users = Variable(torch.LongTensor(batch.uid.values))\n",
    "        tasks = Variable(torch.LongTensor(batch.task_id.values))\n",
    "        users_features = Variable(torch.FloatTensor(batch.values[:,:167]))\n",
    "#         print(users, tasks)\n",
    "        scores = model.forward(users, users_features,tasks)\n",
    "        total_loss += crit(scores, true_rating).data[0]\n",
    "        total_l1 += crit2(scores,true_rating).data[0]\n",
    "    return math.sqrt(total_loss/total_num), total_l1/total_num\n",
    "\n",
    "def forward_means(user_means, jokes, task_lv,task_bias,users_bias_mean, user_add_mean, global_bias):\n",
    "    user_vectors = user_means.unsqueeze(1).repeat(jokes.data.shape[0],1).view(-1,4)\n",
    "    task_vectors = task_lv(jokes)\n",
    "    task_bias = task_bias(jokes)\n",
    "    \n",
    "    partial1 = torch.bmm(user_vectors.unsqueeze(1), task_vectors.unsqueeze(2)).squeeze()\n",
    "    preds = torch.bmm(user_vectors.unsqueeze(1), task_vectors.unsqueeze(2)).squeeze() \\\n",
    "                      + task_bias.squeeze() + global_bias.expand_as(task_bias.squeeze())\n",
    "    m = nn.Tanh()\n",
    "#         return m((user_bias.squeeze() * (preds - .5)) + .5)\n",
    "#         return m(user_bias.squeeze() * preds )\n",
    "    return m(user_bias_mean * preds + user_add_mean)\n",
    "\n",
    "def rmse_means(df, model, user_means, user_bias_mean, user_add_mean):\n",
    "    crit = nn.MSELoss(size_average=False)\n",
    "    crit2 = nn.L1Loss(size_average=False)\n",
    "    total_loss = 0.\n",
    "    total_l1 = 0.\n",
    "    total_num = 0\n",
    "    for batch in batcher(df, 100):\n",
    "        true_rating = Variable(torch.Tensor(batch.bin.values.astype(float)))\n",
    "        total_num = total_num + true_rating.size(0)\n",
    "        tasks = Variable(torch.LongTensor(batch.task_id.values))\n",
    "        scores = forward_means(user_means, tasks, model.task_lv, model.task_bias,user_bias_mean, user_add_mean, model.global_bias)\n",
    "        total_loss += crit(scores, true_rating).data[0]\n",
    "        total_l1 += crit2(scores,true_rating).data[0]\n",
    "    return math.sqrt(total_loss/total_num), total_l1/total_num\n",
    "\n",
    "def train(train_iter, val_iter, test_iter, model):\n",
    "    train_params = itertools.ifilter(lambda p: p.requires_grad, model.parameters())\n",
    "    train_param_list = [param for param in train_params]\n",
    "    opt = optim.SGD(train_param_list, lr=.3)\n",
    "    crit = nn.MSELoss()\n",
    "    crit2 = nn.L1Loss()\n",
    "\n",
    "    print(\"val:\", val(validate_df, model))\n",
    "    for epochs in range(25):\n",
    "        avg_loss = 0\n",
    "        avg_l1 = 0\n",
    "        total = 0\n",
    "        for i,batch in enumerate(batcher(train_df, 100)):\n",
    "            opt.zero_grad()\n",
    "            rating = Variable(torch.Tensor(batch.bin.values.astype(float)))\n",
    "#             print(rating)\n",
    "            users = Variable(torch.LongTensor(batch.uid.values))\n",
    "            users_features = Variable(torch.FloatTensor(batch.values[:,:167]))\n",
    "            tasks = Variable(torch.LongTensor(batch.task_id.values))\n",
    "#             print(users, tasks)\n",
    "            scores = model.forward(users, users_features, tasks) \n",
    "#             + torch.sum(model.user_lut.weight.data.pow(2)) + \\\n",
    "#             torch.sum(model.task_lut.weight.data.pow(2)) + torch.sum(model.user_bias.weight.data.pow(2)) + \\\n",
    "#             torch.sum(model.task_bias.weight.data.pow(2))\n",
    "            loss = crit(scores, rating)\n",
    "            #if i % 1000==0:\n",
    "            #    print (loss.data[0])\n",
    "            loss.backward()\n",
    "            avg_loss += loss.data[0]\n",
    "            avg_l1 += crit2(scores,rating).data[0]\n",
    "            total += 1\n",
    "            opt.step()\n",
    "        #if (epochs == 149 or epochs % 10 == 0):\n",
    "        print(\"train:\", math.sqrt(avg_loss / float(total)), avg_l1/ float(total))\n",
    "        print(\"val:\", val(validate_df, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df, validate_df, test_df = split(cleanframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2051, 1547)\n",
      "2051\n",
      "1547\n",
      "Psych No Bias\n",
      "('val:', (1.070100170663153, 0.9721523249844706))\n",
      "('train:', 1.0645471495562373, 0.96582482906464)\n",
      "('val:', (1.062147466998928, 0.9635358603049966))\n",
      "('train:', 1.0476236377648445, 0.9447403519063655)\n",
      "('val:', (1.029554260610401, 0.9220414438017811))\n",
      "('train:', 1.0183051168961805, 0.9076671302200231)\n",
      "('val:', (1.0167147189391736, 0.9066524364209528))\n",
      "('train:', 0.9947033016063781, 0.8776757787111814)\n",
      "('val:', (0.987773793453043, 0.868399685328481))\n",
      "('train:', 0.979482706534702, 0.8577445163307089)\n",
      "('val:', (0.9735740647125262, 0.8508170513799437))\n",
      "('train:', 0.964530149385859, 0.8381265134467423)\n",
      "('val:', (0.9515819398617673, 0.8201934556560019))\n",
      "('train:', 0.9515435032904801, 0.821427760962064)\n",
      "('val:', (0.9464549919473093, 0.8132837670064212))\n",
      "('train:', 0.9405816263489408, 0.8066996405883627)\n",
      "('val:', (0.9335666221418674, 0.7980643968105373))\n",
      "('train:', 0.9295923769857162, 0.7926477980329689)\n",
      "('val:', (0.9168445584694132, 0.7750413144907694))\n",
      "('train:', 0.9193848310644172, 0.7792243670423168)\n",
      "('val:', (0.9102759671634355, 0.7663772770160195))\n",
      "('train:', 0.908195143950342, 0.7640953430428874)\n",
      "('val:', (0.8987882696914913, 0.7507223689366299))\n",
      "('train:', 0.9016291261742408, 0.7555794056403882)\n",
      "('val:', (0.9027780509569964, 0.7579400560283369))\n",
      "('train:', 0.8945021724542156, 0.7466585027863851)\n",
      "('val:', (0.9075903478261973, 0.7639109055166499))\n",
      "('train:', 0.8876245950951008, 0.7375508931281155)\n",
      "('val:', (0.8851813146223365, 0.7332840008158711))\n",
      "('train:', 0.8798611593238125, 0.7276402241184884)\n",
      "('val:', (0.8741954201537062, 0.7194483927546599))\n",
      "('train:', 0.8765699317844695, 0.7235350970319373)\n",
      "('val:', (0.8746345490269801, 0.7204665109834124))\n",
      "('train:', 0.8717035713237635, 0.7171352283754229)\n",
      "('val:', (0.8670408212793371, 0.7094166802899867))\n",
      "('train:', 0.8682690027309504, 0.7130219651483527)\n",
      "('val:', (0.864466614939345, 0.7073934552781524))\n",
      "('train:', 0.8623084502070931, 0.7051213412313253)\n",
      "('val:', (0.8574358190673479, 0.6980561548876894))\n",
      "('train:', 0.8602564893209488, 0.7026641913315541)\n",
      "('val:', (0.8571671750919051, 0.6982068692473826))\n",
      "('train:', 0.8595745573156623, 0.701980655813438)\n",
      "('val:', (0.8599325863791207, 0.7017028791628095))\n",
      "('train:', 0.8558780477197211, 0.6971645094163366)\n",
      "('val:', (0.8552594911446464, 0.6967471872676675))\n",
      "('train:', 0.8529280177089691, 0.693783599150094)\n",
      "('val:', (0.8616813971399685, 0.7054782973012548))\n",
      "('train:', 0.8504087876397062, 0.690307420827473)\n",
      "('val:', (0.847974671392247, 0.6861685195993744))\n",
      "('train:', 0.850125821927141, 0.690340193835889)\n",
      "('val:', (0.8452902779189624, 0.6831828196480546))\n",
      "Psych Features Only\n",
      "('val:', (1.1062680727198682, 1.015378803001264))\n",
      "('train:', 0.823767481876811, 0.7429019149765126)\n",
      "('val:', (0.7440678391385026, 0.6803526976553237))\n",
      "('train:', 0.7443700651219627, 0.6805879419564728)\n",
      "('val:', (0.7440561681300897, 0.6803266364700747))\n",
      "('train:', 0.7443568191049321, 0.6805673664488752)\n",
      "('val:', (0.7440502184924503, 0.6803004206701452))\n",
      "('train:', 0.7443504767565029, 0.6805559248264699)\n",
      "('val:', (0.7440505126554661, 0.6802987020189822))\n",
      "('train:', 0.7443454508069011, 0.6805458744103983)\n",
      "('val:', (0.7440477857136378, 0.6802928617133787))\n",
      "('train:', 0.7443424392294659, 0.6805401582026781)\n",
      "('val:', (0.7440448588887174, 0.6802873340757108))\n",
      "('train:', 0.744340363959972, 0.6805356095517258)\n",
      "('val:', (0.7440426558326714, 0.6802830421588023))\n",
      "('train:', 0.7443389216718371, 0.6805321507466542)\n",
      "('val:', (0.7440412367533786, 0.680281861288612))\n",
      "('train:', 0.7443379609875023, 0.6805299245940385)\n",
      "('val:', (0.7440406217190975, 0.68028444936901))\n",
      "('train:', 0.7443373183224096, 0.6805282943853079)\n",
      "('val:', (0.7440409499464627, 0.6802889474186286))\n",
      "('train:', 0.7443368612580178, 0.6805270135836599)\n",
      "('val:', (0.7440423192079996, 0.6802967772048134))\n",
      "('train:', 0.7443365003553092, 0.6805259079115822)\n",
      "('val:', (0.7440445701083873, 0.6803061618175537))\n",
      "('train:', 0.744336188762943, 0.6805248898595158)\n",
      "('val:', (0.7440472562222569, 0.6803157653175411))\n",
      "('train:', 0.7443359130622837, 0.6805238479734809)\n",
      "('val:', (0.7440498400849173, 0.6803237856037967))\n",
      "('train:', 0.7443356778031167, 0.6805231355263807)\n",
      "('val:', (0.7440518920685602, 0.6803295233126303))\n",
      "('train:', 0.7443354886216544, 0.6805225055790516)\n",
      "('val:', (0.74405316614353, 0.6803328663949785))\n",
      "('train:', 0.7443353457378676, 0.6805220813407241)\n",
      "('val:', (0.7440536185529663, 0.6803340045480728))\n",
      "('train:', 0.7443352425060236, 0.6805216957290467)\n",
      "('val:', (0.7440533938039244, 0.6803334250043224))\n",
      "('train:', 0.7443351695457605, 0.6805214261866658)\n",
      "('val:', (0.744052753687662, 0.6803317814261834))\n",
      "('train:', 0.744335118385598, 0.6805211638867815)\n",
      "('val:', (0.7440519528900821, 0.6803296866676528))\n",
      "('train:', 0.7443350833337169, 0.6805208852794453)\n",
      "('val:', (0.7440511580567364, 0.6803275513581768))\n",
      "('train:', 0.744335060023208, 0.6805206317604969)\n",
      "('val:', (0.7440504463573338, 0.6803255853719984))\n",
      "('train:', 0.7443350446568736, 0.6805204335227856)\n",
      "('val:', (0.7440498422836596, 0.6803238671752726))\n",
      "('train:', 0.7443350350194603, 0.6805202753081533)\n",
      "('val:', (0.7440493477089802, 0.6803224401667514))\n",
      "('train:', 0.7443350286121836, 0.680520161329556)\n",
      "('val:', (0.7440489542660679, 0.6803212724600917))\n",
      "Psych Full\n",
      "('val:', (1.066390738871396, 0.9680933027892686))\n",
      "('train:', 1.0560699786522159, 0.9545005738222386)\n",
      "('val:', (1.0494338337424693, 0.9464719779534123))\n",
      "('train:', 1.0430261065444426, 0.9384894504206296)\n",
      "('val:', (1.037047955154434, 0.9312128715952911))\n",
      "('train:', 1.0321720408791268, 0.9253975122989526)\n",
      "('val:', (1.0292070549118018, 0.921658674324259))\n",
      "('train:', 1.024814843467115, 0.9167658229366507)\n",
      "('val:', (1.0195671589308986, 0.9110264596021502))\n",
      "('train:', 1.0172132576776065, 0.9078248149586544)\n",
      "('val:', (1.014191876271935, 0.9046866638128456))\n",
      "('train:', 1.0095486132295999, 0.8987876066697662)\n",
      "('val:', (1.007364555541413, 0.896536142667122))\n",
      "('train:', 1.000031467084823, 0.8876493654528806)\n",
      "('val:', (0.9928057599188574, 0.8797504811850618))\n",
      "('train:', 0.991336872905114, 0.8773298739440232)\n",
      "('val:', (0.9913543534906606, 0.8780803677011643))\n",
      "('train:', 0.9834296676606109, 0.8683713320941029)\n",
      "('val:', (0.9802573454119868, 0.8651435271583009))\n",
      "('train:', 0.9711791681912783, 0.8544707970063786)\n",
      "('val:', (0.9699994449386368, 0.8529454346424166))\n",
      "('train:', 0.9646447227474385, 0.8466350022962281)\n",
      "('val:', (0.961492663193062, 0.8432362100846366))\n",
      "('train:', 0.9553131405032602, 0.8360237407652769)\n",
      "('val:', (0.9528865609380645, 0.8332799870826895))\n",
      "('train:', 0.9463519231727009, 0.825880398046568)\n",
      "('val:', (0.9453610606271029, 0.8257540649158366))\n",
      "('train:', 0.9380182867883802, 0.8167183393756733)\n",
      "('val:', (0.9344765451363166, 0.8124714907540257))\n",
      "('train:', 0.9302076487207624, 0.8074187510460122)\n",
      "('val:', (0.932252672190203, 0.8101330831403725))\n",
      "('train:', 0.9199858551153258, 0.7962040222614195)\n",
      "('val:', (0.9214598228356915, 0.7974617246382336))\n",
      "('train:', 0.9092514056712345, 0.784249724716954)\n",
      "('val:', (0.9055202724797949, 0.7810040825558765))\n",
      "('train:', 0.8953968295231013, 0.76888143239157)\n",
      "('val:', (0.8873640553801068, 0.7610090363817616))\n",
      "('train:', 0.8836446177590255, 0.756152244982224)\n",
      "('val:', (0.8796247622802938, 0.7507519922883972))\n",
      "('train:', 0.8719708417837788, 0.7433388083916797)\n",
      "('val:', (0.8668394119865864, 0.7377017602215018))\n",
      "('train:', 0.8581907736739851, 0.7282244611938294)\n",
      "('val:', (0.8534559164638915, 0.7236709488009562))\n",
      "('train:', 0.8461714286963755, 0.7158199489629482)\n",
      "('val:', (0.8430805965330181, 0.7121436860531738))\n",
      "('train:', 0.8327847732131176, 0.7010115156972117)\n",
      "('val:', (0.8297337364851191, 0.6979205468609548))\n",
      "('train:', 0.8205805924511279, 0.6883793630243373)\n",
      "('val:', (0.821491245463009, 0.6887005003243809))\n",
      "('train:', 0.8062694597758299, 0.6734365323061662)\n",
      "('val:', (0.8041399266236982, 0.67060377163916))\n"
     ]
    }
   ],
   "source": [
    "users = int(np.max(train_df.uid.unique()))\n",
    "tasks = int(np.max(train_df.task_id.unique()))\n",
    "print(users, tasks)\n",
    "print(int(np.max(validate_df.uid.unique())))\n",
    "print(int(np.max(validate_df.task_id.unique())))\n",
    "model_full = Model_LightFM_Vanilla_Psych_No_Bias(users+1, tasks+1, 167, k=4)\n",
    "print \"Psych No Bias\"\n",
    "train(train_df, validate_df, test_df, model_full)\n",
    "print \"Psych Features Only\"\n",
    "model_feat = Model_LightFM_Psych_First(tasks+1, 167, k=4)\n",
    "train(train_df, validate_df, test_df, model_feat)\n",
    "print \"Psych Full\"\n",
    "model_full_divided = Model_LightFM_Users_Psych_Given(users+1, tasks+1, model_feat.user_feature_lv, \\\n",
    "                                                     model_feat.num_user_features_add_bias, \\\n",
    "                                        model_feat.task_lv, model_feat.task_bias, model_feat.global_bias, k=4)\n",
    "train(train_df, validate_df, test_df, model_full_divided)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2051, 1547)\n",
      "2051\n",
      "1547\n",
      "Psych Features Only\n",
      "('val:', (0.9236654121878999, 0.8262980817664453))\n",
      "('train:', 0.7464583364120257, 0.6822771350647897)\n",
      "('val:', (0.7446391663832783, 0.6809302161913199))\n",
      "('train:', 0.7449411649091326, 0.6811749654207538)\n",
      "('val:', (0.7446392375417352, 0.680930286886599))\n",
      "('train:', 0.7449342659867216, 0.6811699151203855)\n",
      "('val:', (0.7446392856418375, 0.6809303220903334))\n",
      "('train:', 0.7449402319785907, 0.6811744510851342)\n",
      "('val:', (0.744639281990339, 0.6809303199393307))\n",
      "('train:', 0.7449402268818217, 0.6811744479372516)\n",
      "('val:', (0.74463927521625, 0.6809303160463188))\n",
      "('train:', 0.744940215165872, 0.6811744408919904)\n",
      "('val:', (0.7446392572028685, 0.6809303052307136))\n"
     ]
    }
   ],
   "source": [
    "users = int(np.max(train_df.uid.unique()))\n",
    "tasks = int(np.max(train_df.task_id.unique()))\n",
    "print(users, tasks)\n",
    "print(int(np.max(validate_df.uid.unique())))\n",
    "print(int(np.max(validate_df.task_id.unique())))\n",
    "model_full = Model_LightFM_Vanilla_Psych_No_Bias(users+1, tasks+1, 167, k=4)\n",
    "#print \"Psych No Bias\"\n",
    "#train(train_df, validate_df, test_df, model_full)\n",
    "print \"Psych Features Only\"\n",
    "model_feat = Model_LightFM_Psych_First(tasks+1, 167, k=4)\n",
    "train(train_df, validate_df, test_df, model_feat)\n",
    "print \"Psych Full\"\n",
    "model_full_divided = Model_LightFM_Users_Psych_Given(users+1, tasks+1, model_feat.user_feature_lv, \\\n",
    "                                                     model_feat.num_user_features_add_bias, \\\n",
    "                                        model_feat.task_lv, model_feat.task_bias, model_feat.global_bias, k=4)\n",
    "train(train_df, validate_df, test_df, model_full_divided)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def computeForward(model, users, users_features):\n",
    "    user_vectors = model.user_lv(users)\n",
    "    user_feature_vectors = torch.mm(users_features, model.user_feature_lv.weight)\n",
    "    user_finals = torch.add(user_vectors,user_feature_vectors)\n",
    "    return user_finals\n",
    "\n",
    "def computePsychOnly(model, users, users_features):\n",
    "    user_feature_vectors = torch.mm(users_features,model.user_feature_lv.weight)\n",
    "    return user_feature_vectors\n",
    "    \n",
    "features = np.asarray(model_full.user_feature_lv.weight.data.numpy())\n",
    "np.savetxt(\"psych_feature.csv\", features, delimiter=\",\")\n",
    "users = np.asarray(model_full.user_lv.weight.data.numpy())\n",
    "np.savetxt(\"psych_users.csv\", users, delimiter=\",\")\n",
    "user_mul_bias = np.asarray(model_full.users_bias.weight.data.numpy())\n",
    "np.savetxt(\"psych_users_mul.csv\", user_mul_bias, delimiter=\",\")\n",
    "user_add_bias = np.asarray(model_full.users_add_bias.weight.data.numpy())\n",
    "np.savetxt(\"psych_users_add.csv\", user_add_bias, delimiter=\",\")\n",
    "\n",
    "users = Variable(torch.LongTensor(small_frame_unique.uid.values))\n",
    "users_features = Variable(torch.FloatTensor(small_frame_unique.values[:,:167]))\n",
    "user_vectors_np = computeForward(model_full,users,users_features).data.numpy()\n",
    "np.savetxt(\"psych_user_lv.csv\", user_vectors_np,delimiter=\",\")\n",
    "user_vectors_psych_only_np = computePsychOnly(model_full,users,users_features).data.numpy()\n",
    "np.savetxt(\"psych_only_user_lv.csv\", user_vectors_psych_only_np,delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2051, 1547)\n",
      "2051\n",
      "1547\n",
      "('val:', (0.8905880929782224, 0.7619330212257893))\n",
      "('train:', 0.7559918324748315, 0.6050067468363272)\n",
      "('val:', (0.6513111439973752, 0.49784105635141873))\n",
      "('train:', 0.5913588977777678, 0.44232324634775433)\n",
      "('val:', (0.5481403671809498, 0.404533105926035))\n",
      "('train:', 0.5131431049887925, 0.3764106692619311)\n",
      "('val:', (0.4875359126437761, 0.3567491555911679))\n",
      "('train:', 0.4611720523979751, 0.3371629450827699)\n",
      "('val:', (0.44360345824936215, 0.32453649266616413))\n",
      "('train:', 0.42255281770554626, 0.3099349786791874)\n",
      "('val:', (0.41089400007690474, 0.30197060198030834))\n",
      "('train:', 0.39614885661617405, 0.2923483416014046)\n",
      "('val:', (0.3902047482045066, 0.28853646317273596))\n",
      "('train:', 0.37943007151391367, 0.2817030630626085)\n",
      "('val:', (0.37438708440781043, 0.27852561965111977))\n",
      "('train:', 0.3638368677465081, 0.27223748285039856)\n",
      "('val:', (0.3578877711935199, 0.2689950005773261))\n",
      "('train:', 0.348473095806306, 0.26345734937075826)\n",
      "('val:', (0.34533399490079525, 0.261775785641272))\n",
      "('train:', 0.3389898811562973, 0.2580119965488275)\n",
      "('val:', (0.33738614587947385, 0.257248012509172))\n",
      "('train:', 0.33094460724239816, 0.2535317659733076)\n",
      "('val:', (0.3305273152911445, 0.2532734161856081))\n",
      "('train:', 0.32518277787676164, 0.2502297876687495)\n",
      "('val:', (0.32441911255154765, 0.24990362655045245))\n",
      "('train:', 0.31927327780129783, 0.24692019661438536)\n",
      "('val:', (0.31916209781651717, 0.24692625666954138))\n",
      "('train:', 0.31458505001772546, 0.24427334901751627)\n",
      "('val:', (0.31474526495653876, 0.2444577309125582))\n",
      "('train:', 0.3102048440600711, 0.24195346899927253)\n",
      "('val:', (0.3099621431447043, 0.24202259793508962))\n",
      "('train:', 0.30586734568701174, 0.23969488860872865)\n",
      "('val:', (0.3060015883056868, 0.23987231430293351))\n",
      "('train:', 0.30234345722673417, 0.23771840181869833)\n",
      "('val:', (0.30301178109989885, 0.23825401858785838))\n",
      "('train:', 0.29956914450717925, 0.23619612728934034)\n",
      "('val:', (0.3002188645411706, 0.2366667923326882))\n",
      "('train:', 0.29747098378287773, 0.23496584872549373)\n",
      "('val:', (0.2982453896947467, 0.235569839517681))\n",
      "('train:', 0.2957726486588555, 0.23402513128333657)\n",
      "('val:', (0.29677366902807134, 0.2347400054936185))\n",
      "('train:', 0.2944540091662662, 0.23333202860633084)\n",
      "('val:', (0.2952594382398144, 0.23398198515267898))\n",
      "('train:', 0.2926431262171966, 0.23243760816077377)\n",
      "('val:', (0.2934375811767512, 0.23301674026703145))\n",
      "('train:', 0.2912654694824526, 0.2316951651907693)\n",
      "('val:', (0.29230180130529954, 0.23238371485434567))\n",
      "('train:', 0.29023564184289546, 0.23112751184355573)\n",
      "('val:', (0.29113020796906, 0.23176466028376871))\n",
      "('train:', 0.28927610965859746, 0.2305472142249839)\n",
      "('val:', (0.2902038007818904, 0.23120131861655616))\n"
     ]
    }
   ],
   "source": [
    "users = int(np.max(train_df.uid.unique()))\n",
    "tasks = int(np.max(train_df.task_id.unique()))\n",
    "print(users, tasks)\n",
    "print(int(np.max(validate_df.uid.unique())))\n",
    "print(int(np.max(validate_df.task_id.unique())))\n",
    "model = Vanilla_MF(users+1, tasks+1, 167, k=4)\n",
    "train(train_df, validate_df, test_df, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "1.00000e-02 *\n",
      "  0.5898\n",
      " -1.7759\n",
      "  1.4298\n",
      " -1.4969\n",
      "[torch.FloatTensor of size 4]\n",
      "\n",
      "Variable containing:\n",
      "1.00000e-02 *\n",
      " -2.4286\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 0.2711\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Parameter containing:\n",
      "-0.2014\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "(0.3027451617207054, 0.24544794995265093)\n"
     ]
    }
   ],
   "source": [
    "user_lv_pulled = model.user_lv.weight\n",
    "user_lv_mean = torch.mean(user_lv_pulled,0)\n",
    "print user_lv_mean\n",
    "user_bias_mean = torch.mean(model.users_bias.weight)\n",
    "print user_bias_mean\n",
    "user_add_bias_mean = torch.mean(model.users_add_bias.weight)\n",
    "print user_add_bias_mean\n",
    "print model.global_bias\n",
    "print rmse_means(validate_df, model, user_lv_mean, user_bias_mean, user_add_bias_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
